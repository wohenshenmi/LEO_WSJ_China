{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "从 data_raw/final_merged_all_news 文件里随机挑选 10000 条内容，存为 data_raw/news_10000_rows.csv",
   "id": "32b5bb4a9b9766ac"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "input_file = 'data_raw/final_merged_all_news.csv'\n",
    "output_file = 'data_raw/news_10000_rows.csv'\n",
    "\n",
    "# 读取 csv 文件\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 去除重复记录\n",
    "    df_unique = df.drop_duplicates()\n",
    "\n",
    "    # 随机选择 10000 条不重复记录\n",
    "    if len(df_unique) >= 10000:\n",
    "        df_sample = df_unique.sample(n=10000, random_state=42)\n",
    "    else:\n",
    "        print(f\"文件中的不重复记录数少于 10000 条，仅 {len(df_unique)} 条记录，将全部保存。\")\n",
    "        df_sample = df_unique\n",
    "\n",
    "    # 将随机选择的记录保存为新的 csv 文件\n",
    "    df_sample.to_csv(output_file, index=False)\n",
    "    print(f\"✅ 已随机选择 {len(df_sample)} 条不重复记录并保存到 {output_file}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 输入文件未找到 {input_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 发生错误: {e}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_to_check = 'data_raw/final_merged_all_news.csv'\n",
    "df = pd.read_csv(file_to_check)\n",
    "print(f\"文件 '{file_to_check}' 的实际数据条数是: {len(df)}\")"
   ],
   "id": "428809980adece12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 设置文件路径\n",
    "ALIYUN_OSS_PATH = ''\n",
    "FINAL_CHINA_NEWS_FILE = ALIYUN_OSS_PATH + 'data_processed/final_china_news.csv'\n",
    "DUPLICATE_NEWS_FILE = ALIYUN_OSS_PATH + 'data_processed/duplicate_china_news.csv'\n",
    "\n",
    "# 加载数据\n",
    "print(\"Loading data...\")\n",
    "df_china_news = pd.read_csv(FINAL_CHINA_NEWS_FILE, low_memory=False)\n",
    "print(f\"Data loaded successfully, total {len(df_china_news)} news articles\")\n",
    "\n",
    "# 添加新闻内容长度列\n",
    "df_china_news['content_length'] = df_china_news['CONTENT'].astype(str).apply(len)\n",
    "\n",
    "# 查找重复的新闻 (基于CONTENT列)\n",
    "print(\"Finding duplicate news articles...\")\n",
    "duplicate_mask = df_china_news.duplicated(subset=['CONTENT'], keep=False)\n",
    "df_duplicates = df_china_news[duplicate_mask]\n",
    "\n",
    "# 按CONTENT排序，以便将相同的新闻放在一起\n",
    "df_duplicates = df_duplicates.sort_values(['CONTENT', 'DATE']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Found {len(df_duplicates)} duplicate news articles\")\n",
    "\n",
    "# 保存到CSV文件\n",
    "print(f\"Saving duplicate news to {DUPLICATE_NEWS_FILE}...\")\n",
    "df_duplicates.to_csv(DUPLICATE_NEWS_FILE, index=False, encoding='utf-8')\n",
    "print(\"Save completed!\")\n",
    "\n",
    "# 显示一些统计信息\n",
    "print(\"\\n=== Duplicate News Statistics ===\")\n",
    "print(f\"Total news: {len(df_china_news)}\")\n",
    "print(f\"Duplicate news: {len(df_duplicates)}\")\n",
    "print(f\"Unique news: {len(df_china_news) - len(df_duplicates)}\")\n",
    "print(f\"Actually unique content: {len(df_china_news.CONTENT.unique())}\")\n",
    "\n",
    "# 显示重复最多的几条新闻\n",
    "if len(df_duplicates) > 0:\n",
    "    print(\"\\n=== Most Frequently Duplicated News ===\")\n",
    "    duplicate_counts = df_china_news['CONTENT'].value_counts()\n",
    "    for i, (content, count) in enumerate(duplicate_counts.head(5).items()):\n",
    "        # 获取第一条重复新闻的日期和内容长度\n",
    "        sample_entry = df_china_news[df_china_news['CONTENT'] == content].iloc[0]\n",
    "        date = sample_entry['DATE']\n",
    "        length = sample_entry['content_length']\n",
    "        print(f\"{i + 1}. Duplicated {count} times\")\n",
    "        print(f\"   Date: {date}\")\n",
    "        print(f\"   Content length: {length} characters\")\n",
    "        print(f\"   Content preview: {content[:100]}...\")"
   ],
   "id": "296ec8bea5039411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 设置文件路径\n",
    "ALIYUN_OSS_PATH = ''\n",
    "RAW_NEWS_FILE = ALIYUN_OSS_PATH + 'data_raw/final_merged_all_news.csv'\n",
    "SHORT_ARTICLES_FILE = ALIYUN_OSS_PATH + 'data_processed/short_articles.csv'\n",
    "\n",
    "# 加载数据\n",
    "print(\"Loading raw news data...\")\n",
    "df_raw_news = pd.read_csv(RAW_NEWS_FILE, low_memory=False)\n",
    "print(f\"Data loaded successfully, total {len(df_raw_news)} news articles\")\n",
    "\n",
    "# 假设新闻内容列名为'CONTENT'，日期列名为'DATE'\n",
    "CONTENT_COLUMN = 'CONTENT'\n",
    "DATE_COLUMN = 'DATE'\n",
    "\n",
    "# 计算每篇文章的长度\n",
    "df_raw_news['content_length'] = df_raw_news[CONTENT_COLUMN].astype(str).apply(len)\n",
    "\n",
    "# 找出异常短文（长度小于50字符）\n",
    "short_articles_mask = df_raw_news['content_length'] < 50\n",
    "df_short_articles = df_raw_news[short_articles_mask]\n",
    "\n",
    "# 按长度排序\n",
    "df_short_articles = df_short_articles.sort_values('content_length').reset_index(drop=True)\n",
    "\n",
    "print(f\"Found {len(df_short_articles)} short articles (<50 characters)\")\n",
    "\n",
    "# 保存到CSV文件\n",
    "print(f\"Saving short articles to {SHORT_ARTICLES_FILE}...\")\n",
    "df_short_articles.to_csv(SHORT_ARTICLES_FILE, index=False, encoding='utf-8')\n",
    "print(\"Save completed!\")\n",
    "\n",
    "# 显示统计信息\n",
    "print(\"\\n=== Short Articles Statistics ===\")\n",
    "print(f\"Total articles in raw data: {len(df_raw_news)}\")\n",
    "print(f\"Short articles (<50 characters): {len(df_short_articles)}\")\n",
    "print(f\"Percentage of short articles: {len(df_short_articles) / len(df_raw_news) * 100:.2f}%\")\n",
    "\n",
    "# 显示一些示例\n",
    "if len(df_short_articles) > 0:\n",
    "    print(\"\\n=== Sample Short Articles ===\")\n",
    "    print(\"Content Length | Date       | Content Preview\")\n",
    "    print(\"---------------|------------|----------------\")\n",
    "    for i in range(min(10, len(df_short_articles))):\n",
    "        row = df_short_articles.iloc[i]\n",
    "        content_preview = row[CONTENT_COLUMN][:50] + \"...\" if len(str(row[CONTENT_COLUMN])) > 50 else row[CONTENT_COLUMN]\n",
    "        date = row[DATE_COLUMN] if DATE_COLUMN in df_short_articles.columns else \"N/A\"\n",
    "        print(f\"{row['content_length']:14} | {date:10} | {content_preview}\")"
   ],
   "id": "cdc223042ac70c99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "be1d76cea43c1f30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "427a44e6c6974de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
