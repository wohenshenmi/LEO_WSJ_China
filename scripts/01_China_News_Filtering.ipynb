{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# æ­¥éª¤ 1: å‡†å¤‡å·¥ä½œä¸ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "**ç›®æ ‡:** å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ï¼Œå¹¶è®¾ç½®å¥½æ–‡ä»¶è·¯å¾„å’Œå…¨å±€å˜é‡ã€‚"
   ],
   "id": "3c4e87dfe8957389"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T09:57:12.365583Z",
     "start_time": "2025-07-30T09:57:11.189433Z"
    }
   },
   "source": [
    "import html\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import spacy\n",
    "import unicodedata\n",
    "from flashtext import KeywordProcessor\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import islice # ä¸ºæµ‹è¯•æ¨¡å¼å¼•å…¥\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# --- å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¼€å…³ ---\n",
    "TEST_MODE = True\n",
    "TEST_SAMPLE_SIZE = 50000  # æµ‹è¯•æ¨¡å¼ä¸‹å¤„ç†çš„åŸå§‹æ–°é—»è¡Œæ•°\n",
    "CANDIDATE_SAMPLE_SIZE = 2000 # æµ‹è¯•æ¨¡å¼ä¸‹ç²¾ç­›çš„å€™é€‰æ–‡ç« æ•°\n",
    "\n",
    "# --- å…¨å±€æ–‡ä»¶ä¸å‚æ•°é…ç½® ---\n",
    "ALIYUN_OSS_PATH = ''\n",
    "KEYWORD_JSON_PATH = os.path.join(ALIYUN_OSS_PATH, '../data_raw/china_keywords_collection.json')\n",
    "SOURCE_NEWS_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_raw/final_merged_all_news.csv')\n",
    "CANDIDATES_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_processed/china_news_candidates.csv')\n",
    "FINAL_RESULT_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_processed/final_china_news.csv')\n",
    "REJECTED_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_processed/china_news_rejected_articles.csv')\n",
    "\n",
    "# --- å…¨å±€å¤„ç†å‚æ•° ---\n",
    "NEWS_COLUMN = 'CONTENT'\n",
    "CHUNKSIZE = 20000\n",
    "\n",
    "# --- æ™ºèƒ½é…ç½®å¹¶è¡Œå‚æ•° ---\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "print(\"âœ… å— 1: åº“å¯¼å…¥å’Œé…ç½®å®Œæˆã€‚\")\n",
    "print(\"-\" * 30)\n",
    "if TEST_MODE:\n",
    "    print(f\"ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼ğŸš€ğŸš€ğŸš€\")\n",
    "    print(f\"   - åˆç­›é˜¶æ®µå°†å¤„ç†å‰ {TEST_SAMPLE_SIZE} è¡ŒåŸå§‹æ•°æ®ã€‚\")\n",
    "    print(f\"   - ç²¾ç­›é˜¶æ®µå°†å¤„ç†å‰ {CANDIDATE_SAMPLE_SIZE} ç¯‡å€™é€‰æ–‡ç« ã€‚\")\n",
    "else:\n",
    "    print(\"ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®ã€‚ğŸš¢ğŸš¢ğŸš¢\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"   - å°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å— 1: åº“å¯¼å…¥å’Œé…ç½®å®Œæˆã€‚\n",
      "------------------------------\n",
      "ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼ğŸš€ğŸš€ğŸš€\n",
      "   - åˆç­›é˜¶æ®µå°†å¤„ç†å‰ 50000 è¡ŒåŸå§‹æ•°æ®ã€‚\n",
      "   - ç²¾ç­›é˜¶æ®µå°†å¤„ç†å‰ 2000 ç¯‡å€™é€‰æ–‡ç« ã€‚\n",
      "------------------------------\n",
      "   - å°†ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# æ­¥éª¤ 2: åˆç­›å‡†å¤‡ - æ„å»ºæ™ºèƒ½æ­£åˆ™è¡¨è¾¾å¼\n",
    "\n",
    "**ç›®æ ‡:** è¯»å– ä¸­å›½ç›¸å…³å…³é”®è¯ JSON æ–‡ä»¶ï¼Œå¹¶æ‰§è¡Œæˆ‘ä»¬è®¨è®ºè¿‡çš„æ‰€æœ‰é€»è¾‘æ¥æ„å»ºä¸€ä¸ªå¼ºå¤§ã€é«˜æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ã€‚"
   ],
   "id": "61a97e0c381dad4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:57:12.540579Z",
     "start_time": "2025-07-30T09:57:12.531643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ„å»ºåˆç­›ç”¨çš„ Flashtext å…³é”®è¯å¤„ç†å™¨ ---\n",
    "\n",
    "def build_keyword_processor(json_path):\n",
    "    \"\"\"\n",
    "    ä»å…³é”®è¯ JSON æ–‡ä»¶ä¸­æ„å»ºä¸€ä¸ªé«˜æ•ˆçš„ Flashtext KeywordProcessorã€‚\n",
    "    \"\"\"\n",
    "    print(f\"æ­£åœ¨ä» {json_path} åŠ è½½å…³é”®è¯...\")\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            keywords_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ é”™è¯¯: å…³é”®è¯æ–‡ä»¶æœªæ‰¾åˆ° {json_path}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"å…±åŠ è½½ {len(keywords_data)} ä¸ªå…³é”®è¯å¯¹è±¡ã€‚\")\n",
    "\n",
    "    # 1. æå–å…¨éƒ¨å…³é”®è¯å’Œåˆ«å\n",
    "    all_aliases = set()\n",
    "    for item in keywords_data:\n",
    "        all_aliases.add(item['keyword'])\n",
    "        for alias in item.get('aliases', []):\n",
    "            all_aliases.add(alias)\n",
    "    print(f\"æå–å‡º {len(all_aliases)} ä¸ªä¸é‡å¤çš„å…³é”®è¯/åˆ«åã€‚\")\n",
    "\n",
    "    # 2. åˆå§‹åŒ– Flashtext å¤„ç†å™¨å¹¶æ·»åŠ å…³é”®è¯\n",
    "    # case_sensitive=False ä½¿å…¶ä¸åŒºåˆ†å¤§å°å†™\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "    for kw in all_aliases:\n",
    "        keyword_processor.add_keyword(kw)\n",
    "\n",
    "    print(\"âœ… é«˜æ•ˆå…³é”®è¯å¤„ç†å™¨ (Flashtext) æ„å»ºå®Œæˆã€‚\")\n",
    "    return keyword_processor, keywords_data\n",
    "\n",
    "\n",
    "# æ‰§è¡Œæ„å»º\n",
    "keyword_processor, keywords_data = build_keyword_processor(KEYWORD_JSON_PATH)\n",
    "\n",
    "# å°†å…³é”®è¯å¤„ç†å™¨è®¾ä¸ºå…¨å±€å˜é‡ï¼Œä»¥ä¾¿åç»­å­è¿›ç¨‹å¯ä»¥è®¿é—® (åœ¨æŸäº›ç³»ç»Ÿä¸Šéœ€è¦)\n",
    "global_keyword_processor = keyword_processor\n",
    "\n",
    "print(\"\\nâœ… å— 2: åˆç­›å‡†å¤‡å·¥ä½œå®Œæˆã€‚\")"
   ],
   "id": "3e6d80d362a6a719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä» ../data_raw/china_keywords_collection.json åŠ è½½å…³é”®è¯...\n",
      "å…±åŠ è½½ 274 ä¸ªå…³é”®è¯å¯¹è±¡ã€‚\n",
      "æå–å‡º 394 ä¸ªä¸é‡å¤çš„å…³é”®è¯/åˆ«åã€‚\n",
      "âœ… é«˜æ•ˆå…³é”®è¯å¤„ç†å™¨ (Flashtext) æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "âœ… å— 2: åˆç­›å‡†å¤‡å·¥ä½œå®Œæˆã€‚\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# æ­¥éª¤ 3: æ‰§è¡Œé˜¶æ®µä¸€ - è°ƒç”¨å¤–éƒ¨è„šæœ¬è¿›è¡Œå¿«é€Ÿåˆç­›\n",
    "\n",
    "**ç›®æ ‡:** å¯¹å¤§æ–‡ä»¶è¿›è¡Œåˆ†å—æ‰«æï¼Œåº”ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¹¶ä¿å­˜å€™é€‰é›†ã€‚è¿™å°†æ˜¯æ•´ä¸ªæµç¨‹ä¸­æœ€è€—æ—¶çš„éƒ¨åˆ†ã€‚"
   ],
   "id": "d3d24e1445bfbb23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:57:37.843410Z",
     "start_time": "2025-07-30T09:57:12.552038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- å®šä¹‰å¹¶è¡Œå¤„ç†å‡½æ•° ---\n",
    "\n",
    "def lightweight_clean(text):\n",
    "    \"\"\"ä¸€ä¸ªéå¸¸è½»é‡çº§çš„æ–‡æœ¬æ¸…ç†å‡½æ•°ã€‚\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = html.unescape(text)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    \"\"\"å¤„ç†å•ä¸ªæ•°æ®å—çš„æ ¸å¿ƒä»»åŠ¡ï¼šæ¸…ç†æ–‡æœ¬å¹¶ä½¿ç”¨Flashtextç­›é€‰ã€‚\"\"\"\n",
    "    if NEWS_COLUMN not in chunk_df.columns:\n",
    "        return pd.DataFrame()\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œçš„ keyword_processor æ˜¯åœ¨ä¸»è¿›ç¨‹ä¸­å®šä¹‰çš„ï¼Œå¯ä»¥ç›´æ¥è®¿é—®\n",
    "    cleaned_series = chunk_df[NEWS_COLUMN].astype(str).apply(lightweight_clean)\n",
    "    mask = cleaned_series.apply(lambda x: len(keyword_processor.extract_keywords(x)) > 0)\n",
    "    return chunk_df[mask]\n",
    "\n",
    "# --- æ‰§è¡Œé˜¶æ®µä¸€ - æµå¼æ‰«æä¸åˆç­› (å·²æ”¹ä¸ºå•è¿›ç¨‹é¡ºåºå¤„ç†) ---\n",
    "print(\"--- é˜¶æ®µä¸€: å¼€å§‹ä½¿ç”¨å•è¿›ç¨‹è¿›è¡Œæµå¼åˆç­› ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    chunk_iterator = pd.read_csv(SOURCE_NEWS_FILE, chunksize=CHUNKSIZE, on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "    if TEST_MODE:\n",
    "        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œåªå¤„ç†å‰é¢å‡ ä¸ªå—\n",
    "        num_test_chunks = (TEST_SAMPLE_SIZE // CHUNKSIZE) + 1\n",
    "        chunk_iterator = islice(chunk_iterator, num_test_chunks)\n",
    "        total_chunks = num_test_chunks\n",
    "        print(f\"ğŸš€ æµ‹è¯•æ¨¡å¼: å°†å¤„ç†å‰ {total_chunks} ä¸ªæ•°æ®å— (çº¦ {TEST_SAMPLE_SIZE} è¡Œ)ã€‚\")\n",
    "    else:\n",
    "        # åœ¨å®Œæ•´æ¨¡å¼ä¸‹ï¼Œè®¡ç®—æ€»å—æ•°ä»¥æä¾›å‡†ç¡®è¿›åº¦\n",
    "        print(\"æ­£åœ¨è®¡ç®—æ–‡ä»¶æ€»å—æ•°...\")\n",
    "        # æ³¨æ„ï¼šè¿™ä¸ªè®¡ç®—å¯èƒ½ä¼šå¾ˆæ…¢ï¼Œå¦‚æœæ–‡ä»¶å·¨å¤§ï¼Œå¯ä»¥è€ƒè™‘æ³¨é‡Šæ‰\n",
    "        num_lines = sum(1 for row in open(SOURCE_NEWS_FILE, 'r', encoding='utf-8', errors='ignore'))\n",
    "        total_chunks = (num_lines // CHUNKSIZE) + 1\n",
    "        print(f\"æ–‡ä»¶çº¦åŒ…å« {total_chunks} ä¸ªæ•°æ®å—ã€‚\")\n",
    "\n",
    "    print(\"å°†ä½¿ç”¨å•è¿›ç¨‹é¡ºåºå¤„ç†...\")\n",
    "\n",
    "    is_first_chunk = True\n",
    "    total_candidates = 0\n",
    "\n",
    "    # --- æ”¹ä¸ºå•è¿›ç¨‹é¡ºåºå¤„ç† ---\n",
    "    # ä½¿ç”¨tqdmåŒ…è£…è¿­ä»£å™¨ä»¥æ˜¾ç¤ºè¿›åº¦\n",
    "    for chunk_df in tqdm(chunk_iterator, total=total_chunks, desc=\"é¡ºåºåˆç­›ä¸­\"):\n",
    "        # åœ¨ä¸»è¿›ç¨‹ä¸­ç›´æ¥è°ƒç”¨å¤„ç†å‡½æ•°\n",
    "        candidates_df = process_chunk(chunk_df)\n",
    "\n",
    "        if not candidates_df.empty:\n",
    "            total_candidates += len(candidates_df)\n",
    "            if is_first_chunk:\n",
    "                candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='w', encoding='utf-8')\n",
    "                is_first_chunk = False\n",
    "            else:\n",
    "                candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='a', header=False, encoding='utf-8')\n",
    "\n",
    "    end_time_stage1 = time.time()\n",
    "    print(\"\\n--- åˆç­›æµç¨‹æ‰§è¡Œå®Œæ¯• ---\")\n",
    "    print(f\"æ€»å…±æ‰¾åˆ° {total_candidates} ç¯‡å€™é€‰æ–‡ç« ï¼Œå·²ä¿å­˜åˆ° {CANDIDATES_FILE}\")\n",
    "    print(f\"é˜¶æ®µä¸€ (é¡ºåºåˆç­›) è€—æ—¶: {(end_time_stage1 - start_time):.2f} ç§’ã€‚\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯: åŸå§‹æ–°é—»æ–‡ä»¶æœªæ‰¾åˆ° {SOURCE_NEWS_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "\n",
    "print(\"\\nâœ… å— 3: åˆç­›æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚\")"
   ],
   "id": "792bd4595d23d8f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- é˜¶æ®µä¸€: å¼€å§‹ä½¿ç”¨å•è¿›ç¨‹è¿›è¡Œæµå¼åˆç­› ---\n",
      "ğŸš€ æµ‹è¯•æ¨¡å¼: å°†å¤„ç†å‰ 3 ä¸ªæ•°æ®å— (çº¦ 50000 è¡Œ)ã€‚\n",
      "å°†ä½¿ç”¨å•è¿›ç¨‹é¡ºåºå¤„ç†...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "é¡ºåºåˆç­›ä¸­:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e646d271bdf54455b915cbf7ab85fda4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- åˆç­›æµç¨‹æ‰§è¡Œå®Œæ¯• ---\n",
      "æ€»å…±æ‰¾åˆ° 3430 ç¯‡å€™é€‰æ–‡ç« ï¼Œå·²ä¿å­˜åˆ° ../data_processed/china_news_candidates.csv\n",
      "é˜¶æ®µä¸€ (é¡ºåºåˆç­›) è€—æ—¶: 25.28 ç§’ã€‚\n",
      "\n",
      "âœ… å— 3: åˆç­›æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# æ­¥éª¤ 4: ç²¾ç­›å‡†å¤‡ - åŠ è½½æ¨¡å‹ä¸å®šä¹‰è§„åˆ™\n",
    "\n",
    "**ç›®æ ‡:** è´Ÿè´£åŠ è½½ spaCy æ¨¡å‹å’Œæ•°æ®ï¼Œå¹¶å®šä¹‰æ‰€æœ‰ç”¨äºç²¾ç­›çš„â€œå¦å†³è§„åˆ™â€å‡½æ•°ã€‚"
   ],
   "id": "ace73e1406b18418"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:57:39.146962Z",
     "start_time": "2025-07-30T09:57:37.848415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- ç²¾ç­›å‡†å¤‡ - åŠ è½½æ¨¡å‹ä¸å®šä¹‰å¦å†³è§„åˆ™ ---(å·²ä¿®æ”¹)\n",
    "print(\"--- é˜¶æ®µäºŒå‡†å¤‡: åŠ è½½ spaCy æ¨¡å‹ ---\")\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\", \"lemmatizer\", \"tagger\", \"attribute_ruler\"])\n",
    "    print(f\"âœ… spaCy æ¨¡å‹ '{nlp.meta['name']}' çš„æ ¸å¿ƒç»„ä»¶åŠ è½½æˆåŠŸã€‚\")\n",
    "except OSError:\n",
    "    print(\"é”™è¯¯: spaCyæ¨¡å‹ 'en_core_web_lg' æœªå®‰è£…ã€‚\")\n",
    "    print(\"è¯·åœ¨ä½ çš„ç»ˆç«¯æˆ–å‘½ä»¤è¡Œä¸­è¿è¡Œ: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "\n",
    "if nlp:\n",
    "    # --- å‡†å¤‡ PhraseMatcher (å·²ç®€åŒ–) ---\n",
    "    print(\"æ­£åœ¨å‡†å¤‡ PhraseMatcher...\")\n",
    "    from spacy.matcher import PhraseMatcher\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "    all_aliases = [alias for item in keywords_data for alias in item.get('aliases', [item['keyword']])]\n",
    "    patterns = [nlp.make_doc(text) for text in all_aliases]\n",
    "\n",
    "    matcher.add(\"ChinaKeywords\", patterns)\n",
    "    print(f\"âœ… PhraseMatcher å‡†å¤‡å®Œæˆï¼Œå·²æ·»åŠ  {len(patterns)} ä¸ªæ¨¡å¼ã€‚\")\n",
    "\n",
    "# --- å®šä¹‰å¦å†³è§„åˆ™å‡½æ•° (å·²ç§»é™¤ check_low_tier_only) ---\n",
    "def check_negation(doc, keywords_in_doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"neg\" and token.head.text.lower() in keywords_in_doc:\n",
    "            return True, f\"å¦å®šè¯­å¢ƒ: '{token.text}' ä¿®é¥°äº†å…³é”®è¯ '{token.head.text}'\"\n",
    "    return False, \"\"\n",
    "\n",
    "def check_hypothetical(doc, keywords_in_doc):\n",
    "    for sent in doc.sents:\n",
    "        if any(tok.lower_ in ['if', 'unless'] for tok in sent[:3]):\n",
    "             if any(token.text.lower() in keywords_in_doc for token in sent):\n",
    "                return True, f\"å‡è®¾è¯­å¢ƒ: å¥å­åŒ…å« '{sent[:3].text.strip()}'...\"\n",
    "    return False, \"\"\n",
    "\n",
    "print(\"âœ… å— 4: ç²¾ç­›è§„åˆ™å®šä¹‰å’Œ Matcher å‡†å¤‡å®Œæˆã€‚\")"
   ],
   "id": "eb5b2016c4e20800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- é˜¶æ®µäºŒå‡†å¤‡: åŠ è½½ spaCy æ¨¡å‹ ---\n",
      "âœ… spaCy æ¨¡å‹ 'core_web_lg' çš„æ ¸å¿ƒç»„ä»¶åŠ è½½æˆåŠŸã€‚\n",
      "æ­£åœ¨å‡†å¤‡ PhraseMatcher...\n",
      "âœ… PhraseMatcher å‡†å¤‡å®Œæˆï¼Œå·²æ·»åŠ  397 ä¸ªæ¨¡å¼ã€‚\n",
      "âœ… å— 4: ç²¾ç­›è§„åˆ™å®šä¹‰å’Œ Matcher å‡†å¤‡å®Œæˆã€‚\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# æ­¥éª¤ 5: æ‰§è¡Œé˜¶æ®µäºŒ - ç²¾ç­›æµç¨‹\n",
    "\n",
    "**ç›®æ ‡:** åŠ è½½å€™é€‰é›†ï¼Œåº”ç”¨æ‰€æœ‰å¦å†³è§„åˆ™ï¼Œç„¶åä¿å­˜æœ€ç»ˆç»“æœå’Œè¢«æ‹’ç»çš„æ–‡ç« ã€‚"
   ],
   "id": "8b93ae6fb597319a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:58:22.024947Z",
     "start_time": "2025-07-30T09:57:39.166780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ‰§è¡Œé˜¶æ®µäºŒ - ä½¿ç”¨å¹¶è¡ŒåŒ–çš„ spaCy è¿›è¡Œé«˜æ•ˆç²¾ç­› ---(å·²é›†æˆæµ‹è¯•æ¨¡å¼)\n",
    "\n",
    "print(\"--- é˜¶æ®µäºŒ: å¼€å§‹ç²¾ç­›å€™é€‰é›† ---\")\n",
    "start_time_s2 = time.time()\n",
    "\n",
    "try:\n",
    "    read_csv_kwargs = {'low_memory': False}\n",
    "    if TEST_MODE:\n",
    "        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œåªè¯»å–æ–‡ä»¶çš„å¼€å¤´éƒ¨åˆ†\n",
    "        read_csv_kwargs['nrows'] = CANDIDATE_SAMPLE_SIZE\n",
    "        print(f\"ğŸš€ æµ‹è¯•æ¨¡å¼: æœ€å¤šåŠ è½½å‰ {CANDIDATE_SAMPLE_SIZE} ç¯‡å€™é€‰æ–‡ç« è¿›è¡Œç²¾ç­›ã€‚\")\n",
    "\n",
    "    df_candidates = pd.read_csv(CANDIDATES_FILE, **read_csv_kwargs)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(df_candidates)} ç¯‡å€™é€‰æ–‡ç« ã€‚\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯: å€™é€‰æ–‡ä»¶æœªæ‰¾åˆ° {CANDIDATES_FILE}ã€‚è¯·å…ˆè¿è¡Œå— 3ã€‚\")\n",
    "    df_candidates = pd.DataFrame()\n",
    "\n",
    "if not df_candidates.empty and nlp:\n",
    "    texts = df_candidates[NEWS_COLUMN].astype(str).tolist()\n",
    "    results = []\n",
    "\n",
    "    print(f\"å¼€å§‹ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹å¹¶è¡Œç²¾ç­›...\")\n",
    "    docs = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESSES)\n",
    "\n",
    "    for doc in tqdm(docs, total=len(df_candidates), desc=\"ç²¾ç­›æ–‡ç« \"):\n",
    "        is_rejected, rejection_reason = False, \"\"\n",
    "\n",
    "        matches = matcher(doc)\n",
    "        if not matches:\n",
    "            results.append({'keep': False, 'rejection_reason': 'æœªæ‰¾åˆ°ä»»ä½•å…³é”®è¯(ç²¾ç­›é˜¶æ®µ)'})\n",
    "            continue\n",
    "\n",
    "        found_keywords_text = {doc[start:end].text.lower() for _, start, end in matches}\n",
    "\n",
    "        is_rejected, rejection_reason = check_negation(doc, found_keywords_text)\n",
    "        if not is_rejected:\n",
    "            is_rejected, rejection_reason = check_hypothetical(doc, found_keywords_text)\n",
    "\n",
    "        results.append({'keep': not is_rejected, 'rejection_reason': rejection_reason})\n",
    "\n",
    "    print(\"\\næ­£åœ¨åˆå¹¶ç²¾ç­›ç»“æœ...\")\n",
    "    df_results = pd.DataFrame(results, index=df_candidates.index)\n",
    "    df_final_with_reasons = pd.concat([df_candidates, df_results], axis=1)\n",
    "\n",
    "    df_accepted = df_final_with_reasons[df_final_with_reasons['keep'] == True].drop(columns=['keep', 'rejection_reason'])\n",
    "    df_rejected = df_final_with_reasons[df_final_with_reasons['keep'] == False].drop(columns=['keep'])\n",
    "\n",
    "    print(\"\\n--- ç²¾ç­›å®Œæˆ ---\")\n",
    "    df_accepted.to_csv(FINAL_RESULT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… {len(df_accepted)} ç¯‡æœ€ç»ˆæ–‡ç« å·²ä¿å­˜åˆ°: {FINAL_RESULT_FILE}\")\n",
    "\n",
    "    df_rejected.to_csv(REJECTED_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"â„¹ï¸ {len(df_rejected)} ç¯‡è¢«æ‹’ç»çš„æ–‡ç« å·²ä¿å­˜åˆ°: {REJECTED_FILE} (ä¾›åˆ†æ)\")\n",
    "\n",
    "    end_time_s2 = time.time()\n",
    "    print(f\"é˜¶æ®µäºŒ (ç²¾ç­›) è€—æ—¶: {(end_time_s2 - start_time_s2):.2f} ç§’ã€‚\")\n",
    "\n",
    "else:\n",
    "    print(\"å€™é€‰é›†ä¸ºç©ºæˆ–spaCyæ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡ç²¾ç­›ã€‚\")\n",
    "\n",
    "print(\"\\nâœ… å— 5: ç²¾ç­›æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚\")"
   ],
   "id": "4f5ee5651615939c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- é˜¶æ®µäºŒ: å¼€å§‹ç²¾ç­›å€™é€‰é›† ---\n",
      "ğŸš€ æµ‹è¯•æ¨¡å¼: æœ€å¤šåŠ è½½å‰ 2000 ç¯‡å€™é€‰æ–‡ç« è¿›è¡Œç²¾ç­›ã€‚\n",
      "âœ… æˆåŠŸåŠ è½½ 2000 ç¯‡å€™é€‰æ–‡ç« ã€‚\n",
      "å¼€å§‹ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹å¹¶è¡Œç²¾ç­›...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ç²¾ç­›æ–‡ç« :   0%|          | 0/2000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00d756a56ee74f9c80ed0cb9c4038931"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ­£åœ¨åˆå¹¶ç²¾ç­›ç»“æœ...\n",
      "\n",
      "--- ç²¾ç­›å®Œæˆ ---\n",
      "âœ… 1950 ç¯‡æœ€ç»ˆæ–‡ç« å·²ä¿å­˜åˆ°: ../data_processed/final_china_news.csv\n",
      "â„¹ï¸ 50 ç¯‡è¢«æ‹’ç»çš„æ–‡ç« å·²ä¿å­˜åˆ°: ../data_processed/china_news_rejected_articles.csv (ä¾›åˆ†æ)\n",
      "é˜¶æ®µäºŒ (ç²¾ç­›) è€—æ—¶: 42.85 ç§’ã€‚\n",
      "\n",
      "âœ… å— 5: ç²¾ç­›æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
