{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4e87dfe8957389",
   "metadata": {},
   "source": [
    "# 步骤 1: 准备工作与环境设置\n",
    "\n",
    "**目标:** 导入所有需要的库，并设置好文件路径和全局变量。"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:37:20.285463Z",
     "iopub.status.busy": "2025-07-31T02:37:20.285082Z",
     "iopub.status.idle": "2025-07-31T02:37:32.410184Z",
     "shell.execute_reply": "2025-07-31T02:37:32.409459Z",
     "shell.execute_reply.started": "2025-07-31T02:37:20.285432Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-31T04:30:40.165036Z",
     "start_time": "2025-07-31T04:30:39.115660Z"
    }
   },
   "source": [
    "# --- 步骤 1: 准备工作与环境设置 (最终适配版) ---\n",
    "\n",
    "# --- 核心库 ---\n",
    "import html\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from itertools import islice\n",
    "import shutil # 用于文件复制\n",
    "\n",
    "# --- 数据处理与NLP库 ---\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import spacy\n",
    "from flashtext import KeywordProcessor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 核心配置区 (您需要修改的唯一变量) ---\n",
    "# ==============================================================================\n",
    "# 1. 环境配置\n",
    "# 目的: 根据您当前的运行环境，自动选择正确的文件路径。\n",
    "# 选项:\n",
    "#   'local': 在您自己的电脑上运行。\n",
    "#   'dsw':   在阿里云DSW服务器上运行。\n",
    "RUNNING_ENV = 'local'\n",
    "\n",
    "# 2. 快速测试模式开关\n",
    "TEST_MODE = False\n",
    "TEST_SAMPLE_SIZE = 50000\n",
    "CANDIDATE_SAMPLE_SIZE = 2000\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 路径智能管理 (根据上面的 RUNNING_ENV 自动配置) ---\n",
    "# ==============================================================================\n",
    "print(f\"检测到运行环境为: 【{RUNNING_ENV.upper()}】\")\n",
    "TEMP_DIR = '/tmp' # 服务器的本地临时目录\n",
    "\n",
    "# 根据环境定义数据根路径\n",
    "if RUNNING_ENV == 'local':\n",
    "    # 本地模式: 使用相对于当前脚本(scripts目录)的路径\n",
    "    print(\"使用 'local' 模式的相对路径。\")\n",
    "    BASE_DATA_RAW_PATH = '../data_raw'\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    # DSW模式: 使用挂载的OSS绝对路径\n",
    "    print(\"使用 'dsw' 模式的绝对路径。\")\n",
    "    BASE_DATA_RAW_PATH = '/mnt/data/data_raw'\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "else:\n",
    "    raise ValueError(f\"未知的 RUNNING_ENV: '{RUNNING_ENV}'. 请选择 'local' 或 'dsw'。\")\n",
    "\n",
    "# 使用上面定义的基础路径来构建完整的文件路径\n",
    "KEYWORD_JSON_PATH = os.path.join(BASE_DATA_RAW_PATH, 'china_keywords_collection.json')\n",
    "SOURCE_NEWS_FILE_ORIGINAL = os.path.join(BASE_DATA_RAW_PATH, 'final_merged_all_news.csv')\n",
    "CANDIDATES_FILE_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_candidates.csv')\n",
    "FINAL_RESULT_FILE = os.path.join(BASE_DATA_PROCESSED_PATH, 'final_china_news.csv')\n",
    "REJECTED_FILE = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_rejected_articles.csv')\n",
    "\n",
    "# 初始化将要在后续流程中实际使用的路径变量\n",
    "SOURCE_NEWS_FILE = SOURCE_NEWS_FILE_ORIGINAL\n",
    "CANDIDATES_FILE = CANDIDATES_FILE_ORIGINAL\n",
    "\n",
    "# 如果是服务器环境，则重定向高I/O负载的路径到 /tmp\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW 环境模式已激活，为避免I/O错误，将使用本地临时目录 /tmp ...\")\n",
    "    TEMP_SOURCE_NEWS_FILE = os.path.join(TEMP_DIR, 'final_merged_all_news.csv')\n",
    "    TEMP_CANDIDATES_FILE = os.path.join(TEMP_DIR, 'china_news_candidates.csv')\n",
    "\n",
    "    try:\n",
    "        # 检查原始文件是否存在，如果不存在则无法继续\n",
    "        if not os.path.exists(SOURCE_NEWS_FILE_ORIGINAL):\n",
    "             raise FileNotFoundError(f\"在DSW的源路径 {SOURCE_NEWS_FILE_ORIGINAL} 未找到文件！\")\n",
    "\n",
    "        source_size = os.path.getsize(SOURCE_NEWS_FILE_ORIGINAL)\n",
    "        temp_exists = os.path.exists(TEMP_SOURCE_NEWS_FILE)\n",
    "\n",
    "        if not temp_exists or os.path.getsize(TEMP_SOURCE_NEWS_FILE) != source_size:\n",
    "            print(f\"正在从 {SOURCE_NEWS_FILE_ORIGINAL} 复制到 {TEMP_SOURCE_NEWS_FILE} ... (这可能需要几分钟)\")\n",
    "            shutil.copy(SOURCE_NEWS_FILE_ORIGINAL, TEMP_SOURCE_NEWS_FILE)\n",
    "            print(\"复制完成。\")\n",
    "        else:\n",
    "            print(f\"临时源文件 {TEMP_SOURCE_NEWS_FILE} 已存在且大小一致，跳过复制。\")\n",
    "\n",
    "        # [关键] 重定向路径变量，供后续代码块使用\n",
    "        SOURCE_NEWS_FILE = TEMP_SOURCE_NEWS_FILE\n",
    "        CANDIDATES_FILE = TEMP_CANDIDATES_FILE\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理DSW临时文件时出错: {e}\")\n",
    "        # 如果出错，可以选择停止或回退到原始路径\n",
    "        # 这里我们选择停止，因为在DSW上使用原始路径风险很高\n",
    "        raise e\n",
    "\n",
    "print(\"\\n--- 最终文件路径配置 ---\")\n",
    "print(f\"关键词文件 (读): {KEYWORD_JSON_PATH}\")\n",
    "print(f\"源新闻文件 (读): {SOURCE_NEWS_FILE}\")\n",
    "print(f\"候选集文件 (写/读): {CANDIDATES_FILE}\")\n",
    "print(f\"最终结果文件 (写): {FINAL_RESULT_FILE}\")\n",
    "print(f\"拒绝文件 (写): {REJECTED_FILE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 全局处理与积分系统配置 (这部分保持不变) ---\n",
    "# ==============================================================================\n",
    "NEWS_COLUMN = 'CONTENT'\n",
    "CHUNKSIZE = 20000\n",
    "BATCH_SIZE = 500\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "ACCEPTANCE_THRESHOLD = 5.0\n",
    "LEAD_BONUS_TIER_5 = 20.0\n",
    "LEAD_BONUS_TIER_4 = 15.0\n",
    "LEAD_BONUS_TIER_3 = 10.0\n",
    "LEAD_BONUS_TIER_2 = 5.0\n",
    "TIER_5_SCORE = 5.0\n",
    "TIER_4_SCORE = 4.0\n",
    "TIER_3_SCORE = 3.0\n",
    "TIER_2_SCORE = 2.0\n",
    "TIER_1_SCORE = 1.0\n",
    "NEGATION_PENALTY = -3.0\n",
    "HYPOTHETICAL_PENALTY = -2.0\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 启动信息 ---\n",
    "# ==============================================================================\n",
    "print(\"\\n✅ 块 1: 库导入和配置完成。\")\n",
    "print(\"-\" * 30)\n",
    "if TEST_MODE:\n",
    "    print(f\"🚀🚀🚀 运行在【快速测试模式】下！🚀🚀🚀\")\n",
    "else:\n",
    "    print(\"🚢🚢🚢 运行在【完整数据模式】下。🚢🚢🚢\")\n",
    "print(f\"   - 精筛将采用升级版5级积分制，接受阈值为: {ACCEPTANCE_THRESHOLD} 分\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"   - 初筛将使用单进程顺序处理。\")\n",
    "print(f\"   - 精筛阶段将使用 {N_PROCESSES} 个进程进行并行处理。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到运行环境为: 【LOCAL】\n",
      "使用 'local' 模式的相对路径。\n",
      "\n",
      "--- 最终文件路径配置 ---\n",
      "关键词文件 (读): ../data_raw\\china_keywords_collection.json\n",
      "源新闻文件 (读): ../data_raw\\final_merged_all_news.csv\n",
      "候选集文件 (写/读): ../data_processed\\china_news_candidates.csv\n",
      "最终结果文件 (写): ../data_processed\\final_china_news.csv\n",
      "拒绝文件 (写): ../data_processed\\china_news_rejected_articles.csv\n",
      "\n",
      "✅ 块 1: 库导入和配置完成。\n",
      "------------------------------\n",
      "🚢🚢🚢 运行在【完整数据模式】下。🚢🚢🚢\n",
      "   - 精筛将采用升级版5级积分制，接受阈值为: 5.0 分\n",
      "------------------------------\n",
      "   - 初筛将使用单进程顺序处理。\n",
      "   - 精筛阶段将使用 5 个进程进行并行处理。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "61a97e0c381dad4e",
   "metadata": {},
   "source": [
    "# 步骤 2: 初筛准备 - 构建智能正则表达式\n",
    "\n",
    "**目标:** 读取 中国相关关键词 JSON 文件，并执行我们讨论过的所有逻辑来构建一个强大、高效的正则表达式。"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e6d80d362a6a719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:37:32.412543Z",
     "iopub.status.busy": "2025-07-31T02:37:32.411958Z",
     "iopub.status.idle": "2025-07-31T02:37:32.442443Z",
     "shell.execute_reply": "2025-07-31T02:37:32.441899Z",
     "shell.execute_reply.started": "2025-07-31T02:37:32.412510Z"
    },
    "ExecuteTime": {
     "end_time": "2025-07-31T04:30:40.320652Z",
     "start_time": "2025-07-31T04:30:40.312568Z"
    }
   },
   "source": [
    "# --- 构建初筛用的 Flashtext 关键词处理器 ---\n",
    "def build_keyword_processor(json_path):\n",
    "    print(f\"正在从 {json_path} 加载关键词...\")\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            keywords_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 错误: 关键词文件未找到 {json_path}\")\n",
    "        return None, None\n",
    "    print(f\"共加载 {len(keywords_data)} 个关键词对象。\")\n",
    "    all_aliases = set()\n",
    "    for item in keywords_data:\n",
    "        all_aliases.add(item['keyword'])\n",
    "        for alias in item.get('aliases', []):\n",
    "            all_aliases.add(alias)\n",
    "    print(f\"提取出 {len(all_aliases)} 个不重复的关键词/别名。\")\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "    for kw in all_aliases:\n",
    "        keyword_processor.add_keyword(kw)\n",
    "    print(\"✅ 高效关键词处理器 (Flashtext) 构建完成。\")\n",
    "    return keyword_processor, keywords_data\n",
    "\n",
    "keyword_processor, keywords_data = build_keyword_processor(KEYWORD_JSON_PATH)\n",
    "print(\"\\n✅ 块 2: 初筛准备工作完成。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 ../data_raw\\china_keywords_collection.json 加载关键词...\n",
      "共加载 274 个关键词对象。\n",
      "提取出 394 个不重复的关键词/别名。\n",
      "✅ 高效关键词处理器 (Flashtext) 构建完成。\n",
      "\n",
      "✅ 块 2: 初筛准备工作完成。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d3d24e1445bfbb23",
   "metadata": {},
   "source": [
    "# 步骤 3: 执行阶段一 - 调用外部脚本进行快速初筛\n",
    "\n",
    "**目标:** 对大文件进行分块扫描，应用正则表达式，并保存候选集。这将是整个流程中最耗时的部分。"
   ]
  },
  {
   "cell_type": "code",
   "id": "792bd4595d23d8f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:37:32.443707Z",
     "iopub.status.busy": "2025-07-31T02:37:32.443350Z"
    },
    "ExecuteTime": {
     "end_time": "2025-07-31T04:44:49.688470Z",
     "start_time": "2025-07-31T04:30:40.362789Z"
    }
   },
   "source": [
    "# --- 步骤 3: 执行第一阶段 - 大规模流式初筛 (已优化进度条) ---\n",
    "def lightweight_clean(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = html.unescape(text)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    if NEWS_COLUMN not in chunk_df.columns: return pd.DataFrame()\n",
    "    cleaned_series = chunk_df[NEWS_COLUMN].astype(str).apply(lightweight_clean)\n",
    "    mask = cleaned_series.apply(lambda x: len(keyword_processor.extract_keywords(x)) > 0)\n",
    "    return chunk_df[mask]\n",
    "\n",
    "print(\"--- 阶段一: 开始使用单进程进行流式初筛 ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    total_chunks = None\n",
    "    if not TEST_MODE:\n",
    "        print(\"正在计算文件总块数 (优化方式)...\")\n",
    "        try:\n",
    "            first_col_name = pd.read_csv(SOURCE_NEWS_FILE, nrows=0).columns[0]\n",
    "            row_iterator = pd.read_csv(SOURCE_NEWS_FILE, chunksize=100000, usecols=[first_col_name], on_bad_lines='skip', low_memory=False)\n",
    "            num_lines = sum(len(chunk) for chunk in tqdm(row_iterator, desc=\"预扫描文件行数\"))\n",
    "            total_chunks = (num_lines // CHUNKSIZE) + 1 if CHUNKSIZE > 0 else 1\n",
    "            print(f\"文件包含 {num_lines} 有效行, 约 {total_chunks} 个数据块。\")\n",
    "        except Exception as e:\n",
    "            print(f\"快速计算行数失败: {e}. 将不显示总进度。\")\n",
    "\n",
    "    chunk_iterator = pd.read_csv(SOURCE_NEWS_FILE, chunksize=CHUNKSIZE, on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "    if TEST_MODE:\n",
    "        num_test_chunks = (TEST_SAMPLE_SIZE // CHUNKSIZE) + 1 if CHUNKSIZE > 0 else 1\n",
    "        chunk_iterator = islice(chunk_iterator, num_test_chunks)\n",
    "        total_chunks = num_test_chunks\n",
    "        print(f\"🚀 测试模式: 将处理前 {total_chunks} 个数据块 (约 {TEST_SAMPLE_SIZE} 行)。\")\n",
    "\n",
    "    print(\"将使用单进程顺序处理...\")\n",
    "    is_first_chunk = True\n",
    "    total_candidates = 0\n",
    "\n",
    "    for chunk_df in tqdm(chunk_iterator, total=total_chunks, desc=\"顺序初筛中\"):\n",
    "        candidates_df = process_chunk(chunk_df)\n",
    "        if not candidates_df.empty:\n",
    "            total_candidates += len(candidates_df)\n",
    "            if is_first_chunk:\n",
    "                candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='w', encoding='utf-8')\n",
    "                is_first_chunk = False\n",
    "            else:\n",
    "                candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='a', header=False, encoding='utf-8')\n",
    "\n",
    "    end_time_stage1 = time.time()\n",
    "    print(f\"\\n--- 初筛流程执行完毕 ---\\n总共找到 {total_candidates} 篇候选文章，已保存到 {CANDIDATES_FILE}\")\n",
    "    print(f\"阶段一 (顺序初筛) 耗时: {(end_time_stage1 - start_time) / 60:.2f} 分钟。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 原始新闻文件未找到 {SOURCE_NEWS_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 处理过程中发生错误: {e}\")\n",
    "\n",
    "print(\"\\n✅ 块 3: 初筛流程执行完毕。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段一: 开始使用单进程进行流式初筛 ---\n",
      "正在计算文件总块数 (优化方式)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "预扫描文件行数: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72cdb58da242434a9b66abea9c921079"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件包含 1498771 有效行, 约 75 个数据块。\n",
      "将使用单进程顺序处理...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "顺序初筛中:   0%|          | 0/75 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2d1a6cf054f4479be9edcfce99d6371"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 初筛流程执行完毕 ---\n",
      "总共找到 179380 篇候选文章，已保存到 ../data_processed\\china_news_candidates.csv\n",
      "阶段一 (顺序初筛) 耗时: 14.16 分钟。\n",
      "\n",
      "✅ 块 3: 初筛流程执行完毕。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ace73e1406b18418",
   "metadata": {},
   "source": [
    "# 步骤 4: 精筛准备 - 加载模型与定义规则\n",
    "\n",
    "**目标:** 负责加载 spaCy 模型和数据，并定义所有用于精筛的“否决规则”函数。"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb5b2016c4e20800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T04:44:51.387841Z",
     "start_time": "2025-07-31T04:44:49.776622Z"
    }
   },
   "source": [
    "# --- 步骤 4: 准备第二阶段 - 构建多维度相关性评估引擎 (已升级为5级评分) ---\n",
    "print(\"--- 阶段二准备: 加载 spaCy 及构建评估规则 ---\")\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\", \"lemmatizer\", \"tagger\", \"attribute_ruler\"])\n",
    "    print(f\"✅ spaCy 模型 '{nlp.meta['name']}' 的核心组件加载成功。\")\n",
    "except OSError:\n",
    "    print(\"错误: spaCy模型 'en_core_web_lg' 未安装。请运行: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "\n",
    "if nlp:\n",
    "    print(\"正在构建关键词信息查找表...\")\n",
    "    keyword_lookup = {}\n",
    "    for item in keywords_data:\n",
    "        tier = item.get('relevance_tier', 1)\n",
    "        keyword_lookup[item['keyword'].lower()] = {'tier': tier}\n",
    "        for alias in item.get('aliases', []):\n",
    "            keyword_lookup[alias.lower()] = {'tier': tier}\n",
    "    print(f\"✅ 查找表构建完成，包含 {len(keyword_lookup)} 个词条。\")\n",
    "\n",
    "    print(\"正在准备 PhraseMatcher...\")\n",
    "    from spacy.matcher import PhraseMatcher\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keyword_lookup.keys()]\n",
    "    matcher.add(\"ChinaKeywords\", patterns)\n",
    "    print(f\"✅ PhraseMatcher 准备完成，已添加 {len(patterns)} 个模式。\")\n",
    "\n",
    "    def score_keyword_frequency(matches, doc, lookup):\n",
    "        score = 0\n",
    "        for match_id, start, end in matches:\n",
    "            kw = doc[start:end].text.lower()\n",
    "            tier = lookup.get(kw, {}).get('tier', 1)\n",
    "            if tier == 5: score += TIER_5_SCORE\n",
    "            elif tier == 4: score += TIER_4_SCORE\n",
    "            elif tier == 3: score += TIER_3_SCORE\n",
    "            elif tier == 2: score += TIER_2_SCORE\n",
    "            else: score += TIER_1_SCORE\n",
    "        return score\n",
    "\n",
    "    def score_lead_paragraphs_presence(doc, matcher, lookup):\n",
    "        sents = list(doc.sents)\n",
    "        if not sents: return 0, \"\"\n",
    "        lead_sents_count = min(len(sents), 10)\n",
    "        lead_end_token_index = sents[lead_sents_count - 1].end\n",
    "        matches = matcher(doc)\n",
    "        highest_tier_in_lead = 0\n",
    "        found_kw = \"\"\n",
    "        for match_id, start, end in matches:\n",
    "            if start < lead_end_token_index:\n",
    "                kw = doc[start:end].text.lower()\n",
    "                tier = lookup.get(kw, {}).get('tier', 1)\n",
    "                if tier > highest_tier_in_lead:\n",
    "                    highest_tier_in_lead = tier\n",
    "                    found_kw = kw\n",
    "        if highest_tier_in_lead == 5: return LEAD_BONUS_TIER_5, f\"前导加分-T5(+'{found_kw}')\"\n",
    "        if highest_tier_in_lead == 4: return LEAD_BONUS_TIER_4, f\"前导加分-T4(+'{found_kw}')\"\n",
    "        if highest_tier_in_lead == 3: return LEAD_BONUS_TIER_3, f\"前导加分-T3(+'{found_kw}')\"\n",
    "        if highest_tier_in_lead == 2: return LEAD_BONUS_TIER_2, f\"前导加分-T2(+'{found_kw}')\"\n",
    "        return 0, \"\"\n",
    "\n",
    "    def penalize_hypothetical(doc, keywords_in_doc):\n",
    "        penalty = 0\n",
    "        reasons = []\n",
    "        for sent in doc.sents:\n",
    "            clean_sent_start = sent.text.strip().lower()\n",
    "            if clean_sent_start.startswith(('if ', 'unless ', 'what if')):\n",
    "                if any(token.text.lower() in keywords_in_doc for token in sent):\n",
    "                    penalty += HYPOTHETICAL_PENALTY\n",
    "                    reasons.append(f\"假设句扣分: '{sent.text[:50].strip()}...'\")\n",
    "        return penalty, reasons\n",
    "\n",
    "    def penalize_negation(doc, keywords_in_doc):\n",
    "        penalty = 0\n",
    "        reasons = []\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"neg\" and token.head.text.lower() in keywords_in_doc:\n",
    "                penalty += NEGATION_PENALTY\n",
    "                reasons.append(f\"否定扣分: '{token.text} {token.head.text}'\")\n",
    "        return penalty, reasons\n",
    "\n",
    "    print(\"✅ 块 4: 精筛规则和评估引擎准备完成 (已升级为5级评分)。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段二准备: 加载 spaCy 及构建评估规则 ---\n",
      "✅ spaCy 模型 'core_web_lg' 的核心组件加载成功。\n",
      "正在构建关键词信息查找表...\n",
      "✅ 查找表构建完成，包含 392 个词条。\n",
      "正在准备 PhraseMatcher...\n",
      "✅ PhraseMatcher 准备完成，已添加 392 个模式。\n",
      "✅ 块 4: 精筛规则和评估引擎准备完成 (已升级为5级评分)。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "8b93ae6fb597319a",
   "metadata": {},
   "source": [
    "# 步骤 5: 执行阶段二 - 精筛流程\n",
    "\n",
    "**目标:** 加载候选集，应用所有否决规则，然后保存最终结果和被拒绝的文章。"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f5ee5651615939c",
   "metadata": {
    "tags": [],
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-31T04:44:51.404872Z"
    }
   },
   "source": [
    "# --- 步骤 5: 执行第二阶段 - 上下文精筛与产出 (最终版) ---\n",
    "print(\"--- 阶段二: 开始精筛候选集 ---\")\n",
    "start_time_s2 = time.time()\n",
    "try:\n",
    "    read_csv_kwargs = {'low_memory': False}\n",
    "    if TEST_MODE:\n",
    "        read_csv_kwargs['nrows'] = CANDIDATE_SAMPLE_SIZE\n",
    "        print(f\"🚀 测试模式: 最多加载前 {CANDIDATE_SAMPLE_SIZE} 篇候选文章进行精筛。\")\n",
    "    df_candidates = pd.read_csv(CANDIDATES_FILE, **read_csv_kwargs)\n",
    "    print(f\"✅ 成功加载 {len(df_candidates)} 篇候选文章。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 候选文件未找到 {CANDIDATES_FILE}。请先运行块 3。\")\n",
    "    df_candidates = pd.DataFrame()\n",
    "\n",
    "if not df_candidates.empty and nlp:\n",
    "    texts = df_candidates[NEWS_COLUMN].astype(str)\n",
    "    results = []\n",
    "    print(f\"开始使用 {N_PROCESSES} 个进程并行处理文本...\")\n",
    "    docs_content = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESSES)\n",
    "\n",
    "    for doc in tqdm(docs_content, total=len(df_candidates), desc=\"精筛文章\"):\n",
    "        score = 0\n",
    "        score_details = []\n",
    "        matches = matcher(doc)\n",
    "        if not matches:\n",
    "            results.append({'score': 0, 'reason': '精筛阶段未匹配到任何关键词'})\n",
    "            continue\n",
    "        found_keywords_text = {doc[start:end].text.lower() for _, start, end in matches}\n",
    "        lead_score, lead_reason = score_lead_paragraphs_presence(doc, matcher, keyword_lookup)\n",
    "        if lead_score > 0: score_details.append(lead_reason)\n",
    "        freq_score = score_keyword_frequency(matches, doc, keyword_lookup)\n",
    "        if freq_score > 0: score_details.append(f\"关键词频率分: +{freq_score:.2f}\")\n",
    "        score = lead_score + freq_score\n",
    "        hypo_penalty, hypo_reasons = penalize_hypothetical(doc, found_keywords_text)\n",
    "        nega_penalty, nega_reasons = penalize_negation(doc, found_keywords_text)\n",
    "        score += hypo_penalty\n",
    "        score += nega_penalty\n",
    "        score_details.extend(hypo_reasons)\n",
    "        score_details.extend(nega_reasons)\n",
    "        results.append({'score': score, 'reason': ' | '.join(score_details)})\n",
    "\n",
    "    print(\"\\n正在合并精筛结果...\")\n",
    "    df_results = pd.DataFrame(results, index=df_candidates.index)\n",
    "    df_results['keep'] = df_results['score'] >= ACCEPTANCE_THRESHOLD\n",
    "    df_final_with_reasons = pd.concat([df_candidates, df_results], axis=1)\n",
    "    df_accepted = df_final_with_reasons[df_final_with_reasons['keep'] == True].drop(columns=['keep', 'score', 'reason'])\n",
    "    df_rejected = df_final_with_reasons[df_final_with_reasons['keep'] == False].drop(columns=['keep'])\n",
    "\n",
    "    print(\"\\n--- 精筛完成 ---\")\n",
    "    df_accepted.to_csv(FINAL_RESULT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"✅ {len(df_accepted)} 篇最终文章已保存到: {FINAL_RESULT_FILE}\")\n",
    "    df_rejected.to_csv(REJECTED_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"ℹ️ {len(df_rejected)} 篇被拒绝的文章已保存到: {REJECTED_FILE} (供分析)\")\n",
    "\n",
    "    end_time_s2 = time.time()\n",
    "    total_minutes = (end_time_s2 - start_time_s2) / 60\n",
    "    print(f\"阶段二 (精筛) 耗时: {total_minutes:.2f} 分钟。\")\n",
    "else:\n",
    "    print(\"候选集为空或spaCy模型未加载，跳过精筛。\")\n",
    "\n",
    "print(\"\\n✅ 块 5: 精筛流程执行完毕。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段二: 开始精筛候选集 ---\n",
      "✅ 成功加载 179380 篇候选文章。\n",
      "开始使用 5 个进程并行处理文本...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "精筛文章:   0%|          | 0/179380 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "228e6fdd0fd2493d9b04690203f3d40d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
