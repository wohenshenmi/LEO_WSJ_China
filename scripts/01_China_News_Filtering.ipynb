{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4e87dfe8957389",
   "metadata": {},
   "source": [
    "# æ­¥éª¤ 1: å‡†å¤‡å·¥ä½œä¸ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "**ç›®æ ‡:** å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ï¼Œå¹¶è®¾ç½®å¥½æ–‡ä»¶è·¯å¾„å’Œå…¨å±€å˜é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:37:20.285463Z",
     "iopub.status.busy": "2025-07-31T02:37:20.285082Z",
     "iopub.status.idle": "2025-07-31T02:37:32.410184Z",
     "shell.execute_reply": "2025-07-31T02:37:32.409459Z",
     "shell.execute_reply.started": "2025-07-31T02:37:20.285432Z"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-08-01T14:56:03.310467Z",
     "start_time": "2025-08-01T14:56:02.276166Z"
    }
   },
   "source": [
    "# --- æ­¥éª¤ 1: å‡†å¤‡å·¥ä½œä¸ç¯å¢ƒè®¾ç½® ---\n",
    "\n",
    "# --- æ ¸å¿ƒåº“ ---\n",
    "import html\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from itertools import islice # ç”¨äºåœ¨æµ‹è¯•æ¨¡å¼ä¸‹æˆªå–æ•°æ®æµ\n",
    "import shutil # ç”¨äºåœ¨æœåŠ¡å™¨æ¨¡å¼ä¸‹é«˜æ•ˆå¤åˆ¶æ–‡ä»¶\n",
    "\n",
    "# --- æ•°æ®å¤„ç†ä¸NLPåº“ ---\n",
    "import pandas as pd\n",
    "import psutil # ç”¨äºæ™ºèƒ½æ£€æµ‹CPUæ ¸å¿ƒæ•°\n",
    "import spacy\n",
    "from flashtext import KeywordProcessor # ç”¨äºé«˜é€Ÿåˆç­›\n",
    "from tqdm.auto import tqdm # ç”¨äºæ˜¾ç¤ºç¾è§‚çš„è¿›åº¦æ¡\n",
    "\n",
    "# ==============================================================================\n",
    "# --- æ ¸å¿ƒé…ç½®åŒº (é€šå¸¸æ‚¨åªéœ€è¦ä¿®æ”¹è¿™éƒ¨åˆ†) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. ç¯å¢ƒé…ç½®\n",
    "# ç›®çš„: è‡ªåŠ¨é€‚é…ä¸åŒè¿è¡Œç¯å¢ƒï¼Œè§£å†³æœåŠ¡å™¨ä¸Šå› ç½‘ç»œå­˜å‚¨(NFS)å¯¼è‡´çš„I/Oé”™è¯¯\n",
    "#       æˆ–æœ¬åœ°ä¸æœåŠ¡å™¨ç›®å½•ç»“æ„ä¸ä¸€è‡´çš„é—®é¢˜ã€‚\n",
    "# é€‰é¡¹:\n",
    "#   'local': åœ¨æ‚¨è‡ªå·±çš„ç”µè„‘ä¸Šè¿è¡Œã€‚ä»£ç å°†ä½¿ç”¨ç›¸å¯¹è·¯å¾„ (å¦‚ ../data_raw)ã€‚\n",
    "#   'dsw':   åœ¨é˜¿é‡Œäº‘DSWæˆ–å…¶ä»–è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚ä»£ç å°†ä½¿ç”¨ç»å¯¹è·¯å¾„\n",
    "#            (å¦‚ /mnt/data/data_raw)ï¼Œå¹¶æ™ºèƒ½åœ°å°†I/Oå¯†é›†å‹æ“ä½œé‡å®šå‘\n",
    "#            åˆ°æœåŠ¡å™¨æœ¬åœ°çš„ /tmp ç›®å½•ï¼Œä»¥è·å¾—æœ€ä½³ç¨³å®šæ€§å’Œé€Ÿåº¦ã€‚\n",
    "RUNNING_ENV = 'local'\n",
    "\n",
    "# 2. å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¼€å…³\n",
    "# ç›®çš„: ç”¨äºå¿«é€Ÿè°ƒè¯•ä»£ç é€»è¾‘ï¼Œé¿å…å¤„ç†å…¨é‡æ•°æ®è€—è´¹å¤§é‡æ—¶é—´ã€‚\n",
    "# æ•ˆæœ:\n",
    "#   True:  åªå¤„ç†å¼€å¤´æŒ‡å®šæ•°é‡(TEST_SAMPLE_SIZE)çš„åŸå§‹æ–°é—»å’Œå€™é€‰æ–°é—»ï¼Œ\n",
    "#          å¯ä»¥å¿«é€ŸéªŒè¯æ•´ä¸ªæµç¨‹æ˜¯å¦é€šç•…ã€‚\n",
    "#   False: å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œç”¨äºæ­£å¼äº§å‡ºæœ€ç»ˆç»“æœã€‚\n",
    "TEST_MODE = False\n",
    "TEST_SAMPLE_SIZE = 50000        # æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œä»æºæ–‡ä»¶è¯»å–çš„æœ€å¤§è¡Œæ•°\n",
    "CANDIDATE_SAMPLE_SIZE = 2000    # æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œä»å€™é€‰é›†æ–‡ä»¶è¯»å–çš„æœ€å¤§è¡Œæ•°\n",
    "\n",
    "# ==============================================================================\n",
    "# --- è·¯å¾„æ™ºèƒ½ç®¡ç† (æ ¹æ®ä¸Šé¢çš„ RUNNING_ENV è‡ªåŠ¨é…ç½®ï¼Œé€šå¸¸æ— éœ€ä¿®æ”¹) ---\n",
    "# ==============================================================================\n",
    "print(f\"æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€{RUNNING_ENV.upper()}ã€‘\")\n",
    "TEMP_DIR = '/tmp' # å®šä¹‰æœåŠ¡å™¨çš„æœ¬åœ°ä¸´æ—¶ç›®å½•\n",
    "\n",
    "# æ ¹æ®ç¯å¢ƒå®šä¹‰æ•°æ®æ ¹è·¯å¾„\n",
    "if RUNNING_ENV == 'local':\n",
    "    # æœ¬åœ°æ¨¡å¼: ä½¿ç”¨ç›¸å¯¹äºå½“å‰è„šæœ¬(é€šå¸¸åœ¨ 'scripts' ç›®å½•)çš„è·¯å¾„\n",
    "    print(\"ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_RAW_PATH = '../data_raw'\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    # DSWæ¨¡å¼: ä½¿ç”¨æœåŠ¡å™¨ä¸ŠæŒ‚è½½å­˜å‚¨çš„ç»å¯¹è·¯å¾„\n",
    "    print(\"ä½¿ç”¨ 'dsw' æ¨¡å¼çš„ç»å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_RAW_PATH = '/mnt/data/data_raw'\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "else:\n",
    "    # é”™è¯¯å¤„ç†ï¼Œé˜²æ­¢é…ç½®é”™è¯¯\n",
    "    raise ValueError(f\"æœªçŸ¥çš„ RUNNING_ENV: '{RUNNING_ENV}'. è¯·é€‰æ‹© 'local' æˆ– 'dsw'ã€‚\")\n",
    "\n",
    "# ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„åŸºç¡€è·¯å¾„æ¥æ„å»ºâ€œåŸå§‹â€æˆ–â€œæœ€ç»ˆâ€çš„æ–‡ä»¶è·¯å¾„\n",
    "KEYWORD_JSON_PATH = os.path.join(BASE_DATA_RAW_PATH, 'china_keywords_collection.json')\n",
    "SOURCE_NEWS_FILE_ORIGINAL = os.path.join(BASE_DATA_RAW_PATH, 'final_merged_all_news.csv')\n",
    "CANDIDATES_FILE_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_candidates.csv')\n",
    "FINAL_RESULT_FILE = os.path.join(BASE_DATA_PROCESSED_PATH, 'final_china_news.csv')\n",
    "REJECTED_FILE = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_rejected_articles.csv')\n",
    "\n",
    "# åˆå§‹åŒ–å°†è¦åœ¨åç»­æµç¨‹ä¸­å®é™…ä½¿ç”¨çš„è·¯å¾„å˜é‡ï¼Œé»˜è®¤æŒ‡å‘åŸå§‹è·¯å¾„\n",
    "SOURCE_NEWS_FILE = SOURCE_NEWS_FILE_ORIGINAL\n",
    "CANDIDATES_FILE = CANDIDATES_FILE_ORIGINAL\n",
    "\n",
    "# å¦‚æœæ˜¯æœåŠ¡å™¨ç¯å¢ƒï¼Œåˆ™æ‰§è¡ŒI/Oä¼˜åŒ–ï¼šé‡å®šå‘é«˜è´Ÿè½½çš„è¯»å†™è·¯å¾„åˆ° /tmp\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW ç¯å¢ƒæ¨¡å¼å·²æ¿€æ´»ï¼Œä¸ºé¿å…I/Oé”™è¯¯ï¼Œå°†ä½¿ç”¨æœ¬åœ°ä¸´æ—¶ç›®å½• /tmp ...\")\n",
    "    TEMP_SOURCE_NEWS_FILE = os.path.join(TEMP_DIR, 'final_merged_all_news.csv')\n",
    "    TEMP_CANDIDATES_FILE = os.path.join(TEMP_DIR, 'china_news_candidates.csv')\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(SOURCE_NEWS_FILE_ORIGINAL):\n",
    "             raise FileNotFoundError(f\"åœ¨DSWçš„æºè·¯å¾„ {SOURCE_NEWS_FILE_ORIGINAL} æœªæ‰¾åˆ°æ–‡ä»¶ï¼\")\n",
    "\n",
    "        source_size = os.path.getsize(SOURCE_NEWS_FILE_ORIGINAL)\n",
    "        temp_exists = os.path.exists(TEMP_SOURCE_NEWS_FILE)\n",
    "\n",
    "        # æ™ºèƒ½å¤åˆ¶ï¼šä»…åœ¨ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨æˆ–å¤§å°ä¸æºæ–‡ä»¶ä¸ä¸€è‡´æ—¶æ‰æ‰§è¡Œå¤åˆ¶æ“ä½œ\n",
    "        if not temp_exists or os.path.getsize(TEMP_SOURCE_NEWS_FILE) != source_size:\n",
    "            print(f\"æ­£åœ¨ä» {SOURCE_NEWS_FILE_ORIGINAL} å¤åˆ¶åˆ° {TEMP_SOURCE_NEWS_FILE} ... (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)\")\n",
    "            shutil.copy(SOURCE_NEWS_FILE_ORIGINAL, TEMP_SOURCE_NEWS_FILE)\n",
    "            print(\"å¤åˆ¶å®Œæˆã€‚\")\n",
    "        else:\n",
    "            print(f\"ä¸´æ—¶æºæ–‡ä»¶ {TEMP_SOURCE_NEWS_FILE} å·²å­˜åœ¨ä¸”å¤§å°ä¸€è‡´ï¼Œè·³è¿‡å¤åˆ¶ã€‚\")\n",
    "\n",
    "        # [å…³é”®] é‡å®šå‘è·¯å¾„å˜é‡ï¼Œè®©åç»­ä»£ç å—é€æ˜åœ°ä½¿ç”¨/tmpä¸‹çš„æ–‡ä»¶\n",
    "        SOURCE_NEWS_FILE = TEMP_SOURCE_NEWS_FILE\n",
    "        CANDIDATES_FILE = TEMP_CANDIDATES_FILE\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç†DSWä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "        # å¦‚æœå‡ºé”™ï¼Œåœæ­¢æ‰§è¡Œï¼Œå› ä¸ºåœ¨DSWä¸Šä½¿ç”¨åŸå§‹ç½‘ç»œè·¯å¾„é£é™©å¾ˆé«˜\n",
    "        raise e\n",
    "\n",
    "print(\"\\n--- æœ€ç»ˆæ–‡ä»¶è·¯å¾„é…ç½® ---\")\n",
    "print(f\"å…³é”®è¯æ–‡ä»¶ (è¯»): {KEYWORD_JSON_PATH}\")\n",
    "print(f\"æºæ–°é—»æ–‡ä»¶ (è¯»): {SOURCE_NEWS_FILE}\")\n",
    "print(f\"å€™é€‰é›†æ–‡ä»¶ (å†™/è¯»): {CANDIDATES_FILE}\")\n",
    "print(f\"æœ€ç»ˆç»“æœæ–‡ä»¶ (å†™): {FINAL_RESULT_FILE}\")\n",
    "print(f\"æ‹’ç»æ–‡ä»¶ (å†™): {REJECTED_FILE}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- å…¨å±€å¤„ç†ä¸ç§¯åˆ†ç³»ç»Ÿé…ç½® ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- å…¨å±€å¤„ç†å‚æ•° ---\n",
    "NEWS_COLUMN = 'CONTENT' # å®šä¹‰æ–°é—»å†…å®¹æ‰€åœ¨çš„åˆ—å\n",
    "CHUNKSIZE = 20000       # å®šä¹‰åˆç­›æ—¶æ¯ä¸ªæ•°æ®å—çš„å¤§å°(è¡Œæ•°)\n",
    "BATCH_SIZE = 500        # å®šä¹‰spaCyå¹¶è¡Œå¤„ç†æ—¶æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°(æ–‡æ¡£æ•°)\n",
    "\n",
    "# --- æ™ºèƒ½é…ç½®å¹¶è¡Œå‚æ•° ---\n",
    "# ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ç‰©ç†CPUæ ¸å¿ƒï¼Œå¹¶ä¿ç•™ä¸€ä¸ªæ ¸å¿ƒç»™æ“ä½œç³»ç»Ÿï¼Œä»¥é¿å…ç³»ç»Ÿå¡é¡¿\n",
    "# logical=False åœ¨ç‰©ç†æœºæˆ–é«˜æ€§èƒ½è™šæ‹Ÿæœºä¸Šæ›´ç¨³å¥ï¼Œé¿å…è¶…çº¿ç¨‹å¹²æ‰°\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8) # æœ€å¤šä½¿ç”¨8ä¸ªè¿›ç¨‹ï¼Œé˜²æ­¢èµ„æºè¿‡åº¦æ¶ˆè€—\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "# --- ç²¾ç­›ç§¯åˆ†ç³»ç»Ÿé…ç½® (æ ¸å¿ƒè°ƒä¼˜åŒºåŸŸ) ---\n",
    "# è°ƒä¼˜æ€è·¯: é€šè¿‡é«˜é¢çš„\"å‰å¯¼å¥–åŠ±\"å’Œ\"å…³é”®è¯é¢‘ç‡åˆ†\"ç¡®ä¿é«˜ä»·å€¼æ–‡ç« èƒ½è·å¾—è¿œè¶…\n",
    "#           é˜ˆå€¼çš„åˆ†æ•°ï¼Œä½¿å…¶è¶³ä»¥æŠµå¾¡å°‘é‡è´Ÿé¢ä¿¡å·ï¼ˆå¦‚å‡è®¾å¥ï¼‰çš„æ‰£åˆ†ã€‚\n",
    "\n",
    "# 1. æœ€ç»ˆå†³ç­–é˜ˆå€¼\n",
    "# æè¿°: ä¸€ç¯‡æ–‡ç« çš„æœ€ç»ˆæ€»åˆ†å¿…é¡»é«˜äºæ­¤é˜ˆå€¼æ‰ä¼šè¢«æ¥å—ã€‚è¿™æ˜¯è°ƒèŠ‚ç­›é€‰å™¨\n",
    "#       å®½æ¾åº¦çš„æœ€ä¸»è¦æ æ†ã€‚è°ƒä½ä¼šå¢åŠ é€šè¿‡ç‡ï¼Œè°ƒé«˜åˆ™æ›´ä¸¥æ ¼ã€‚\n",
    "ACCEPTANCE_THRESHOLD = 5.0\n",
    "\n",
    "# 2. æ­£å‘åŠ åˆ†é¡¹ (å¥–åŠ±ç›¸å…³ä¿¡å·)\n",
    "# æè¿°: å¥–åŠ±é‚£äº›åœ¨æ–‡ç« å¼€å¤´(å‰10å¥)å°±ç‚¹æ˜ä¸»é¢˜çš„æ–‡ç« ã€‚\n",
    "#       æ ¹æ®å¼€å¤´å‡ºç°çš„æœ€é«˜çº§åˆ«å…³é”®è¯ç»™äºˆä¸åŒå¥–åŠ±ã€‚\n",
    "LEAD_BONUS_TIER_5 = 20.0 # T5è¯åœ¨å¼€å¤´ï¼Œå‡ ä¹æ˜¯å†³å®šæ€§çš„\n",
    "LEAD_BONUS_TIER_4 = 15.0\n",
    "LEAD_BONUS_TIER_3 = 10.0\n",
    "LEAD_BONUS_TIER_2 = 5.0  # T2è¯åœ¨å¼€å¤´ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå¼ºä¿¡å·\n",
    "\n",
    "# æè¿°: å¥–åŠ±é‚£äº›é€šç¯‡éƒ½åœ¨è®¨è®ºç›¸å…³è¯é¢˜çš„æ–‡ç« ï¼Œè¡¡é‡å…¶â€œç›¸å…³æ·±åº¦â€ã€‚\n",
    "#       é«˜ç­‰çº§çš„è¯ç»™äºˆä¸æˆæ¯”ä¾‹çš„é«˜åˆ†ï¼Œä»¥ä½“ç°å…¶é‡è¦æ€§ã€‚\n",
    "TIER_5_SCORE = 5.0       # ä¾‹å¦‚:\n",
    "TIER_4_SCORE = 4.0       # ä¾‹å¦‚:\n",
    "TIER_3_SCORE = 3.0       # ä¾‹å¦‚:\n",
    "TIER_2_SCORE = 2.0       # ä¾‹å¦‚:\n",
    "TIER_1_SCORE = 1.0       # ä¾‹å¦‚:\n",
    "\n",
    "# 3. è´Ÿå‘æ‰£åˆ†é¡¹ (æƒ©ç½šé£é™©ä¿¡å·)\n",
    "# æè¿°: æƒ©ç½šé‚£äº›å¯èƒ½å¼•å…¥ä¸ç›¸å…³ä¿¡æ¯çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "NEGATION_PENALTY = -3.0     # ä¾‹å¦‚: \"not about China\"\n",
    "HYPOTHETICAL_PENALTY = -2.0 # ä¾‹å¦‚: \"if China were to...\"\n",
    "\n",
    "# ==============================================================================\n",
    "# --- å¯åŠ¨ä¿¡æ¯ ---\n",
    "# ==============================================================================\n",
    "print(\"\\nâœ… å— 1: åº“å¯¼å…¥å’Œé…ç½®å®Œæˆã€‚\")\n",
    "print(\"-\" * 30)\n",
    "if TEST_MODE:\n",
    "    print(f\"ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼ğŸš€ğŸš€ğŸš€\")\n",
    "else:\n",
    "    print(\"ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ã€‚ğŸš¢ğŸš¢ğŸš¢\")\n",
    "print(f\"   - ç²¾ç­›å°†é‡‡ç”¨å‡çº§ç‰ˆ5çº§ç§¯åˆ†åˆ¶ï¼Œæ¥å—é˜ˆå€¼ä¸º: {ACCEPTANCE_THRESHOLD} åˆ†\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"   - åˆç­›å°†ä½¿ç”¨å•è¿›ç¨‹é¡ºåºå¤„ç†ã€‚\")\n",
    "print(f\"   - ç²¾ç­›é˜¶æ®µå°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€LOCALã€‘\n",
      "ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\n",
      "\n",
      "--- æœ€ç»ˆæ–‡ä»¶è·¯å¾„é…ç½® ---\n",
      "å…³é”®è¯æ–‡ä»¶ (è¯»): ../data_raw\\china_keywords_collection.json\n",
      "æºæ–°é—»æ–‡ä»¶ (è¯»): ../data_raw\\final_merged_all_news.csv\n",
      "å€™é€‰é›†æ–‡ä»¶ (å†™/è¯»): ../data_processed\\china_news_candidates.csv\n",
      "æœ€ç»ˆç»“æœæ–‡ä»¶ (å†™): ../data_processed\\final_china_news.csv\n",
      "æ‹’ç»æ–‡ä»¶ (å†™): ../data_processed\\china_news_rejected_articles.csv\n",
      "\n",
      "âœ… å— 1: åº“å¯¼å…¥å’Œé…ç½®å®Œæˆã€‚\n",
      "------------------------------\n",
      "ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ã€‚ğŸš¢ğŸš¢ğŸš¢\n",
      "   - ç²¾ç­›å°†é‡‡ç”¨å‡çº§ç‰ˆ5çº§ç§¯åˆ†åˆ¶ï¼Œæ¥å—é˜ˆå€¼ä¸º: 5.0 åˆ†\n",
      "------------------------------\n",
      "   - åˆç­›å°†ä½¿ç”¨å•è¿›ç¨‹é¡ºåºå¤„ç†ã€‚\n",
      "   - ç²¾ç­›é˜¶æ®µå°†ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "61a97e0c381dad4e",
   "metadata": {},
   "source": [
    "# æ­¥éª¤ 2: åˆç­›å‡†å¤‡ - æ„å»ºæ™ºèƒ½æ­£åˆ™è¡¨è¾¾å¼\n",
    "\n",
    "**ç›®æ ‡:** è¯»å– ä¸­å›½ç›¸å…³å…³é”®è¯ JSON æ–‡ä»¶ï¼Œå¹¶æ‰§è¡Œæˆ‘ä»¬è®¨è®ºè¿‡çš„æ‰€æœ‰é€»è¾‘æ¥æ„å»ºä¸€ä¸ªå¼ºå¤§ã€é«˜æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e6d80d362a6a719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:37:32.412543Z",
     "iopub.status.busy": "2025-07-31T02:37:32.411958Z",
     "iopub.status.idle": "2025-07-31T02:37:32.442443Z",
     "shell.execute_reply": "2025-07-31T02:37:32.441899Z",
     "shell.execute_reply.started": "2025-07-31T02:37:32.412510Z"
    },
    "ExecuteTime": {
     "end_time": "2025-08-01T14:56:03.473609Z",
     "start_time": "2025-08-01T14:56:03.464912Z"
    }
   },
   "source": [
    "# --- æ„å»ºåˆç­›ç”¨çš„ Flashtext å…³é”®è¯å¤„ç†å™¨ ---\n",
    "def build_keyword_processor(json_path):\n",
    "    print(f\"æ­£åœ¨ä» {json_path} åŠ è½½å…³é”®è¯...\")\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            keywords_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ é”™è¯¯: å…³é”®è¯æ–‡ä»¶æœªæ‰¾åˆ° {json_path}\")\n",
    "        return None, None\n",
    "    print(f\"å…±åŠ è½½ {len(keywords_data)} ä¸ªå…³é”®è¯å¯¹è±¡ã€‚\")\n",
    "    all_aliases = set()\n",
    "    for item in keywords_data:\n",
    "        all_aliases.add(item['keyword'])\n",
    "        for alias in item.get('aliases', []):\n",
    "            all_aliases.add(alias)\n",
    "    print(f\"æå–å‡º {len(all_aliases)} ä¸ªä¸é‡å¤çš„å…³é”®è¯/åˆ«åã€‚\")\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "    for kw in all_aliases:\n",
    "        keyword_processor.add_keyword(kw)\n",
    "    print(\"âœ… é«˜æ•ˆå…³é”®è¯å¤„ç†å™¨ (Flashtext) æ„å»ºå®Œæˆã€‚\")\n",
    "    return keyword_processor, keywords_data\n",
    "\n",
    "keyword_processor, keywords_data = build_keyword_processor(KEYWORD_JSON_PATH)\n",
    "print(\"\\nâœ… å— 2: åˆç­›å‡†å¤‡å·¥ä½œå®Œæˆã€‚\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä» ../data_raw\\china_keywords_collection.json åŠ è½½å…³é”®è¯...\n",
      "å…±åŠ è½½ 274 ä¸ªå…³é”®è¯å¯¹è±¡ã€‚\n",
      "æå–å‡º 394 ä¸ªä¸é‡å¤çš„å…³é”®è¯/åˆ«åã€‚\n",
      "âœ… é«˜æ•ˆå…³é”®è¯å¤„ç†å™¨ (Flashtext) æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "âœ… å— 2: åˆç­›å‡†å¤‡å·¥ä½œå®Œæˆã€‚\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d3d24e1445bfbb23",
   "metadata": {},
   "source": [
    "# æ­¥éª¤ 3: æ‰§è¡Œé˜¶æ®µä¸€ - è°ƒç”¨å¤–éƒ¨è„šæœ¬è¿›è¡Œå¿«é€Ÿåˆç­›\n",
    "\n",
    "**ç›®æ ‡:** å¯¹å¤§æ–‡ä»¶è¿›è¡Œåˆ†å—æ‰«æï¼Œåº”ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¹¶ä¿å­˜å€™é€‰é›†ã€‚è¿™å°†æ˜¯æ•´ä¸ªæµç¨‹ä¸­æœ€è€—æ—¶çš„éƒ¨åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "id": "792bd4595d23d8f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T02:37:32.443707Z",
     "iopub.status.busy": "2025-07-31T02:37:32.443350Z"
    },
    "ExecuteTime": {
     "end_time": "2025-08-01T14:57:01.356474Z",
     "start_time": "2025-08-01T14:56:03.500290Z"
    }
   },
   "source": [
    "# --- æ­¥éª¤ 3: æ‰§è¡Œç¬¬ä¸€é˜¶æ®µ - å¤§è§„æ¨¡æµå¼åˆç­› (å·²ä¼˜åŒ–è¿›åº¦æ¡) ---\n",
    "def lightweight_clean(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = html.unescape(text)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    if NEWS_COLUMN not in chunk_df.columns: return pd.DataFrame()\n",
    "    cleaned_series = chunk_df[NEWS_COLUMN].astype(str).apply(lightweight_clean)\n",
    "    mask = cleaned_series.apply(lambda x: len(keyword_processor.extract_keywords(x)) > 0)\n",
    "    return chunk_df[mask]\n",
    "\n",
    "print(\"--- é˜¶æ®µä¸€: å¼€å§‹ä½¿ç”¨å•è¿›ç¨‹è¿›è¡Œæµå¼åˆç­› ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    total_chunks = None\n",
    "    if not TEST_MODE:\n",
    "        print(\"æ­£åœ¨è®¡ç®—æ–‡ä»¶æ€»å—æ•° (ä¼˜åŒ–æ–¹å¼)...\")\n",
    "        try:\n",
    "            first_col_name = pd.read_csv(SOURCE_NEWS_FILE, nrows=0).columns[0]\n",
    "            row_iterator = pd.read_csv(SOURCE_NEWS_FILE, chunksize=100000, usecols=[first_col_name], on_bad_lines='skip', low_memory=False)\n",
    "            num_lines = sum(len(chunk) for chunk in tqdm(row_iterator, desc=\"é¢„æ‰«ææ–‡ä»¶è¡Œæ•°\"))\n",
    "            total_chunks = (num_lines // CHUNKSIZE) + 1 if CHUNKSIZE > 0 else 1\n",
    "            print(f\"æ–‡ä»¶åŒ…å« {num_lines} æœ‰æ•ˆè¡Œ, çº¦ {total_chunks} ä¸ªæ•°æ®å—ã€‚\")\n",
    "        except Exception as e:\n",
    "            print(f\"å¿«é€Ÿè®¡ç®—è¡Œæ•°å¤±è´¥: {e}. å°†ä¸æ˜¾ç¤ºæ€»è¿›åº¦ã€‚\")\n",
    "\n",
    "    chunk_iterator = pd.read_csv(SOURCE_NEWS_FILE, chunksize=CHUNKSIZE, on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "    if TEST_MODE:\n",
    "        num_test_chunks = (TEST_SAMPLE_SIZE // CHUNKSIZE) + 1 if CHUNKSIZE > 0 else 1\n",
    "        chunk_iterator = islice(chunk_iterator, num_test_chunks)\n",
    "        total_chunks = num_test_chunks\n",
    "        print(f\"ğŸš€ æµ‹è¯•æ¨¡å¼: å°†å¤„ç†å‰ {total_chunks} ä¸ªæ•°æ®å— (çº¦ {TEST_SAMPLE_SIZE} è¡Œ)ã€‚\")\n",
    "\n",
    "    print(\"å°†ä½¿ç”¨å•è¿›ç¨‹é¡ºåºå¤„ç†...\")\n",
    "    is_first_chunk = True\n",
    "    total_candidates = 0\n",
    "\n",
    "    for chunk_df in tqdm(chunk_iterator, total=total_chunks, desc=\"é¡ºåºåˆç­›ä¸­\"):\n",
    "        candidates_df = process_chunk(chunk_df)\n",
    "        if not candidates_df.empty:\n",
    "            total_candidates += len(candidates_df)\n",
    "            if is_first_chunk:\n",
    "                candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='w', encoding='utf-8')\n",
    "                is_first_chunk = False\n",
    "            else:\n",
    "                candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='a', header=False, encoding='utf-8')\n",
    "\n",
    "    end_time_stage1 = time.time()\n",
    "    print(f\"\\n--- åˆç­›æµç¨‹æ‰§è¡Œå®Œæ¯• ---\\næ€»å…±æ‰¾åˆ° {total_candidates} ç¯‡å€™é€‰æ–‡ç« ï¼Œå·²ä¿å­˜åˆ° {CANDIDATES_FILE}\")\n",
    "    print(f\"é˜¶æ®µä¸€ (é¡ºåºåˆç­›) è€—æ—¶: {(end_time_stage1 - start_time) / 60:.2f} åˆ†é’Ÿã€‚\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯: åŸå§‹æ–°é—»æ–‡ä»¶æœªæ‰¾åˆ° {SOURCE_NEWS_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "\n",
    "print(\"\\nâœ… å— 3: åˆç­›æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- é˜¶æ®µä¸€: å¼€å§‹ä½¿ç”¨å•è¿›ç¨‹è¿›è¡Œæµå¼åˆç­› ---\n",
      "æ­£åœ¨è®¡ç®—æ–‡ä»¶æ€»å—æ•° (ä¼˜åŒ–æ–¹å¼)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "é¢„æ‰«ææ–‡ä»¶è¡Œæ•°: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d56ec91f3d7a4212822921238af2f903"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶åŒ…å« 1560878 æœ‰æ•ˆè¡Œ, çº¦ 79 ä¸ªæ•°æ®å—ã€‚\n",
      "å°†ä½¿ç”¨å•è¿›ç¨‹é¡ºåºå¤„ç†...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "é¡ºåºåˆç­›ä¸­:   0%|          | 0/79 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a59062e639e2438aabb07bf17f92df63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 46\u001B[39m\n\u001B[32m     43\u001B[39m total_candidates = \u001B[32m0\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m chunk_df \u001B[38;5;129;01min\u001B[39;00m tqdm(chunk_iterator, total=total_chunks, desc=\u001B[33m\"\u001B[39m\u001B[33mé¡ºåºåˆç­›ä¸­\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m     candidates_df = process_chunk(chunk_df)\n\u001B[32m     47\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m candidates_df.empty:\n\u001B[32m     48\u001B[39m         total_candidates += \u001B[38;5;28mlen\u001B[39m(candidates_df)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mprocess_chunk\u001B[39m\u001B[34m(chunk_df)\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m NEWS_COLUMN \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m chunk_df.columns: \u001B[38;5;28;01mreturn\u001B[39;00m pd.DataFrame()\n\u001B[32m     14\u001B[39m cleaned_series = chunk_df[NEWS_COLUMN].astype(\u001B[38;5;28mstr\u001B[39m).apply(lightweight_clean)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m mask = cleaned_series.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlen\u001B[39m(keyword_processor.extract_keywords(x)) > \u001B[32m0\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m chunk_df[mask]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4800\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4801\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4802\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4807\u001B[39m     **kwargs,\n\u001B[32m   4808\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4809\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4810\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4811\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4926\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4927\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4928\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m SeriesApply(\n\u001B[32m   4929\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4930\u001B[39m         func,\n\u001B[32m   4931\u001B[39m         convert_dtype=convert_dtype,\n\u001B[32m   4932\u001B[39m         by_row=by_row,\n\u001B[32m   4933\u001B[39m         args=args,\n\u001B[32m   4934\u001B[39m         kwargs=kwargs,\n\u001B[32m-> \u001B[39m\u001B[32m4935\u001B[39m     ).apply()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1419\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1421\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1422\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_standard()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1496\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1497\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1498\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1499\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1500\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1501\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1502\u001B[39m mapped = obj._map_values(\n\u001B[32m   1503\u001B[39m     mapper=curried, na_action=action, convert=\u001B[38;5;28mself\u001B[39m.convert_dtype\n\u001B[32m   1504\u001B[39m )\n\u001B[32m   1506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1507\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1508\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1509\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\pandas\\core\\base.py:925\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    922\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    923\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer(values, mapper, convert=convert)\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2999\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mprocess_chunk.<locals>.<lambda>\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m NEWS_COLUMN \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m chunk_df.columns: \u001B[38;5;28;01mreturn\u001B[39;00m pd.DataFrame()\n\u001B[32m     14\u001B[39m cleaned_series = chunk_df[NEWS_COLUMN].astype(\u001B[38;5;28mstr\u001B[39m).apply(lightweight_clean)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m mask = cleaned_series.apply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlen\u001B[39m(keyword_processor.extract_keywords(x)) > \u001B[32m0\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m chunk_df[mask]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\flashtext\\keyword.py:482\u001B[39m, in \u001B[36mKeywordProcessor.extract_keywords\u001B[39m\u001B[34m(self, sentence, span_info)\u001B[39m\n\u001B[32m    480\u001B[39m idx = \u001B[32m0\u001B[39m\n\u001B[32m    481\u001B[39m sentence_len = \u001B[38;5;28mlen\u001B[39m(sentence)\n\u001B[32m--> \u001B[39m\u001B[32m482\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m idx < sentence_len:\n\u001B[32m    483\u001B[39m     char = sentence[idx]\n\u001B[32m    484\u001B[39m     \u001B[38;5;66;03m# when we reach a character that might denote word end\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ace73e1406b18418",
   "metadata": {},
   "source": [
    "# æ­¥éª¤ 4: ç²¾ç­›å‡†å¤‡ - åŠ è½½æ¨¡å‹ä¸å®šä¹‰è§„åˆ™\n",
    "\n",
    "**ç›®æ ‡:** è´Ÿè´£åŠ è½½ spaCy æ¨¡å‹å’Œæ•°æ®ï¼Œå¹¶å®šä¹‰æ‰€æœ‰ç”¨äºç²¾ç­›çš„â€œå¦å†³è§„åˆ™â€å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb5b2016c4e20800",
   "metadata": {},
   "source": [
    "# --- æ­¥éª¤ 4: å‡†å¤‡ç¬¬äºŒé˜¶æ®µ - æ„å»ºå¤šç»´åº¦ç›¸å…³æ€§è¯„ä¼°å¼•æ“ (å·²å‡çº§ä¸º5çº§è¯„åˆ†) ---\n",
    "print(\"--- é˜¶æ®µäºŒå‡†å¤‡: åŠ è½½ spaCy åŠæ„å»ºè¯„ä¼°è§„åˆ™ ---\")\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\", \"lemmatizer\", \"tagger\", \"attribute_ruler\"])\n",
    "    print(f\"âœ… spaCy æ¨¡å‹ '{nlp.meta['name']}' çš„æ ¸å¿ƒç»„ä»¶åŠ è½½æˆåŠŸã€‚\")\n",
    "except OSError:\n",
    "    print(\"é”™è¯¯: spaCyæ¨¡å‹ 'en_core_web_lg' æœªå®‰è£…ã€‚è¯·è¿è¡Œ: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "\n",
    "if nlp:\n",
    "    print(\"æ­£åœ¨æ„å»ºå…³é”®è¯ä¿¡æ¯æŸ¥æ‰¾è¡¨...\")\n",
    "    keyword_lookup = {}\n",
    "    for item in keywords_data:\n",
    "        tier = item.get('relevance_tier', 1)\n",
    "        keyword_lookup[item['keyword'].lower()] = {'tier': tier}\n",
    "        for alias in item.get('aliases', []):\n",
    "            keyword_lookup[alias.lower()] = {'tier': tier}\n",
    "    print(f\"âœ… æŸ¥æ‰¾è¡¨æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(keyword_lookup)} ä¸ªè¯æ¡ã€‚\")\n",
    "\n",
    "    print(\"æ­£åœ¨å‡†å¤‡ PhraseMatcher...\")\n",
    "    from spacy.matcher import PhraseMatcher\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = [nlp.make_doc(text) for text in keyword_lookup.keys()]\n",
    "    matcher.add(\"ChinaKeywords\", patterns)\n",
    "    print(f\"âœ… PhraseMatcher å‡†å¤‡å®Œæˆï¼Œå·²æ·»åŠ  {len(patterns)} ä¸ªæ¨¡å¼ã€‚\")\n",
    "\n",
    "    def score_keyword_frequency(matches, doc, lookup):\n",
    "        score = 0\n",
    "        for match_id, start, end in matches:\n",
    "            kw = doc[start:end].text.lower()\n",
    "            tier = lookup.get(kw, {}).get('tier', 1)\n",
    "            if tier == 5: score += TIER_5_SCORE\n",
    "            elif tier == 4: score += TIER_4_SCORE\n",
    "            elif tier == 3: score += TIER_3_SCORE\n",
    "            elif tier == 2: score += TIER_2_SCORE\n",
    "            else: score += TIER_1_SCORE\n",
    "        return score\n",
    "\n",
    "    def score_lead_paragraphs_presence(doc, matcher, lookup):\n",
    "        sents = list(doc.sents)\n",
    "        if not sents: return 0, \"\"\n",
    "        lead_sents_count = min(len(sents), 10)\n",
    "        lead_end_token_index = sents[lead_sents_count - 1].end\n",
    "        matches = matcher(doc)\n",
    "        highest_tier_in_lead = 0\n",
    "        found_kw = \"\"\n",
    "        for match_id, start, end in matches:\n",
    "            if start < lead_end_token_index:\n",
    "                kw = doc[start:end].text.lower()\n",
    "                tier = lookup.get(kw, {}).get('tier', 1)\n",
    "                if tier > highest_tier_in_lead:\n",
    "                    highest_tier_in_lead = tier\n",
    "                    found_kw = kw\n",
    "        if highest_tier_in_lead == 5: return LEAD_BONUS_TIER_5, f\"å‰å¯¼åŠ åˆ†-T5(+'{found_kw}')\"\n",
    "        if highest_tier_in_lead == 4: return LEAD_BONUS_TIER_4, f\"å‰å¯¼åŠ åˆ†-T4(+'{found_kw}')\"\n",
    "        if highest_tier_in_lead == 3: return LEAD_BONUS_TIER_3, f\"å‰å¯¼åŠ åˆ†-T3(+'{found_kw}')\"\n",
    "        if highest_tier_in_lead == 2: return LEAD_BONUS_TIER_2, f\"å‰å¯¼åŠ åˆ†-T2(+'{found_kw}')\"\n",
    "        return 0, \"\"\n",
    "\n",
    "    def penalize_hypothetical(doc, keywords_in_doc):\n",
    "        penalty = 0\n",
    "        reasons = []\n",
    "        for sent in doc.sents:\n",
    "            clean_sent_start = sent.text.strip().lower()\n",
    "            if clean_sent_start.startswith(('if ', 'unless ', 'what if')):\n",
    "                if any(token.text.lower() in keywords_in_doc for token in sent):\n",
    "                    penalty += HYPOTHETICAL_PENALTY\n",
    "                    reasons.append(f\"å‡è®¾å¥æ‰£åˆ†: '{sent.text[:50].strip()}...'\")\n",
    "        return penalty, reasons\n",
    "\n",
    "    def penalize_negation(doc, keywords_in_doc):\n",
    "        penalty = 0\n",
    "        reasons = []\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"neg\" and token.head.text.lower() in keywords_in_doc:\n",
    "                penalty += NEGATION_PENALTY\n",
    "                reasons.append(f\"å¦å®šæ‰£åˆ†: '{token.text} {token.head.text}'\")\n",
    "        return penalty, reasons\n",
    "\n",
    "    print(\"âœ… å— 4: ç²¾ç­›è§„åˆ™å’Œè¯„ä¼°å¼•æ“å‡†å¤‡å®Œæˆ (å·²å‡çº§ä¸º5çº§è¯„åˆ†)ã€‚\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b93ae6fb597319a",
   "metadata": {},
   "source": [
    "# æ­¥éª¤ 5: æ‰§è¡Œé˜¶æ®µäºŒ - ç²¾ç­›æµç¨‹\n",
    "\n",
    "**ç›®æ ‡:** åŠ è½½å€™é€‰é›†ï¼Œåº”ç”¨æ‰€æœ‰å¦å†³è§„åˆ™ï¼Œç„¶åä¿å­˜æœ€ç»ˆç»“æœå’Œè¢«æ‹’ç»çš„æ–‡ç« ã€‚"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 5: æ‰§è¡Œç¬¬äºŒé˜¶æ®µ - ä¸Šä¸‹æ–‡ç²¾ç­›ä¸äº§å‡º (æœ€ç»ˆç‰ˆï¼Œå·²é€‚é…DSWå¤šæ ¸) ---\n",
    "print(\"--- é˜¶æ®µäºŒ: å¼€å§‹ç²¾ç­›å€™é€‰é›† ---\")\n",
    "start_time_s2 = time.time()\n",
    "try:\n",
    "    read_csv_kwargs = {'low_memory': False}\n",
    "    if TEST_MODE:\n",
    "        read_csv_kwargs['nrows'] = CANDIDATE_SAMPLE_SIZE\n",
    "        print(f\"ğŸš€ æµ‹è¯•æ¨¡å¼: æœ€å¤šåŠ è½½å‰ {CANDIDATE_SAMPLE_SIZE} ç¯‡å€™é€‰æ–‡ç« è¿›è¡Œç²¾ç­›ã€‚\")\n",
    "    df_candidates = pd.read_csv(CANDIDATES_FILE, **read_csv_kwargs)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(df_candidates)} ç¯‡å€™é€‰æ–‡ç« ã€‚\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯: å€™é€‰æ–‡ä»¶æœªæ‰¾åˆ° {CANDIDATES_FILE}ã€‚è¯·å…ˆè¿è¡Œå— 3ã€‚\")\n",
    "    df_candidates = pd.DataFrame()\n",
    "\n",
    "if not df_candidates.empty and nlp:\n",
    "    # DSWä¼˜åŒ–1: å°†Pandas Seriesè½¬ä¸ºPythonåŸç”Ÿlistï¼Œæé«˜è·¨è¿›ç¨‹åºåˆ—åŒ–çš„ç¨³å®šæ€§å’Œé€Ÿåº¦\n",
    "    texts = df_candidates[NEWS_COLUMN].astype(str).tolist()\n",
    "    results = []\n",
    "    print(f\"å¼€å§‹ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†æ–‡æœ¬...\")\n",
    "\n",
    "    # DSWä¼˜åŒ–2: ä½¿ç”¨ with nlp.select_pipes() ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚\n",
    "    # è¿™æ˜¯åœ¨ DSW ç­‰ç¯å¢ƒä¸­å®ç°ç¨³å®šå¤šè¿›ç¨‹çš„å…³é”®ã€‚\n",
    "    # å®ƒèƒ½ç¡®ä¿å­è¿›ç¨‹åªåŠ è½½å’Œä½¿ç”¨å½“å‰ä»»åŠ¡å¿…éœ€çš„ç»„ä»¶ï¼Œé¿å…è¿›ç¨‹å¡æ­»ã€‚\n",
    "    # æˆ‘ä»¬éœ€è¦ 'senter' (ç”¨äº doc.sents) å’Œ 'parser' (ç”¨äº token.dep_)\n",
    "    with nlp.select_pipes(enable=[\"senter\", \"parser\"]):\n",
    "        docs_content = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESSES)\n",
    "\n",
    "        # å°†è¿­ä»£å¤„ç†æ”¾åœ¨ 'with' å—å†…éƒ¨ï¼Œä»¥ç¡®ä¿åœ¨å¤šè¿›ç¨‹ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œ\n",
    "        for doc in tqdm(docs_content, total=len(df_candidates), desc=\"ç²¾ç­›æ–‡ç« \"):\n",
    "            score = 0\n",
    "            score_details = []\n",
    "            matches = matcher(doc)\n",
    "            if not matches:\n",
    "                results.append({'score': 0, 'reason': 'ç²¾ç­›é˜¶æ®µæœªåŒ¹é…åˆ°ä»»ä½•å…³é”®è¯'})\n",
    "                continue\n",
    "\n",
    "            # ä¸šåŠ¡é€»è¾‘ä¿æŒä¸å˜ï¼Œä¾ç„¶ä½¿ç”¨5çº§è¯„åˆ†ç³»ç»Ÿ\n",
    "            found_keywords_text = {doc[start:end].text.lower() for _, start, end in matches}\n",
    "\n",
    "            lead_score, lead_reason = score_lead_paragraphs_presence(doc, matcher, keyword_lookup)\n",
    "            if lead_score > 0: score_details.append(lead_reason)\n",
    "\n",
    "            freq_score = score_keyword_frequency(matches, doc, keyword_lookup)\n",
    "            if freq_score > 0: score_details.append(f\"å…³é”®è¯é¢‘ç‡åˆ†: +{freq_score:.2f}\")\n",
    "\n",
    "            score = lead_score + freq_score\n",
    "\n",
    "            hypo_penalty, hypo_reasons = penalize_hypothetical(doc, found_keywords_text)\n",
    "            nega_penalty, nega_reasons = penalize_negation(doc, found_keywords_text)\n",
    "\n",
    "            score += hypo_penalty\n",
    "            score += nega_penalty\n",
    "\n",
    "            score_details.extend(hypo_reasons)\n",
    "            score_details.extend(nega_reasons)\n",
    "\n",
    "            results.append({'score': score, 'reason': ' | '.join(score_details)})\n",
    "\n",
    "    # ä»¥ä¸‹ç»“æœåˆå¹¶ä¸ä¿å­˜éƒ¨åˆ†ï¼Œåœ¨ 'with' å—ä¹‹å¤–æ‰§è¡Œ\n",
    "    print(\"\\næ­£åœ¨åˆå¹¶ç²¾ç­›ç»“æœ...\")\n",
    "    df_results = pd.DataFrame(results, index=df_candidates.index)\n",
    "    df_results['keep'] = df_results['score'] >= ACCEPTANCE_THRESHOLD\n",
    "    df_final_with_reasons = pd.concat([df_candidates, df_results], axis=1)\n",
    "    df_accepted = df_final_with_reasons[df_final_with_reasons['keep'] == True].drop(columns=['keep', 'score', 'reason'])\n",
    "    df_rejected = df_final_with_reasons[df_final_with_reasons['keep'] == False].drop(columns=['keep'])\n",
    "\n",
    "    print(\"\\n--- ç²¾ç­›å®Œæˆ ---\")\n",
    "    df_accepted.to_csv(FINAL_RESULT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… {len(df_accepted)} ç¯‡æœ€ç»ˆæ–‡ç« å·²ä¿å­˜åˆ°: {FINAL_RESULT_FILE}\")\n",
    "    df_rejected.to_csv(REJECTED_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"â„¹ï¸ {len(df_rejected)} ç¯‡è¢«æ‹’ç»çš„æ–‡ç« å·²ä¿å­˜åˆ°: {REJECTED_FILE} (ä¾›åˆ†æ)\")\n",
    "\n",
    "    end_time_s2 = time.time()\n",
    "    total_minutes = (end_time_s2 - start_time_s2) / 60\n",
    "    print(f\"é˜¶æ®µäºŒ (ç²¾ç­›) è€—æ—¶: {total_minutes:.2f} åˆ†é’Ÿã€‚\")\n",
    "else:\n",
    "    print(\"å€™é€‰é›†ä¸ºç©ºæˆ–spaCyæ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡ç²¾ç­›ã€‚\")\n",
    "\n",
    "print(\"\\nâœ… å— 5: ç²¾ç­›æµç¨‹æ‰§è¡Œå®Œæ¯•ã€‚\")"
   ],
   "id": "2740297e8c542bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "26688609a108a558",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
