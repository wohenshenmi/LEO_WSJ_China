{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 1: 准备工作与环境设置\n",
    "\n",
    "**目标:** 导入所有需要的库，并设置好文件路径和全局变量。"
   ],
   "id": "3c4e87dfe8957389"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-29T06:16:50.217300Z",
     "start_time": "2025-07-29T06:16:49.190752Z"
    }
   },
   "source": [
    "# --- 导入库与全局配置 ---\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import time\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 新增的库，用于优化 ---\n",
    "# Flashtext 用于高效的多关键词搜索，替代正则表达式\n",
    "from flashtext import KeywordProcessor\n",
    "# concurrent.futures 和 multiprocessing 用于实现并行处理\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "\n",
    "# 让 tqdm 与 pandas 的 apply 配合使用\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 全局文件与参数配置 ---\n",
    "#ALIYUN_OSS_PATH = '/mnt/aliyun_oss/'\n",
    "ALIYUN_OSS_PATH = ''\n",
    "KEYWORD_JSON_PATH = ALIYUN_OSS_PATH + 'data_raw/china_keywords_collection.json'\n",
    "SOURCE_NEWS_FILE = ALIYUN_OSS_PATH + 'data_raw/final_merged_all_news.csv'\n",
    "CANDIDATES_FILE = ALIYUN_OSS_PATH + 'data_processed/china_news_candidates.csv'\n",
    "FINAL_RESULT_FILE = ALIYUN_OSS_PATH + 'data_processed/final_china_news.csv'\n",
    "REJECTED_FILE = ALIYUN_OSS_PATH + 'data_processed/china_news_rejected_articles.csv'\n",
    "\n",
    "# 新闻列的列名\n",
    "NEWS_COLUMN = 'CONTENT'\n",
    "\n",
    "# 分块处理时每个块的大小\n",
    "CHUNKSIZE = 10000\n",
    "\n",
    "# --- 新增：并行处理配置 ---\n",
    "# 使用 CPU 核心数减 1，留一个核心给系统，避免卡顿\n",
    "NUM_PROCESSES = mp.cpu_count() - 1 if mp.cpu_count() > 1 else 1\n",
    "\n",
    "print(f\"✅ 块 1: 库导入和配置完成。将使用 {NUM_PROCESSES} 个进程进行并行处理。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 块 1: 库导入和配置完成。将使用 11 个进程进行并行处理。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 2: 初筛准备 - 构建智能正则表达式\n",
    "\n",
    "**目标:** 读取 中国相关关键词 JSON 文件，并执行我们讨论过的所有逻辑来构建一个强大、高效的正则表达式。"
   ],
   "id": "61a97e0c381dad4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T06:16:50.229794Z",
     "start_time": "2025-07-29T06:16:50.220306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 构建初筛用的 Flashtext 关键词处理器 ---\n",
    "\n",
    "def build_keyword_processor(json_path):\n",
    "    \"\"\"\n",
    "    从关键词 JSON 文件中构建一个高效的 Flashtext KeywordProcessor。\n",
    "    \"\"\"\n",
    "    print(f\"正在从 {json_path} 加载关键词...\")\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            keywords_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 错误: 关键词文件未找到 {json_path}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"共加载 {len(keywords_data)} 个关键词对象。\")\n",
    "\n",
    "    # 1. 提取全部别名\n",
    "    all_aliases = set()\n",
    "    for item in keywords_data:\n",
    "        all_aliases.add(item['keyword'])\n",
    "        for alias in item.get('aliases', []):\n",
    "            all_aliases.add(alias)\n",
    "\n",
    "    print(f\"提取出 {len(all_aliases)} 个不重复的关键词/别名。\")\n",
    "\n",
    "    # 2. 初始化 Flashtext 处理器\n",
    "    # case_sensitive=False 使其不区分大小写，等同于原正则的 (?i)\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "\n",
    "    # 3. 将所有关键词添加到处理器中\n",
    "    # Flashtext 内部会构建优化的数据结构 (Aho-Corasick automaton)\n",
    "    for kw in all_aliases:\n",
    "        keyword_processor.add_keyword(kw)\n",
    "\n",
    "    print(\"✅ 高效关键词处理器 (Flashtext) 构建完成。\")\n",
    "\n",
    "    return keyword_processor, keywords_data\n",
    "\n",
    "\n",
    "# 执行构建\n",
    "keyword_processor, keywords_data = build_keyword_processor(KEYWORD_JSON_PATH)\n",
    "\n",
    "print(\"\\n✅ 块 2: 初筛准备工作完成。\")"
   ],
   "id": "3e6d80d362a6a719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 data_raw/china_keywords_collection.json 加载关键词...\n",
      "共加载 274 个关键词对象。\n",
      "提取出 394 个不重复的关键词/别名。\n",
      "✅ 高效关键词处理器 (Flashtext) 构建完成。\n",
      "\n",
      "✅ 块 2: 初筛准备工作完成。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 3: 执行阶段一 - 调用外部脚本进行快速初筛\n",
    "\n",
    "**目标:** 对大文件进行分块扫描，应用正则表达式，并保存候选集。这将是整个流程中最耗时的部分。"
   ],
   "id": "d3d24e1445bfbb23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T06:19:23.296588Z",
     "start_time": "2025-07-29T06:16:50.377764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 步骤 3: 执行阶段一 - 调用外部脚本进行并行初筛 (优化版) ---\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"--- 阶段一: 调用外部脚本进行并行初筛 ---\")\n",
    "start_time_script = time.time()\n",
    "\n",
    "# 构建要执行的命令\n",
    "# 我们将Python解释器的路径、脚本名和所有必需的文件路径作为参数传递\n",
    "command = [\n",
    "    sys.executable,  # 使用当前Jupyter环境的Python解释器，保证环境一致\n",
    "    '_parallel_filter.py',\n",
    "    SOURCE_NEWS_FILE,\n",
    "    KEYWORD_JSON_PATH,\n",
    "    CANDIDATES_FILE\n",
    "]\n",
    "\n",
    "print(f\"将要执行的命令: {' '.join(command)}\")\n",
    "\n",
    "try:\n",
    "    # 执行脚本\n",
    "    # check=True: 如果脚本返回非零退出码（即出错），则会抛出异常\n",
    "    # capture_output=True: 捕获脚本的 stdout 和 stderr\n",
    "    # text=True: 将捕获的输出解码为文本\n",
    "    result = subprocess.run(\n",
    "        command,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding='utf-8' # 明确编码以避免乱码\n",
    "    )\n",
    "\n",
    "    # 打印脚本的实时输出\n",
    "    print(\"\\n--- 外部脚本输出 ---\")\n",
    "    print(result.stdout)\n",
    "    print(\"---------------------\\n\")\n",
    "\n",
    "    end_time_script = time.time()\n",
    "    print(f\"✅ 外部脚本执行成功！\")\n",
    "    print(f\"阶段一总耗时: {(end_time_script - start_time_script) / 60:.2f} 分钟。\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    # 如果脚本执行失败，打印详细的错误信息\n",
    "    print(\"\\n❌ 外部脚本执行失败！\")\n",
    "    print(f\"返回码: {e.returncode}\")\n",
    "    print(\"\\n--- STDOUT (标准输出) ---\")\n",
    "    print(e.stdout)\n",
    "    print(\"\\n--- STDERR (错误输出) ---\")\n",
    "    print(e.stderr)\n",
    "    print(\"--------------------------\\n\")\n",
    "    print(\"请检查上面的错误信息。\")\n",
    "\n",
    "print(\"✅ 块 3: 初筛流程执行完毕。\")"
   ],
   "id": "792bd4595d23d8f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段一: 调用外部脚本进行并行初筛 ---\n",
      "将要执行的命令: C:\\Users\\liuma\\anaconda3\\envs\\LEO_WSJ_China\\python.exe parallel_filter.py data_raw/final_merged_all_news.csv data_raw/china_keywords_collection.json data_processed/china_news_candidates.csv\n",
      "\n",
      "--- 外部脚本输出 ---\n",
      "--- 开始执行并行初筛脚本 ---\n",
      "✅ 关键词处理器构建完成，包含 394 个关键词。\n",
      "正在计算文件总块数...\n",
      "文件约包含 377 个数据块。开始处理...\n",
      "\n",
      "--- 初筛流程执行完毕 ---\n",
      "总共处理了 150 个数据块。\n",
      "总共找到 179380 篇候选文章。\n",
      "结果已保存到: data_processed/china_news_candidates.csv\n",
      "耗时: 2.54 分钟。\n",
      "\n",
      "---------------------\n",
      "\n",
      "✅ 外部脚本执行成功！\n",
      "阶段一总耗时: 2.55 分钟。\n",
      "✅ 块 3: 初筛流程执行完毕。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 4: 精筛准备 - 加载模型与定义规则\n",
    "\n",
    "**目标:** 负责加载 spaCy 模型和数据，并定义所有用于精筛的“否决规则”函数。"
   ],
   "id": "ace73e1406b18418"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T06:19:24.985598Z",
     "start_time": "2025-07-29T06:19:23.351046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 精筛准备 - 加载模型与定义否决规则 ---\n",
    "print(\"--- 阶段二准备: 加载 spaCy 模型 ---\")\n",
    "# 禁用不需要的组件可以加快加载速度和减少内存占用\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\", \"lemmatizer\", \"attribute_ruler\"])\n",
    "print(f\"✅ spaCy 模型 '{nlp.meta['name']}' 的基础组件加载成功。\")\n",
    "\n",
    "# --- 准备 PhraseMatcher ---\n",
    "print(\"正在准备 PhraseMatcher...\")\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "keyword_lookup = {}\n",
    "patterns = []\n",
    "\n",
    "for item in keywords_data:\n",
    "    # 为每个别名创建 Doc 对象作为 pattern\n",
    "    for alias in item.get('aliases', [item['keyword']]):\n",
    "        # 使用 nlp.make_doc 确保 pattern 是 spaCy 的 Doc 对象\n",
    "        patterns.append(nlp.make_doc(alias))\n",
    "        # 建立一个小写的 alias -> item 信息的查找表\n",
    "        keyword_lookup[alias.lower()] = {\n",
    "            'type': item.get('type'),\n",
    "            'category': item.get('category'),\n",
    "            'tier': item.get('relevance_tier')\n",
    "        }\n",
    "\n",
    "# 一次性将所有 patterns 加入 matcher，效率更高\n",
    "matcher.add(\"ChinaKeywords\", patterns)\n",
    "print(f\"✅ PhraseMatcher 准备完成，已添加 {len(patterns)} 个模式。\")\n",
    "\n",
    "\n",
    "# --- 定义否决规则函数 (保持不变) ---\n",
    "def check_negation(doc, keywords_in_doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"neg\":\n",
    "            if token.head.text.lower() in keywords_in_doc:\n",
    "                return True, f\"否定语境: '{token.text}' 修饰了关键词 '{token.head.text}'\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_hypothetical(doc, keywords_in_doc):\n",
    "    for sent in doc.sents:\n",
    "        if sent.root.text.lower() == 'if' or sent[0].text.lower() in ['if', 'unless']:\n",
    "            for token in sent:\n",
    "                if token.text.lower() in keywords_in_doc:\n",
    "                    return True, f\"假设语境: 句子以 '{sent[0].text}' 开头\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "def check_low_tier_only(found_keywords_info):\n",
    "    if not found_keywords_info:\n",
    "        return True, \"未找到任何关键词\"\n",
    "    tiers = [info['tier'] for info in found_keywords_info]\n",
    "    if all(tier <= 2 for tier in tiers):\n",
    "        strong_categories = {\"Politics\", \"Economics\", \"Geopolitics\", \"Technology\", \"Finance\", \"Military\"}\n",
    "        categories = {info['category'] for info in found_keywords_info}\n",
    "        if not strong_categories.intersection(categories):\n",
    "            return True, \"只包含Tier 1/2的弱相关关键词 (如文化、体育)\"\n",
    "    return False, \"\"\n",
    "\n",
    "\n",
    "print(\"✅ 块 4: 精筛规则定义和 Matcher 准备完成。\")\n"
   ],
   "id": "eb5b2016c4e20800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段二准备: 加载 spaCy 模型 ---\n",
      "✅ spaCy 模型 'core_web_lg' 的基础组件加载成功。\n",
      "正在准备 PhraseMatcher...\n",
      "✅ PhraseMatcher 准备完成，已添加 397 个模式。\n",
      "✅ 块 4: 精筛规则定义和 Matcher 准备完成。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 5: 执行阶段二 - 精筛流程\n",
    "\n",
    "**目标:** 加载候选集，应用所有否决规则，然后保存最终结果和被拒绝的文章。"
   ],
   "id": "8b93ae6fb597319a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T06:55:29.426809Z",
     "start_time": "2025-07-29T06:19:25.003059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 执行阶段二 - 使用 PhraseMatcher 进行高效精筛 ---\n",
    "\n",
    "print(\"--- 阶段二: 开始精筛候选集 ---\")\n",
    "start_time_s2 = time.time()  # 记录精筛阶段的开始时间\n",
    "\n",
    "try:\n",
    "    # 从上一步生成的 CSV 文件加载候选集\n",
    "    # low_memory=False 可以稍微加快读取速度，但会占用更多内存，对于候选集通常是值得的\n",
    "    df_candidates = pd.read_csv(CANDIDATES_FILE, low_memory=False)\n",
    "    print(f\"✅ 成功加载 {len(df_candidates)} 篇候选文章。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 候选文件未找到 {CANDIDATES_FILE}。请先运行优化后的块 3。\")\n",
    "    df_candidates = pd.DataFrame()  # 创建空DataFrame以避免后续代码出错\n",
    "\n",
    "if not df_candidates.empty:\n",
    "    texts = df_candidates[NEWS_COLUMN].astype(str)\n",
    "\n",
    "    results = []\n",
    "    # 使用 nlp.select_pipes 来临时启用需要的组件，这是一个管理计算资源的好习惯\n",
    "    with nlp.select_pipes(enable=[\"parser\"]):\n",
    "        # 批量处理文本，nlp.pipe 会自动利用多核优势\n",
    "        # batch_size 可以根据你的 CPU 核心数和内存来调整\n",
    "        docs = nlp.pipe(texts, batch_size=50)\n",
    "\n",
    "        # 使用 tqdm 包装以显示进度\n",
    "        for doc in tqdm(docs, total=len(df_candidates), desc=\"精筛文章\"):\n",
    "            rejection_reason = \"\"\n",
    "            is_rejected = False\n",
    "\n",
    "            # 1. 【核心优化】使用 PhraseMatcher 高效查找关键词\n",
    "            # matcher(doc) 会在 C 语言层面以极高速度完成所有关键词的查找\n",
    "            matches = matcher(doc)\n",
    "\n",
    "            # 如果没有匹配到任何关键词，直接拒绝 (这通常是健全性检查)\n",
    "            if not matches:\n",
    "                results.append({'keep': False, 'rejection_reason': '未找到任何关键词(精筛阶段)'})\n",
    "                continue\n",
    "\n",
    "            # 2. 【核心优化】从匹配结果中高效地收集信息\n",
    "            # 使用集合推导式，快速获取所有匹配到的、不重复的关键词文本\n",
    "            found_keywords_text = {doc[start:end].text.lower() for match_id, start, end in matches}\n",
    "\n",
    "            # 使用列表推导式，快速从查找字典中获取关键词的详细信息\n",
    "            found_keywords_info = [keyword_lookup[kw] for kw in found_keywords_text if kw in keyword_lookup]\n",
    "\n",
    "            # --- 3. 依次应用否决规则 (这部分逻辑保持不变) ---\n",
    "            is_rejected, rejection_reason = check_low_tier_only(found_keywords_info)\n",
    "\n",
    "            if not is_rejected:\n",
    "                is_rejected, rejection_reason = check_negation(doc, found_keywords_text)\n",
    "\n",
    "            if not is_rejected:\n",
    "                is_rejected, rejection_reason = check_hypothetical(doc, found_keywords_text)\n",
    "\n",
    "            # ... 您可以在这里加入更多未来的规则 ...\n",
    "\n",
    "            results.append({\n",
    "                'keep': not is_rejected,\n",
    "                'rejection_reason': rejection_reason\n",
    "            })\n",
    "\n",
    "    # --- 合并与保存结果 ---\n",
    "    print(\"\\n正在合并精筛结果...\")\n",
    "    df_results = pd.DataFrame(results, index=df_candidates.index)\n",
    "    df_final_with_reasons = pd.concat([df_candidates, df_results], axis=1)\n",
    "\n",
    "    # 分离通过和被拒绝的文章\n",
    "    df_accepted = df_final_with_reasons[df_final_with_reasons['keep'] == True].drop(\n",
    "        columns=['keep', 'rejection_reason'])\n",
    "    df_rejected = df_final_with_reasons[df_final_with_reasons['keep'] == False].drop(columns=['keep'])\n",
    "\n",
    "    print(\"\\n--- 精筛完成 ---\")\n",
    "    # 将最终结果保存为 CSV\n",
    "    df_accepted.to_csv(FINAL_RESULT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"✅ {len(df_accepted)} 篇最终文章已保存到: {FINAL_RESULT_FILE}\")\n",
    "\n",
    "    df_rejected.to_csv(REJECTED_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"ℹ️ {len(df_rejected)} 篇被拒绝的文章已保存到: {REJECTED_FILE} (供分析)\")\n",
    "\n",
    "    end_time_s2 = time.time()\n",
    "    print(f\"阶段二耗时: {(end_time_s2 - start_time_s2) / 60:.2f} 分钟。\")\n",
    "\n",
    "else:\n",
    "    print(\"候选集为空，无需精筛。\")\n",
    "\n",
    "print(\"\\n✅ 块 5: 精筛流程执行完毕。\")\n"
   ],
   "id": "4f5ee5651615939c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段二: 开始精筛候选集 ---\n",
      "✅ 成功加载 179380 篇候选文章。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "精筛文章: 100%|██████████| 179380/179380 [35:50<00:00, 83.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在合并精筛结果...\n",
      "\n",
      "--- 精筛完成 ---\n",
      "✅ 178273 篇最终文章已保存到: data_processed/final_china_news.csv\n",
      "ℹ️ 1107 篇被拒绝的文章已保存到: data_processed/china_news_rejected_articles.csv (供分析)\n",
      "阶段二耗时: 36.07 分钟。\n",
      "\n",
      "✅ 块 5: 精筛流程执行完毕。\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
