{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 1: 准备工作与环境设置\n",
    "\n",
    "**目标:** 导入所有需要的库，并设置好文件路径和全局变量。"
   ],
   "id": "3c4e87dfe8957389"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T04:07:04.998040Z",
     "start_time": "2025-07-30T04:07:03.859578Z"
    }
   },
   "source": [
    "import html\n",
    "# --- 导入库与全局配置 ---\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import psutil  # 用于智能检测系统资源\n",
    "import spacy\n",
    "import unicodedata\n",
    "# --- 新增的优化库 ---\n",
    "from flashtext import KeywordProcessor\n",
    "from tqdm.auto import tqdm  # 使用tqdm.auto以获得更好的显示效果\n",
    "\n",
    "# --- 全局文件与参数配置 ---\n",
    "ALIYUN_OSS_PATH = ''  #ALIYUN_OSS_PATH = '/mnt/data/scripts/'\n",
    "KEYWORD_JSON_PATH = os.path.join(ALIYUN_OSS_PATH, '../data_raw/china_keywords_collection.json')\n",
    "SOURCE_NEWS_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_raw/final_merged_all_news.csv')\n",
    "CANDIDATES_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_processed/china_news_candidates.csv')\n",
    "FINAL_RESULT_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_processed/final_china_news.csv')\n",
    "REJECTED_FILE = os.path.join(ALIYUN_OSS_PATH, '../data_processed/china_news_rejected_articles.csv')\n",
    "\n",
    "# --- 全局处理参数 ---\n",
    "NEWS_COLUMN = 'CONTENT'\n",
    "CHUNKSIZE = 20000  # 适当增大块大小，以减少IO次数\n",
    "\n",
    "# --- 智能配置并行参数 ---\n",
    "cpu_cores = psutil.cpu_count(logical=False) # 使用物理核心数\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8) # 使用核心数-1，但最多不超过8个\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "BATCH_SIZE = 500 # spaCy并行处理时的批大小\n",
    "\n",
    "print(\"✅ 块 1: 库导入和配置完成。\")\n",
    "print(f\"   - 将使用 Flashtext 进行高效初筛。\")\n",
    "print(f\"   - 将使用 {N_PROCESSES} 个进程进行并行精筛。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 块 1: 库导入和配置完成。\n",
      "   - 将使用 Flashtext 进行高效初筛。\n",
      "   - 将使用 5 个进程进行并行精筛。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 2: 初筛准备 - 构建智能正则表达式\n",
    "\n",
    "**目标:** 读取 中国相关关键词 JSON 文件，并执行我们讨论过的所有逻辑来构建一个强大、高效的正则表达式。"
   ],
   "id": "61a97e0c381dad4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T04:07:05.167988Z",
     "start_time": "2025-07-30T04:07:05.160615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 构建初筛用的 Flashtext 关键词处理器 ---\n",
    "\n",
    "def build_keyword_processor(json_path):\n",
    "    \"\"\"\n",
    "    从关键词 JSON 文件中构建一个高效的 Flashtext KeywordProcessor。\n",
    "    \"\"\"\n",
    "    print(f\"正在从 {json_path} 加载关键词...\")\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            keywords_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 错误: 关键词文件未找到 {json_path}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"共加载 {len(keywords_data)} 个关键词对象。\")\n",
    "\n",
    "    # 1. 提取全部关键词和别名\n",
    "    all_aliases = set()\n",
    "    for item in keywords_data:\n",
    "        all_aliases.add(item['keyword'])\n",
    "        for alias in item.get('aliases', []):\n",
    "            all_aliases.add(alias)\n",
    "    print(f\"提取出 {len(all_aliases)} 个不重复的关键词/别名。\")\n",
    "\n",
    "    # 2. 初始化 Flashtext 处理器并添加关键词\n",
    "    # case_sensitive=False 使其不区分大小写\n",
    "    keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "    for kw in all_aliases:\n",
    "        keyword_processor.add_keyword(kw)\n",
    "\n",
    "    print(\"✅ 高效关键词处理器 (Flashtext) 构建完成。\")\n",
    "    return keyword_processor, keywords_data\n",
    "\n",
    "\n",
    "# 执行构建\n",
    "keyword_processor, keywords_data = build_keyword_processor(KEYWORD_JSON_PATH)\n",
    "\n",
    "# 将关键词处理器设为全局变量，以便后续子进程可以访问 (在某些系统上需要)\n",
    "global_keyword_processor = keyword_processor\n",
    "\n",
    "print(\"\\n✅ 块 2: 初筛准备工作完成。\")"
   ],
   "id": "3e6d80d362a6a719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 ../data_raw/china_keywords_collection.json 加载关键词...\n",
      "共加载 274 个关键词对象。\n",
      "提取出 394 个不重复的关键词/别名。\n",
      "✅ 高效关键词处理器 (Flashtext) 构建完成。\n",
      "\n",
      "✅ 块 2: 初筛准备工作完成。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 3: 执行阶段一 - 调用外部脚本进行快速初筛\n",
    "\n",
    "**目标:** 对大文件进行分块扫描，应用正则表达式，并保存候选集。这将是整个流程中最耗时的部分。"
   ],
   "id": "d3d24e1445bfbb23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T04:07:13.945780Z",
     "start_time": "2025-07-30T04:07:05.201020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 单元格 3 (最终优化版) ---\n",
    "\n",
    "# --- 定义并行处理函数 ---\n",
    "\n",
    "def lightweight_clean(text):\n",
    "    \"\"\"一个非常轻量级的文本清理函数。\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = html.unescape(text)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    \"\"\"\n",
    "    处理单个数据块的核心任务：清理文本并使用Flashtext筛选。\n",
    "    这个函数将在每个子进程中独立运行。\n",
    "    \"\"\"\n",
    "    if NEWS_COLUMN not in chunk_df.columns:\n",
    "        return pd.DataFrame() # 返回空DataFrame\n",
    "\n",
    "    # 清理文本\n",
    "    cleaned_series = chunk_df[NEWS_COLUMN].astype(str).apply(lightweight_clean)\n",
    "\n",
    "    # 使用在步骤2中创建的全局 keyword_processor 进行查找\n",
    "    mask = cleaned_series.apply(lambda x: len(keyword_processor.extract_keywords(x)) > 0)\n",
    "\n",
    "    # 返回匹配到的候选行\n",
    "    return chunk_df[mask]\n",
    "\n",
    "# --- 执行阶段一 - 流式并行扫描与初筛 ---\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "print(\"--- 阶段一: 开始使用多进程进行高效流式初筛 ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # 1. 创建CSV文件迭代器，这是流式处理的关键\n",
    "    chunk_iterator = pd.read_csv(SOURCE_NEWS_FILE, chunksize=CHUNKSIZE, on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "    is_first_chunk = True\n",
    "    total_candidates = 0\n",
    "\n",
    "    # 预计算总块数以提供准确的进度条\n",
    "    print(\"正在计算文件总块数...\")\n",
    "    num_lines = sum(1 for row in open(SOURCE_NEWS_FILE, 'r', encoding='utf-8', errors='ignore'))\n",
    "    total_chunks = (num_lines // CHUNKSIZE) + 1\n",
    "    print(f\"文件约包含 {total_chunks} 个数据块。将使用 {N_PROCESSES} 个进程并行处理...\")\n",
    "\n",
    "    # 2. 使用 ProcessPoolExecutor 进行流式并行处理\n",
    "    with ProcessPoolExecutor(max_workers=N_PROCESSES) as executor:\n",
    "\n",
    "        # 3. 使用 executor.map 将迭代器中的数据块流式地分发给子进程\n",
    "        # executor.map 会自动处理任务分发和结果收集，并保持顺序\n",
    "        results_iterator = executor.map(process_chunk, chunk_iterator)\n",
    "\n",
    "        # 4. 使用tqdm包装结果迭代器，以显示总体进度\n",
    "        for candidates_df in tqdm(results_iterator, total=total_chunks, desc=\"并行初筛中\"):\n",
    "            if not candidates_df.empty:\n",
    "                total_candidates += len(candidates_df)\n",
    "                # 写入到CSV文件\n",
    "                if is_first_chunk:\n",
    "                    candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='w', encoding='utf-8')\n",
    "                    is_first_chunk = False\n",
    "                else:\n",
    "                    candidates_df.to_csv(CANDIDATES_FILE, index=False, mode='a', header=False, encoding='utf-8')\n",
    "\n",
    "    end_time_stage1 = time.time()\n",
    "    print(\"\\n--- 初筛流程执行完毕 ---\")\n",
    "    print(f\"总共找到 {total_candidates} 篇候选文章，已保存到 {CANDIDATES_FILE}\")\n",
    "    print(f\"阶段一 (并行初筛) 耗时: {(end_time_stage1 - start_time) / 60:.2f} 分钟。\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 原始新闻文件未找到 {SOURCE_NEWS_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 处理过程中发生错误: {e}\")\n",
    "\n",
    "print(\"\\n✅ 块 3: 初筛流程执行完毕。\")"
   ],
   "id": "792bd4595d23d8f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段一: 开始使用多进程进行高效流式初筛 ---\n",
      "正在计算文件总块数...\n",
      "文件约包含 754 个数据块。将使用 5 个进程并行处理...\n",
      "❌ 处理过程中发生错误: A child process terminated abruptly, the process pool is not usable anymore\n",
      "\n",
      "✅ 块 3: 初筛流程执行完毕。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 4: 精筛准备 - 加载模型与定义规则\n",
    "\n",
    "**目标:** 负责加载 spaCy 模型和数据，并定义所有用于精筛的“否决规则”函数。"
   ],
   "id": "ace73e1406b18418"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T04:07:15.091330Z",
     "start_time": "2025-07-30T04:07:13.956999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 精筛准备 - 加载模型与定义否决规则 ---\n",
    "print(\"--- 阶段二准备: 加载 spaCy 模型 ---\")\n",
    "\n",
    "# 禁用所有不需要的组件，只保留'parser'用于句法分析 (token.dep_, token.head, doc.sents)\n",
    "# 这是对内存和速度的巨大优化\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"ner\", \"lemmatizer\", \"tagger\", \"attribute_ruler\"])\n",
    "    print(f\"✅ spaCy 模型 '{nlp.meta['name']}' 的核心组件加载成功。\")\n",
    "except OSError:\n",
    "    print(\"错误: spaCy模型 'en_core_web_lg' 未安装。\")\n",
    "    print(\"请在你的终端或命令行中运行: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None\n",
    "\n",
    "if nlp:\n",
    "    # --- 准备 PhraseMatcher ---\n",
    "    print(\"正在准备 PhraseMatcher...\")\n",
    "    from spacy.matcher import PhraseMatcher\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "\n",
    "    keyword_lookup = {}\n",
    "    patterns = [nlp.make_doc(alias) for item in keywords_data for alias in item.get('aliases', [item['keyword']])]\n",
    "\n",
    "    for item in keywords_data:\n",
    "        for alias in item.get('aliases', [item['keyword']]):\n",
    "            keyword_lookup[alias.lower()] = {\n",
    "                'type': item.get('type'),\n",
    "                'category': item.get('category'),\n",
    "                'tier': item.get('relevance_tier')\n",
    "            }\n",
    "\n",
    "    matcher.add(\"ChinaKeywords\", patterns)\n",
    "    print(f\"✅ PhraseMatcher 准备完成，已添加 {len(patterns)} 个模式。\")\n",
    "\n",
    "# --- 定义否决规则函数 (保持不变) ---\n",
    "def check_negation(doc, keywords_in_doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"neg\" and token.head.text.lower() in keywords_in_doc:\n",
    "            return True, f\"否定语境: '{token.text}' 修饰了关键词 '{token.head.text}'\"\n",
    "    return False, \"\"\n",
    "\n",
    "def check_hypothetical(doc, keywords_in_doc):\n",
    "    for sent in doc.sents:\n",
    "        # 改进检查逻辑，更准确\n",
    "        if any(tok.lower_ in ['if', 'unless'] for tok in sent[:3]):\n",
    "             if any(token.text.lower() in keywords_in_doc for token in sent):\n",
    "                return True, f\"假设语境: 句子包含 '{sent[:3].text.strip()}'...\"\n",
    "    return False, \"\"\n",
    "\n",
    "def check_low_tier_only(found_keywords_info):\n",
    "    if not found_keywords_info: return True, \"未找到任何关键词\"\n",
    "    tiers = [info['tier'] for info in found_keywords_info]\n",
    "    if all(tier <= 2 for tier in tiers):\n",
    "        strong_categories = {\"Politics\", \"Economics\", \"Geopolitics\", \"Technology\", \"Finance\", \"Military\"}\n",
    "        categories = {info['category'] for info in found_keywords_info}\n",
    "        if not strong_categories.intersection(categories):\n",
    "            return True, \"只包含Tier 1/2的弱相关关键词 (如文化、体育)\"\n",
    "    return False, \"\"\n",
    "\n",
    "print(\"✅ 块 4: 精筛规则定义和 Matcher 准备完成。\")"
   ],
   "id": "eb5b2016c4e20800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段二准备: 加载 spaCy 模型 ---\n",
      "✅ spaCy 模型 'core_web_lg' 的核心组件加载成功。\n",
      "正在准备 PhraseMatcher...\n",
      "✅ PhraseMatcher 准备完成，已添加 397 个模式。\n",
      "✅ 块 4: 精筛规则定义和 Matcher 准备完成。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 步骤 5: 执行阶段二 - 精筛流程\n",
    "\n",
    "**目标:** 加载候选集，应用所有否决规则，然后保存最终结果和被拒绝的文章。"
   ],
   "id": "8b93ae6fb597319a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-30T04:07:15.101548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 执行阶段二 - 使用并行化的 spaCy 进行高效精筛 ---\n",
    "\n",
    "print(\"--- 阶段二: 开始精筛候选集 ---\")\n",
    "start_time_s2 = time.time()\n",
    "\n",
    "try:\n",
    "    df_candidates = pd.read_csv(CANDIDATES_FILE, low_memory=False)\n",
    "    print(f\"✅ 成功加载 {len(df_candidates)} 篇候选文章。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 候选文件未找到 {CANDIDATES_FILE}。请先运行块 3。\")\n",
    "    df_candidates = pd.DataFrame()\n",
    "\n",
    "if not df_candidates.empty and nlp:\n",
    "    texts = df_candidates[NEWS_COLUMN].astype(str).tolist() # 转为list以获得最佳性能\n",
    "    results = []\n",
    "\n",
    "    print(f\"开始使用 {N_PROCESSES} 个进程并行精筛...\")\n",
    "    # --- 核心优化：使用 n_process 并行处理 ---\n",
    "    docs = nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESSES)\n",
    "\n",
    "    for doc in tqdm(docs, total=len(df_candidates), desc=\"精筛文章\"):\n",
    "        rejection_reason = \"\"\n",
    "        is_rejected = False\n",
    "\n",
    "        matches = matcher(doc)\n",
    "        if not matches:\n",
    "            results.append({'keep': False, 'rejection_reason': '未找到任何关键词(精筛阶段)'})\n",
    "            continue\n",
    "\n",
    "        found_keywords_text = {doc[start:end].text.lower() for match_id, start, end in matches}\n",
    "        found_keywords_info = [keyword_lookup[kw] for kw in found_keywords_text if kw in keyword_lookup]\n",
    "\n",
    "        is_rejected, rejection_reason = check_low_tier_only(found_keywords_info)\n",
    "        if not is_rejected:\n",
    "            is_rejected, rejection_reason = check_negation(doc, found_keywords_text)\n",
    "        if not is_rejected:\n",
    "            is_rejected, rejection_reason = check_hypothetical(doc, found_keywords_text)\n",
    "\n",
    "        results.append({'keep': not is_rejected, 'rejection_reason': rejection_reason})\n",
    "\n",
    "    # --- 合并与保存结果 ---\n",
    "    print(\"\\n正在合并精筛结果...\")\n",
    "    df_results = pd.DataFrame(results, index=df_candidates.index)\n",
    "    df_final_with_reasons = pd.concat([df_candidates, df_results], axis=1)\n",
    "\n",
    "    df_accepted = df_final_with_reasons[df_final_with_reasons['keep'] == True].drop(columns=['keep', 'rejection_reason'])\n",
    "    df_rejected = df_final_with_reasons[df_final_with_reasons['keep'] == False].drop(columns=['keep'])\n",
    "\n",
    "    print(\"\\n--- 精筛完成 ---\")\n",
    "    df_accepted.to_csv(FINAL_RESULT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"✅ {len(df_accepted)} 篇最终文章已保存到: {FINAL_RESULT_FILE}\")\n",
    "\n",
    "    df_rejected.to_csv(REJECTED_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"ℹ️ {len(df_rejected)} 篇被拒绝的文章已保存到: {REJECTED_FILE} (供分析)\")\n",
    "\n",
    "    end_time_s2 = time.time()\n",
    "    print(f\"阶段二 (精筛) 耗时: {(end_time_s2 - start_time_s2) / 60:.2f} 分钟。\")\n",
    "\n",
    "else:\n",
    "    print(\"候选集为空或spaCy模型未加载，跳过精筛。\")\n",
    "\n",
    "print(\"\\n✅ 块 5: 精筛流程执行完毕。\")"
   ],
   "id": "4f5ee5651615939c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段二: 开始精筛候选集 ---\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
