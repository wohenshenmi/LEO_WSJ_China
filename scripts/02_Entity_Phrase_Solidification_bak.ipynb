{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# é˜¶æ®µä¸€: å€™é€‰çŸ­è¯­çš„è‡ªåŠ¨åŒ–å‘çŽ° (The Discovery Engine)\n",
    "\n",
    "**æ€»ç›®æ ‡:** ä»Žçº¦17ä¸‡æ¡ä¸­å›½æ–°é—»ä¸­ï¼Œé€šè¿‡ä¸‰ç§äº’è¡¥çš„ç®—æ³•ï¼ˆGensim Phrases, spaCy NER, spaCy Noun Chunksï¼‰ï¼Œå¤§è§„æ¨¡æŒ–æŽ˜æ½œåœ¨çš„ã€æœ‰æ„ä¹‰çš„å¤šè¯çŸ­è¯­ï¼Œä¸ºåŽç»­çš„äººå·¥å®¡æ ¸çŽ¯èŠ‚æä¾›ä¸€ä¸ªé«˜è´¨é‡çš„â€œå€™é€‰æ± â€ã€‚\n",
    "\n",
    "---\n",
    "### æ­¥éª¤ 1.0 - çŽ¯å¢ƒè®¾ç½®ä¸Žåº“å¯¼å…¥"
   ],
   "metadata": {},
   "id": "473538b1c279a8ae"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.0: çŽ¯å¢ƒè®¾ç½®ä¸Žåº“å¯¼å…¥ ---\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from gensim.models.phrases import Phrases\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "\n",
    "# --- å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¼€å…³ ---\n",
    "TEST_MODE = True\n",
    "TEST_SAMPLE_SIZE = 1000\n",
    "\n",
    "# --- é…ç½®åŒº ---\n",
    "ALIYUN_OSS_PATH = ''\n",
    "# è¾“å…¥æ–‡ä»¶è·¯å¾„\n",
    "INPUT_CHINA_DATA_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/final_china_news.csv\")\n",
    "\n",
    "# ä¸­é—´äº§å‡ºæ–‡ä»¶è·¯å¾„\n",
    "TOKEN_LISTS_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/intermediate_raw_token_lists.pkl\")\n",
    "CANDIDATES_GENSIM_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/candidates_gensim.pkl\")\n",
    "CANDIDATES_NER_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/candidates_ner.pkl\")\n",
    "# CANDIDATES_NOUN_CHUNKS_PATH å·²è¢«ç§»é™¤\n",
    "\n",
    "# --- å¹¶è¡Œå¤„ç†é…ç½® ---\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "print(\"--- çŽ¯å¢ƒå‡†å¤‡ ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"ðŸš€ðŸš€ðŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†å‰ {TEST_SAMPLE_SIZE} è¡Œæ•°æ®ï¼ðŸš€ðŸš€ðŸš€\")\n",
    "else:\n",
    "    print(\"ðŸš¢ðŸš¢ðŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®ã€‚ðŸš¢ðŸš¢ðŸš¢\")\n",
    "print(f\"è¾“å…¥æ–‡ä»¶è·¯å¾„: {INPUT_CHINA_DATA_PATH}\")\n",
    "print(f\"å°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\")\n",
    "\n",
    "# --- åŠ è½½spaCyæ¨¡åž‹ ---\n",
    "print(\"\\næ­£åœ¨åŠ è½½spaCyæ¨¡åž‹ 'en_core_web_lg' (å®Œæ•´æ¨¡å¼)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(f\"spaCyæ¨¡åž‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {time.time() - start_time:.2f} ç§’\")\n",
    "except OSError:\n",
    "    print(\"é”™è¯¯: spaCyæ¨¡åž‹ 'en_core_web_lg' æœªå®‰è£…ã€‚\")\n",
    "    print(\"è¯·åœ¨ä½ çš„ç»ˆç«¯æˆ–å‘½ä»¤è¡Œä¸­è¿è¡Œ: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:10:42.754605Z",
     "start_time": "2025-07-30T09:10:39.924655Z"
    }
   },
   "id": "b5deacf9889d8c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- çŽ¯å¢ƒå‡†å¤‡ ---\n",
      "ðŸš€ðŸš€ðŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†å‰ 1000 è¡Œæ•°æ®ï¼ðŸš€ðŸš€ðŸš€\n",
      "è¾“å…¥æ–‡ä»¶è·¯å¾„: ../data_processed/final_china_news.csv\n",
      "å°†ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\n",
      "\n",
      "æ­£åœ¨åŠ è½½spaCyæ¨¡åž‹ 'en_core_web_lg' (å®Œæ•´æ¨¡å¼)...\n",
      "spaCyæ¨¡åž‹åŠ è½½æˆåŠŸï¼è€—æ—¶: 1.16 ç§’\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### æ­¥éª¤ 1.1 - æ–‡æœ¬é¢„åˆ†è¯\n",
    "\n",
    "éœ€æ±‚: ä¸º Gensim å‡†å¤‡ raw_token_listsã€‚\n",
    "å®žçŽ°: ä½¿ç”¨å¹¶è¡ŒåŒ–çš„ nlp.pipeï¼Œä½†åªå¯ç”¨åˆ†è¯åŠŸèƒ½ä»¥èŽ·å¾—æœ€é«˜é€Ÿåº¦ã€‚"
   ],
   "metadata": {},
   "id": "fa0acf6370673b99"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.1: å¹¶è¡Œé¢„åˆ†è¯ ---\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "print(\"--- æ­¥éª¤ 1.1a: æ•°æ®åŠ è½½ ---\")\n",
    "start_time_load = time.time()\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    df = pd.read_csv(INPUT_CHINA_DATA_PATH, dtype=str, nrows=read_nrows)\n",
    "    df.dropna(subset=['CONTENT'], inplace=True)\n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆï¼è€—æ—¶: {time.time() - start_time_load:.2f} ç§’ã€‚ æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_CHINA_DATA_PATH}\")\n",
    "\n",
    "if not df.empty and nlp:\n",
    "    print(\"\\n--- æ­¥éª¤ 1.1b: å¼€å§‹å¹¶è¡Œé¢„åˆ†è¯ ---\")\n",
    "    start_time_process = time.time()\n",
    "    raw_token_lists = []\n",
    "\n",
    "    # ç¦ç”¨æ‰€æœ‰ä¸éœ€è¦çš„ç®¡é“ç»„ä»¶ä»¥èŽ·å¾—æœ€å¿«é€Ÿåº¦\n",
    "    disabled_pipes = [pipe for pipe in nlp.pipe_names if pipe not in ['tok2vec']]\n",
    "\n",
    "    with nlp.select_pipes(disable=disabled_pipes):\n",
    "        docs = nlp.pipe(df['CONTENT'], batch_size=1000, n_process=N_PROCESSES)\n",
    "        for doc in tqdm(docs, total=len(df), desc=\"é¢„åˆ†è¯ä¸­\"):\n",
    "            tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "            raw_token_lists.append(tokens)\n",
    "\n",
    "    print(f\"\\né¢„åˆ†è¯å¤„ç†å®Œæˆï¼è€—æ—¶: {(time.time() - start_time_process)/60:.2f} åˆ†é’Ÿã€‚\")\n",
    "\n",
    "    print(f\"æ­£åœ¨ä¿å­˜é¢„åˆ†è¯ç»“æžœ ({len(raw_token_lists)}ç¯‡æ–‡ç« ) åˆ° {TOKEN_LISTS_PATH}...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'wb') as f:\n",
    "        pickle.dump(raw_token_lists, f)\n",
    "    print(\"ä¿å­˜æˆåŠŸï¼\")\n",
    "else:\n",
    "    print(\"DataFrameä¸ºç©ºæˆ–spaCyæ¨¡åž‹æœªåŠ è½½ï¼Œè·³è¿‡å¤„ç†ã€‚\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:11:14.566073Z",
     "start_time": "2025-07-30T09:10:42.909682Z"
    }
   },
   "id": "b041192e11a76d3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­¥éª¤ 1.1a: æ•°æ®åŠ è½½ ---\n",
      "æ•°æ®åŠ è½½å®Œæˆï¼è€—æ—¶: 0.04 ç§’ã€‚ æ•°æ®å½¢çŠ¶: (1000, 2)\n",
      "\n",
      "--- æ­¥éª¤ 1.1b: å¼€å§‹å¹¶è¡Œé¢„åˆ†è¯ ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "é¢„åˆ†è¯ä¸­:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba53445db8bf4219b42d68ede9789d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é¢„åˆ†è¯å¤„ç†å®Œæˆï¼è€—æ—¶: 0.53 åˆ†é’Ÿã€‚\n",
      "æ­£åœ¨ä¿å­˜é¢„åˆ†è¯ç»“æžœ (1000ç¯‡æ–‡ç« ) åˆ° ../data_processed/intermediate_raw_token_lists.pkl...\n",
      "ä¿å­˜æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ­¥éª¤ 1.2 - Gensim Phrases å‘çŽ°\n",
    "\n",
    "éœ€æ±‚: ä½¿ç”¨ä¸Šä¸€æ­¥çš„ raw_token_lists è®­ç»ƒ Phrases æ¨¡åž‹ã€‚\n",
    "å®žçŽ°: ä»Ž .pkl æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œç„¶åŽè¿›è¡Œè®­ç»ƒå’Œä¿å­˜ã€‚"
   ],
   "id": "d1ff7362e63509a5"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.2: Gensim Phrases å‘çŽ° ---\n",
    "if os.path.exists(TOKEN_LISTS_PATH):\n",
    "    print(f\"--- æ­¥éª¤ 1.2: å¼€å§‹ Gensim Phrases å‘çŽ° ---\")\n",
    "    print(f\"æ­£åœ¨ä»Ž {TOKEN_LISTS_PATH} åŠ è½½é¢„åˆ†è¯ç»“æžœ...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'rb') as f:\n",
    "        raw_token_lists_gensim = pickle.load(f)\n",
    "    print(\"åŠ è½½å®Œæˆã€‚\")\n",
    "\n",
    "    print(\"æ­£åœ¨ä¸ºGensim Phraseså‡†å¤‡å°å†™åŒ–çš„tokenåˆ—è¡¨...\")\n",
    "    documents_for_gensim = [[token.lower() for token in doc] for doc in tqdm(raw_token_lists_gensim, desc=\"è½¬å°å†™\")]\n",
    "    del raw_token_lists_gensim\n",
    "\n",
    "    print(\"\\nå¼€å§‹è®­ç»ƒPhrasesæ¨¡åž‹...\")\n",
    "    start_time = time.time()\n",
    "    min_count = 5 if TEST_MODE else 20\n",
    "    print(f\"ä½¿ç”¨ min_count = {min_count}\")\n",
    "    phrases_model = Phrases(documents_for_gensim, min_count=min_count, threshold=10.0, delimiter='_')\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Phrasesæ¨¡åž‹è®­ç»ƒå®Œæˆï¼è€—æ—¶: {train_time:.2f} ç§’ã€‚\")\n",
    "\n",
    "    print(\"\\næ­£åœ¨å¯¼å‡ºGensimå‘çŽ°çš„å€™é€‰çŸ­è¯­...\")\n",
    "    gensim_candidates_raw = phrases_model.export_phrases()\n",
    "\n",
    "    # ä½¿ç”¨æœ€ç®€å•ã€æœ€å…¼å®¹çš„äºŒå…ƒç»„è§£åŒ…\n",
    "    gensim_candidates = {\n",
    "        phrase.replace('_', ' '): score\n",
    "        for phrase, score in gensim_candidates_raw\n",
    "    }\n",
    "\n",
    "    print(f\"Gensim Phraseså‘çŽ°äº† {len(gensim_candidates)} ä¸ªå€™é€‰çŸ­è¯­ã€‚\")\n",
    "\n",
    "    print(f\"\\næ­£åœ¨å°†Gensimå€™é€‰ç»“æžœä¿å­˜åˆ° {CANDIDATES_GENSIM_PATH}...\")\n",
    "    with open(CANDIDATES_GENSIM_PATH, 'wb') as f:\n",
    "        pickle.dump(gensim_candidates, f)\n",
    "    print(\"ä¿å­˜æˆåŠŸï¼\")\n",
    "else:\n",
    "    print(f\"âŒ é”™è¯¯: Gensimçš„è¾“å…¥æ–‡ä»¶ {TOKEN_LISTS_PATH} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œæ­¥éª¤ 1.1ã€‚\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:11:15.759147Z",
     "start_time": "2025-07-30T09:11:14.577439Z"
    }
   },
   "id": "39e6bc84e9f5a316",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­¥éª¤ 1.2: å¼€å§‹ Gensim Phrases å‘çŽ° ---\n",
      "æ­£åœ¨ä»Ž ../data_processed/intermediate_raw_token_lists.pkl åŠ è½½é¢„åˆ†è¯ç»“æžœ...\n",
      "åŠ è½½å®Œæˆã€‚\n",
      "æ­£åœ¨ä¸ºGensim Phraseså‡†å¤‡å°å†™åŒ–çš„tokenåˆ—è¡¨...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "è½¬å°å†™:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "024bf10bba7e4e3780d6196d3ca7b1ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹è®­ç»ƒPhrasesæ¨¡åž‹...\n",
      "ä½¿ç”¨ min_count = 5\n",
      "Phrasesæ¨¡åž‹è®­ç»ƒå®Œæˆï¼è€—æ—¶: 0.67 ç§’ã€‚\n",
      "\n",
      "æ­£åœ¨å¯¼å‡ºGensimå‘çŽ°çš„å€™é€‰çŸ­è¯­...\n",
      "æ£€æµ‹åˆ°Gensimè¿”å›žå­—å…¸æ ¼å¼ï¼Œä½¿ç”¨ .items() è¿›è¡Œè§£åŒ…ã€‚\n",
      "Gensim Phraseså‘çŽ°äº† 3262 ä¸ªå€™é€‰çŸ­è¯­ã€‚\n",
      "\n",
      "æ­£åœ¨å°†Gensimå€™é€‰ç»“æžœä¿å­˜åˆ° ../data_processed/candidates_gensim.pkl...\n",
      "ä¿å­˜æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ­¥éª¤ 1.3 - å‘½åå®žä½“è¯†åˆ« (NER)\n",
    "\n",
    "éœ€æ±‚: ä»ŽåŽŸæ–‡ä¸­å¹¶è¡Œæå–å‘½åå®žä½“ã€‚\n",
    "å®žçŽ°: ä½¿ç”¨ nlp.pipe å¹¶è¡Œå¤„ç†ï¼Œåªå¯ç”¨ NER ç›¸å…³ç»„ä»¶ã€‚"
   ],
   "id": "c5cb1602462a0186"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:11:49.274956Z",
     "start_time": "2025-07-30T09:11:15.771376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.3: å¹¶è¡Œå‘½åå®žä½“è¯†åˆ« (NER) ---\n",
    "\n",
    "if 'df' in locals() and not df.empty and nlp:\n",
    "    print(\"\\n--- æ­¥éª¤ 1.3: å¼€å§‹å¹¶è¡ŒNERæå– ---\")\n",
    "    start_time_process = time.time()\n",
    "    ner_candidates = Counter()\n",
    "\n",
    "    print(f\"å°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹ã€‚\")\n",
    "    TARGET_ENTITY_LABELS = {'ORG', 'PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'PRODUCT', 'EVENT'}\n",
    "\n",
    "    # ä¸“æ³¨äºŽ NER, å¯ç”¨å…¶ä¾èµ–çš„ç»„ä»¶\n",
    "    with nlp.select_pipes(enable=[\"tok2vec\", \"tagger\", \"ner\"]):\n",
    "        docs = nlp.pipe(df['CONTENT'], batch_size=500, n_process=N_PROCESSES)\n",
    "        for doc in tqdm(docs, total=len(df), desc=\"NERæå–ä¸­\"):\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in TARGET_ENTITY_LABELS and ' ' in ent.text:\n",
    "                    ner_candidates[ent.text.strip().lower()] += 1\n",
    "\n",
    "    print(f\"\\nNERæå–å®Œæˆï¼è€—æ—¶: {(time.time() - start_time_process)/60:.2f} åˆ†é’Ÿã€‚\")\n",
    "\n",
    "    print(f\"æ­£åœ¨ä¿å­˜NERå€™é€‰ç»“æžœ ({len(ner_candidates)}ä¸ª) åˆ° {CANDIDATES_NER_PATH}...\")\n",
    "    with open(CANDIDATES_NER_PATH, 'wb') as f:\n",
    "        pickle.dump(ner_candidates, f)\n",
    "    print(\"ä¿å­˜æˆåŠŸï¼\")\n",
    "\n",
    "    # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ¸…ç†DataFrame\n",
    "    del ner_candidates\n",
    "    del df\n",
    "else:\n",
    "    print(\"DataFrame (df) æœªåŠ è½½æˆ–ä¸ºç©ºï¼Œè·³è¿‡NERæå–ã€‚\")\n",
    "\n",
    "print(\"\\n\\nâœ…âœ…âœ… é˜¶æ®µä¸€ï¼ˆç²¾ç®€ç‰ˆï¼‰å…¨éƒ¨å®Œæˆï¼âœ…âœ…âœ…\")\n",
    "print(\"Gensim å’Œ NER ä¸¤ç§æ¥æºçš„å€™é€‰çŸ­è¯­éƒ½å·²ç”Ÿæˆå¹¶ä¿å­˜ã€‚\")\n",
    "print(\"ä¸‹ä¸€æ­¥æ˜¯è¿è¡ŒåŽç»­çš„Notebookæ¥æ•´åˆå’Œå®¡æ ¸è¿™äº›ç»“æžœã€‚\")"
   ],
   "id": "84483f7d0a465376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ­¥éª¤ 1.3: å¼€å§‹å¹¶è¡ŒNERæå– ---\n",
      "å°†ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹ã€‚\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NERæå–ä¸­:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5f6f56f2daa49589bfcd51bab7e9fdd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NERæå–å®Œæˆï¼è€—æ—¶: 0.56 åˆ†é’Ÿã€‚\n",
      "æ­£åœ¨ä¿å­˜NERå€™é€‰ç»“æžœ (8493ä¸ª) åˆ° ../data_processed/candidates_ner.pkl...\n",
      "ä¿å­˜æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "552cfac281b6b864"
  }
 ],
 "execution_count": null
}
