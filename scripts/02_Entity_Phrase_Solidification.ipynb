{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 阶段一: 候选短语的自动化发现 (The Discovery Engine)\n",
    "\n",
    "**总目标:** 从约17万条中国新闻中，通过三种互补的算法（Gensim Phrases, spaCy NER, spaCy Noun Chunks），大规模挖掘潜在的、有意义的多词短语，为后续的人工审核环节提供一个高质量的“候选池”。\n",
    "\n",
    "---\n",
    "### 步骤 1.0: 环境设置与库导入"
   ],
   "id": "473538b1c279a8ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T07:42:29.562130Z",
     "start_time": "2025-07-29T07:42:29.168797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "from gensim.models.phrases import Phrases\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import pickle # 用于保存中间结果\n",
    "\n",
    "# --- 配置区 ---\n",
    "\n",
    "ALIYUN_OSS_PATH = ''    #ALIYUN_OSS_PATH = '/mnt/aliyun_oss/'\n",
    "\n",
    "# 输入文件: 筛选并去重后的中国相关文章数据 (CSV格式)\n",
    "INPUT_CHINA_DATA_PATH = ALIYUN_OSS_PATH + \"../data_processed/final_china_news.csv\"\n",
    "\n",
    "# 中间产出文件: 保存分词结果，避免重复运行耗时步骤\n",
    "TOKEN_LISTS_PATH = ALIYUN_OSS_PATH + \"data_processed/intermediate_raw_token_lists.pkl\"\n",
    "\n",
    "# 最终产出文件: 保存本阶段发现的所有候选短语，以便下一阶段整合\n",
    "CANDIDATES_GENSIM_PATH = ALIYUN_OSS_PATH + \"data_processed/candidates_gensim.pkl\"\n",
    "CANDIDATES_NER_PATH = ALIYUN_OSS_PATH + \"data_processed/candidates_ner.pkl\"\n",
    "CANDIDATES_NOUN_CHUNKS_PATH = ALIYUN_OSS_PATH + \"data_processed/candidates_noun_chunks.pkl\"\n",
    "\n",
    "print(\"--- 环境准备 ---\")\n",
    "print(f\"输入文件路径: {INPUT_CHINA_DATA_PATH}\")\n",
    "\n",
    "# --- 加载spaCy模型 ---\n",
    "# 这里我们需要完整的模型来进行NER和语法分析\n",
    "print(\"\\n正在加载spaCy模型 'en_core_web_md' (完整模式)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(f\"spaCy模型加载成功！耗时: {time.time() - start_time:.2f} 秒\")\n",
    "except OSError:\n",
    "    print(\"错误: spaCy模型 'en_core_web_md' 未安装。\")\n",
    "    print(\"请在你的终端或命令行中运行: python -m spacy download en_core_web_md\")\n"
   ],
   "id": "b5deacf9889d8c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 环境准备 ---\n",
      "输入文件路径: ../data_processed/final_china_news.csv\n",
      "\n",
      "正在加载spaCy模型 'en_core_web_md' (完整模式)...\n",
      "spaCy模型加载成功！耗时: 0.39 秒\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.1: 前置任务 - 文本预分词\n",
    "\n",
    "**逻辑与目的:**\n",
    "为了让后续的`Gensim Phrases`模型能够高效运行，我们需要先将长文本切分成单词列表。此处的处理非常轻量，目的是保留尽可能多的原始短语结构，包括大小写和特殊字符组合。\n",
    "\n",
    "**动作:**\n",
    "1. 加载数据。\n",
    "2. 创建分词器。\n",
    "3. 批量分词，只进行最基础的过滤（非标点、非空格），保留原始文本。\n",
    "4. 生成并保存内存对象 `raw_token_lists`。"
   ],
   "id": "fa0acf6370673b99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-29T07:42:29.573290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"\\n--- 步骤 1.1: 文本预分词 ---\")\n",
    "print(f\"正在从 {INPUT_CHINA_DATA_PATH} 读取数据...\")\n",
    "\n",
    "start_time = time.time()\n",
    "# 指定dtype为str，避免pandas自动推断类型出错\n",
    "df = pd.read_csv(INPUT_CHINA_DATA_PATH, dtype={'DATE': str, 'CONTENT': str})\n",
    "# 清理可能存在的空内容行\n",
    "df.dropna(subset=['CONTENT'], inplace=True)\n",
    "load_time = time.time() - start_time\n",
    "print(f\"数据加载完成！耗时: {load_time:.2f} 秒。 数据形状: {df.shape}\")\n",
    "\n",
    "# --- 批量分词 ---\n",
    "print(\"\\n开始批量分词（保留原始大小写和结构），这将花费一些时间...\")\n",
    "# 预估时间：17万条新闻，约15 - 40分钟\n",
    "start_time = time.time()\n",
    "\n",
    "raw_token_lists = []\n",
    "# 使用nlp.pipe进行高效批量处理\n",
    "# 我们只对CONTENT列进行操作\n",
    "for doc in tqdm(nlp.pipe(df['CONTENT'], batch_size=500), total=len(df), desc=\"预分词中\"):\n",
    "    # 过滤条件：只要token既不是纯标点符号，也不是纯空格，就保留其原始文本\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    raw_token_lists.append(tokens)\n",
    "\n",
    "tokenize_time = time.time() - start_time\n",
    "print(f\"预分词处理完成！耗时: {tokenize_time/60:.2f} 分钟。\")\n",
    "print(f\"已生成 {len(raw_token_lists)} 篇文章的原始token列表。\")\n",
    "print(\"\\n示例 (第一篇文章的前20个token):\")\n",
    "print(raw_token_lists[0][:20])\n",
    "\n",
    "# --- 保存中间结果 ---\n",
    "print(f\"\\n正在将预分词结果保存到 {TOKEN_LISTS_PATH}...\")\n",
    "with open(TOKEN_LISTS_PATH, 'wb') as f:\n",
    "    pickle.dump(raw_token_lists, f)\n",
    "print(\"保存成功！\")"
   ],
   "id": "b041192e11a76d3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 1.1: 文本预分词 ---\n",
      "正在从 ../data_processed/final_china_news.csv 读取数据...\n",
      "数据加载完成！耗时: 5.88 秒。 数据形状: (178273, 2)\n",
      "\n",
      "开始批量分词（保留原始大小写和结构），这将花费一些时间...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "预分词中:   0%|          | 0/178273 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     17\u001B[39m raw_token_lists = []\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# 使用nlp.pipe进行高效批量处理\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# 我们只对CONTENT列进行操作\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m tqdm(nlp.pipe(df[\u001B[33m'\u001B[39m\u001B[33mCONTENT\u001B[39m\u001B[33m'\u001B[39m], batch_size=\u001B[32m500\u001B[39m), total=\u001B[38;5;28mlen\u001B[39m(df), desc=\u001B[33m\"\u001B[39m\u001B[33m预分词中\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# 过滤条件：只要token既不是纯标点符号，也不是纯空格，就保留其原始文本\u001B[39;00m\n\u001B[32m     22\u001B[39m     tokens = [token.text \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token.is_punct \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token.is_space]\n\u001B[32m     23\u001B[39m     raw_token_lists.append(tokens)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m   1182\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[32m   1183\u001B[39m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[32m   1184\u001B[39m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1622\u001B[39m, in \u001B[36mLanguage.pipe\u001B[39m\u001B[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001B[39m\n\u001B[32m   1620\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1621\u001B[39m         docs = pipe(docs)\n\u001B[32m-> \u001B[39m\u001B[32m1622\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs:\n\u001B[32m   1623\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m doc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:245\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:245\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1619\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1616\u001B[39m     docs = \u001B[38;5;28mself\u001B[39m._multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001B[32m   1617\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1618\u001B[39m     \u001B[38;5;66;03m# if n_process == 1, no processes are forked.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1619\u001B[39m     docs = (\u001B[38;5;28mself\u001B[39m._ensure_doc(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts)\n\u001B[32m   1620\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1621\u001B[39m         docs = pipe(docs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1132\u001B[39m, in \u001B[36mLanguage._ensure_doc\u001B[39m\u001B[34m(self, doc_like)\u001B[39m\n\u001B[32m   1130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m doc_like\n\u001B[32m   1131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.make_doc(doc_like)\n\u001B[32m   1133\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[32m   1134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Doc(\u001B[38;5;28mself\u001B[39m.vocab).from_bytes(doc_like)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1124\u001B[39m, in \u001B[36mLanguage.make_doc\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m   1120\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) > \u001B[38;5;28mself\u001B[39m.max_length:\n\u001B[32m   1121\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1122\u001B[39m         Errors.E088.format(length=\u001B[38;5;28mlen\u001B[39m(text), max_length=\u001B[38;5;28mself\u001B[39m.max_length)\n\u001B[32m   1123\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tokenizer(text)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:160\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:196\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:400\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._tokenize\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:480\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._attach_tokens\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\vocab.pyx:205\u001B[39m, in \u001B[36mspacy.vocab.Vocab.get\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\vocab.pyx:234\u001B[39m, in \u001B[36mspacy.vocab.Vocab._new_lexeme\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\lang\\lex_attrs.py:145\u001B[39m, in \u001B[36mlower\u001B[39m\u001B[34m(string)\u001B[39m\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mlower\u001B[39m(string: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m string.lower()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.2: 候选发现策略一 - 统计共现 (Gensim `Phrases`)\n",
    "\n",
    "**逻辑与目的:**\n",
    "利用统计学原理，找出那些“粘合度”很高的、频繁在一起出现的单词组合。此方法对发现**专业术语**和高频实体特别有效。\n",
    "\n",
    "**动作:**\n",
    "1.  标准化输入：创建`raw_token_lists`的小写副本。\n",
    "2.  训练`Phrases`模型。\n",
    "3.  导出候选短语及其得分。"
   ],
   "id": "562b1bdb5b1b55b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\n--- 步骤 1.2: Gensim Phrases 发现 ---\")\n",
    "\n",
    "# --- 1. 标准化输入 ---\n",
    "print(\"正在为Gensim Phrases准备小写化的token列表...\")\n",
    "# 创建一个深拷贝，避免修改原始的raw_token_lists\n",
    "documents_for_gensim = [[token.lower() for token in doc] for doc in tqdm(raw_token_lists, desc=\"转小写\")]\n",
    "\n",
    "# --- 2. 训练模型 ---\n",
    "print(\"\\n开始训练Phrases模型...\")\n",
    "# 预估时间：17万篇文章，约1 - 10分钟\n",
    "start_time = time.time()\n",
    "# min_count: 一个词组至少要在语料中出现20次\n",
    "# threshold: 粘合度阈值，10是一个常用的起点\n",
    "phrases_model = Phrases(documents_for_gensim, min_count=20, threshold=10.0, delimiter=b'_')\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Phrases模型训练完成！耗时: {train_time:.2f} 秒。\")\n",
    "\n",
    "# --- 3. 导出候选 ---\n",
    "print(\"\\n正在导出Gensim发现的候选短语...\")\n",
    "# phrases_model.export_phrases() 返回一个迭代器，我们将其转换为字典\n",
    "gensim_candidates_raw = phrases_model.export_phrases(documents_for_gensim)\n",
    "# 将bytes转换为utf-8字符串，并存入字典\n",
    "gensim_candidates = {phrase.decode('utf-8'): score for phrase, score in gensim_candidates_raw}\n",
    "print(f\"Gensim Phrases发现了 {len(gensim_candidates)} 个候选短语。\")\n",
    "\n",
    "# --- 保存结果 ---\n",
    "print(f\"\\n正在将Gensim候选结果保存到 {CANDIDATES_GENSIM_PATH}...\")\n",
    "with open(CANDIDATES_GENSIM_PATH, 'wb') as f:\n",
    "    pickle.dump(gensim_candidates, f)\n",
    "print(\"保存成功！\")"
   ],
   "id": "8297a5529535219a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.3: 候选发现策略二 - 命名实体识别 (spaCy NER)\n",
    "\n",
    "**逻辑与目的:**\n",
    "利用预训练的深度学习模型，直接、精准地识别出文本中的人名、地名、组织机构名等**命名实体**。\n",
    "\n",
    "**动作:**\n",
    "1.  批量识别原始文本。\n",
    "2.  提取我们感兴趣的实体类型。\n",
    "3.  计数并收集。"
   ],
   "id": "ff06593c486f7183"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\n--- 步骤 1.3: spaCy NER 发现 ---\")\n",
    "print(\"开始批量提取命名实体，这将是本阶段最耗时的步骤之一...\")\n",
    "# 预估时间：17万篇文章，约 0.75 - 2 小时\n",
    "start_time = time.time()\n",
    "\n",
    "ner_candidates = Counter()\n",
    "# 定义我们感兴趣的实体类型\n",
    "TARGET_ENTITY_LABELS = {'ORG', 'PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'PRODUCT', 'EVENT'}\n",
    "\n",
    "# 再次使用高效的nlp.pipe，在原始CONTENT列上操作\n",
    "for doc in tqdm(nlp.pipe(df['CONTENT'], batch_size=200), total=len(df), desc=\"NER实体提取中\"):\n",
    "    for ent in doc.ents:\n",
    "        # 检查实体标签是否在我们感兴趣的范围内\n",
    "        if ent.label_ in TARGET_ENTITY_LABELS:\n",
    "            # 清理实体文本，移除首尾空格\n",
    "            entity_text = ent.text.strip()\n",
    "            # 我们只对包含空格的多词实体感兴趣\n",
    "            if ' ' in entity_text:\n",
    "                # 转为小写后计入Counter，以实现聚合\n",
    "                ner_candidates[entity_text.lower()] += 1\n",
    "\n",
    "ner_time = time.time() - start_time\n",
    "print(f\"命名实体提取完成！耗时: {ner_time/60:.2f} 分钟。\")\n",
    "print(f\"spaCy NER提取了 {len(ner_candidates)} 个唯一的多词实体候选。\")\n",
    "\n",
    "# --- 保存结果 ---\n",
    "print(f\"\\n正在将NER候选结果保存到 {CANDIDATES_NER_PATH}...\")\n",
    "with open(CANDIDATES_NER_PATH, 'wb') as f:\n",
    "    pickle.dump(ner_candidates, f)\n",
    "print(\"保存成功！\")"
   ],
   "id": "721636d37477e39e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.4: 候选发现策略三 - 语法结构分析 (spaCy `noun_chunks`)\n",
    "\n",
    "**逻辑与目的:**\n",
    "作为补充，利用语法分析器找出所有符合“名词短语”结构的部分，捕获非标准实体和描述性短语。\n",
    "\n",
    "**动作:**\n",
    "1.  批量分析原始文本。\n",
    "2.  提取名词短语。\n",
    "3.  清洗并计数。"
   ],
   "id": "c178dad59390c7d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\n--- 步骤 1.4: spaCy Noun Chunks 发现 ---\")\n",
    "print(\"开始批量提取名词短语，这也将花费较长时间...\")\n",
    "# 预估时间：17万篇文章，约 0.75 - 2 小时\n",
    "start_time = time.time()\n",
    "\n",
    "noun_chunk_candidates = Counter()\n",
    "# 定义一个简单的限定词列表，用于清理\n",
    "determiners = {'the', 'a', 'an', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "# 同样在原始CONTENT列上操作\n",
    "for doc in tqdm(nlp.pipe(df['CONTENT'], batch_size=200), total=len(df), desc=\"名词短语提取中\"):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # 1. 获取名词短语文本并转为小写\n",
    "        phrase_text = chunk.text.lower()\n",
    "\n",
    "        # 2. 清理首尾的冠词/限定词\n",
    "        words = phrase_text.split()\n",
    "        if words:\n",
    "            if words[0] in determiners:\n",
    "                words = words[1:]\n",
    "            if words and words[-1] in determiners:\n",
    "                words = words[:-1]\n",
    "\n",
    "        cleaned_phrase = \" \".join(words).strip()\n",
    "\n",
    "        # 3. 过滤掉不合格的短语\n",
    "        if cleaned_phrase and ' ' in cleaned_phrase: # 确保是多词短语\n",
    "            noun_chunk_candidates[cleaned_phrase] += 1\n",
    "\n",
    "noun_chunk_time = time.time() - start_time\n",
    "print(f\"名词短语提取完成！耗时: {noun_chunk_time/60:.2f} 分钟。\")\n",
    "print(f\"spaCy提取了 {len(noun_chunk_candidates)} 个唯一名词短语候选。\")\n",
    "\n",
    "# --- 保存结果 ---\n",
    "print(f\"\\n正在将名词短语候选结果保存到 {CANDIDATES_NOUN_CHUNKS_PATH}...\")\n",
    "with open(CANDIDATES_NOUN_CHUNKS_PATH, 'wb') as f:\n",
    "    pickle.dump(noun_chunk_candidates, f)\n",
    "print(\"保存成功！\")\n",
    "\n",
    "print(\"\\n\\n=== 阶段一全部完成！ ===\")\n",
    "print(\"所有三种来源的候选短语都已生成并保存。下一步是运行 '05_Candidate_Integration_and_Review.ipynb' 来整合和审核这些结果。\")"
   ],
   "id": "5b3d68923874d5a0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
