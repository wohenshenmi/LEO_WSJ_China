{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# é˜¶æ®µä¸€: å€™é€‰çŸ­è¯­çš„è‡ªåŠ¨åŒ–å‘çŽ° (The Discovery Engine)\n",
    "\n",
    "**æ€»ç›®æ ‡:** ä»Žçº¦17ä¸‡æ¡ä¸­å›½æ–°é—»ä¸­ï¼Œé€šè¿‡ä¸‰ç§äº’è¡¥çš„ç®—æ³•ï¼ˆGensim Phrases, spaCy NER, spaCy Noun Chunksï¼‰ï¼Œå¤§è§„æ¨¡æŒ–æŽ˜æ½œåœ¨çš„ã€æœ‰æ„ä¹‰çš„å¤šè¯çŸ­è¯­ï¼Œä¸ºåŽç»­çš„äººå·¥å®¡æ ¸çŽ¯èŠ‚æä¾›ä¸€ä¸ªé«˜è´¨é‡çš„â€œå€™é€‰æ± â€ã€‚\n",
    "\n",
    "---\n",
    "### æ­¥éª¤ 1.0 - çŽ¯å¢ƒè®¾ç½®ä¸Žåº“å¯¼å…¥"
   ],
   "id": "6803724fbd5fc279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:14:16.502652Z",
     "start_time": "2025-08-01T13:14:13.910842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.0: çŽ¯å¢ƒè®¾ç½®ä¸Žåº“å¯¼å…¥  ---\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from gensim.models.phrases import Phrases\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm # ç”¨äºŽæ˜¾ç¤ºç¾Žè§‚çš„è¿›åº¦æ¡\n",
    "import pickle # ç”¨äºŽé«˜æ•ˆåœ°ä¿å­˜å’ŒåŠ è½½Pythonå¯¹è±¡ (å¦‚åˆ—è¡¨ã€å­—å…¸)\n",
    "import multiprocessing as mp # Pythonå†…ç½®çš„å¤šè¿›ç¨‹åº“\n",
    "import psutil # ç”¨äºŽæ™ºèƒ½æ£€æµ‹CPUæ ¸å¿ƒæ•°\n",
    "import shutil # ç”¨äºŽåœ¨æœåŠ¡å™¨æ¨¡å¼ä¸‹é«˜æ•ˆå¤åˆ¶æ–‡ä»¶\n",
    "\n",
    "# ==============================================================================\n",
    "# --- æ ¸å¿ƒé…ç½®åŒº (é€šå¸¸æ‚¨åªéœ€è¦ä¿®æ”¹è¿™éƒ¨åˆ†) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. çŽ¯å¢ƒé…ç½®\n",
    "# ç›®çš„: è‡ªåŠ¨é€‚é…ä¸åŒè¿è¡ŒçŽ¯å¢ƒï¼Œè§£å†³æœåŠ¡å™¨ä¸Šå› ç½‘ç»œå­˜å‚¨(NFS)å¯¼è‡´çš„I/Oé”™è¯¯\n",
    "#       æˆ–æœ¬åœ°ä¸ŽæœåŠ¡å™¨ç›®å½•ç»“æž„ä¸ä¸€è‡´çš„é—®é¢˜ã€‚\n",
    "# é€‰é¡¹:\n",
    "#   'local': åœ¨æ‚¨è‡ªå·±çš„ç”µè„‘ä¸Šè¿è¡Œã€‚ä»£ç å°†ä½¿ç”¨ç›¸å¯¹è·¯å¾„ (å¦‚ ../data_processed)ã€‚\n",
    "#   'dsw':   åœ¨é˜¿é‡Œäº‘DSWæˆ–å…¶ä»–è¿œç¨‹æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚ä»£ç å°†ä½¿ç”¨ç»å¯¹è·¯å¾„\n",
    "#            (å¦‚ /mnt/data/data_processed)ï¼Œå¹¶æ™ºèƒ½åœ°å°†I/Oå¯†é›†åž‹æ“ä½œé‡å®šå‘\n",
    "#            åˆ°æœåŠ¡å™¨æœ¬åœ°çš„ /tmp ç›®å½•ï¼Œä»¥èŽ·å¾—æœ€ä½³ç¨³å®šæ€§å’Œé€Ÿåº¦ã€‚\n",
    "RUNNING_ENV = 'local'\n",
    "\n",
    "# 2. å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¼€å…³\n",
    "# ç›®çš„: ç”¨äºŽå¿«é€Ÿè°ƒè¯•ä»£ç é€»è¾‘ï¼Œé¿å…å¤„ç†å…¨é‡æ•°æ®è€—è´¹å¤§é‡æ—¶é—´ã€‚\n",
    "# æ•ˆæžœ:\n",
    "#   True:  åªå¤„ç†å¼€å¤´æŒ‡å®šæ•°é‡(TEST_SAMPLE_SIZE)çš„æ–°é—»ï¼Œ\n",
    "#          å¯ä»¥å¿«é€ŸéªŒè¯æ•´ä¸ªæµç¨‹æ˜¯å¦é€šç•…ã€‚\n",
    "#   False: å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œç”¨äºŽæ­£å¼äº§å‡ºæœ€ç»ˆç»“æžœã€‚\n",
    "TEST_MODE = True\n",
    "TEST_SAMPLE_SIZE = 1000 # æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œä»Žæºæ–‡ä»¶è¯»å–çš„æœ€å¤§è¡Œæ•°\n",
    "\n",
    "# =============================================================================\n",
    "# --- è·¯å¾„æ™ºèƒ½ç®¡ç† (æ ¹æ®ä¸Šé¢çš„ RUNNING_ENV è‡ªåŠ¨é…ç½®ï¼Œé€šå¸¸æ— éœ€ä¿®æ”¹) ---\n",
    "# =============================================================================\n",
    "print(f\"æ£€æµ‹åˆ°è¿è¡ŒçŽ¯å¢ƒä¸º: ã€{RUNNING_ENV.upper()}ã€‘\")\n",
    "TEMP_DIR = '/tmp' # å®šä¹‰æœåŠ¡å™¨çš„æœ¬åœ°ä¸´æ—¶ç›®å½•\n",
    "\n",
    "# æ ¹æ®çŽ¯å¢ƒå®šä¹‰æ•°æ®æ ¹è·¯å¾„\n",
    "if RUNNING_ENV == 'local':\n",
    "    # æœ¬åœ°æ¨¡å¼: ä½¿ç”¨ç›¸å¯¹äºŽå½“å‰è„šæœ¬(é€šå¸¸åœ¨ 'scripts' ç›®å½•)çš„è·¯å¾„\n",
    "    print(\"ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    # DSWæ¨¡å¼: ä½¿ç”¨æœåŠ¡å™¨ä¸ŠæŒ‚è½½å­˜å‚¨çš„ç»å¯¹è·¯å¾„\n",
    "    print(\"ä½¿ç”¨ 'dsw' æ¨¡å¼çš„ç»å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "    if not os.path.exists(TEMP_DIR): os.makedirs(TEMP_DIR) # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨\n",
    "else:\n",
    "    # é”™è¯¯å¤„ç†ï¼Œé˜²æ­¢é…ç½®é”™è¯¯\n",
    "    raise ValueError(f\"æœªçŸ¥çš„ RUNNING_ENV: '{RUNNING_ENV}'. è¯·é€‰æ‹© 'local' æˆ– 'dsw'ã€‚\")\n",
    "\n",
    "# --- å®šä¹‰æ‰€æœ‰â€œåŽŸå§‹â€æ–‡ä»¶è·¯å¾„ (è¿™äº›è·¯å¾„æŒ‡å‘ä½ çš„ç½‘ç»œå­˜å‚¨æˆ–é¡¹ç›®ç›®å½•) ---\n",
    "INPUT_CHINA_DATA_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'final_china_news.csv')\n",
    "TOKEN_LISTS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'intermediate_raw_token_lists.pkl')\n",
    "CANDIDATES_GENSIM_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidates_gensim.pkl')\n",
    "CANDIDATES_NER_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidates_ner.pkl')\n",
    "\n",
    "# --- åˆå§‹åŒ–å°†è¦åœ¨åŽç»­æµç¨‹ä¸­å®žé™…ä½¿ç”¨çš„è·¯å¾„å˜é‡ï¼Œé»˜è®¤æŒ‡å‘åŽŸå§‹è·¯å¾„ ---\n",
    "INPUT_CHINA_DATA_PATH = INPUT_CHINA_DATA_ORIGINAL\n",
    "TOKEN_LISTS_PATH = TOKEN_LISTS_ORIGINAL\n",
    "CANDIDATES_GENSIM_PATH = CANDIDATES_GENSIM_ORIGINAL\n",
    "CANDIDATES_NER_PATH = CANDIDATES_NER_ORIGINAL\n",
    "\n",
    "# --- DSWçŽ¯å¢ƒä¸‹çš„I/Oä¼˜åŒ–ï¼šé‡å®šå‘é«˜è´Ÿè½½çš„è¯»å†™è·¯å¾„åˆ° /tmp ---\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW çŽ¯å¢ƒæ¨¡å¼å·²æ¿€æ´»ï¼Œä¸ºé¿å…I/Oé”™è¯¯ï¼Œå°†ä½¿ç”¨æœ¬åœ°ä¸´æ—¶ç›®å½• /tmp ...\")\n",
    "    # å®šä¹‰æ‰€æœ‰åœ¨æœ¬åœ°ä¸´æ—¶ç›®å½•ä¸­å¯¹åº”çš„æ–‡ä»¶è·¯å¾„\n",
    "    TEMP_INPUT_CHINA_DATA = os.path.join(TEMP_DIR, 'final_china_news.csv')\n",
    "    TEMP_TOKEN_LISTS = os.path.join(TEMP_DIR, 'intermediate_raw_token_lists.pkl')\n",
    "    TEMP_CANDIDATES_GENSIM = os.path.join(TEMP_DIR, 'candidates_gensim.pkl')\n",
    "    TEMP_CANDIDATES_NER = os.path.join(TEMP_DIR, 'candidates_ner.pkl')\n",
    "\n",
    "    try:\n",
    "        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æžœä¸å­˜åœ¨ï¼Œæ— æ³•ç»§ç»­\n",
    "        if not os.path.exists(INPUT_CHINA_DATA_ORIGINAL):\n",
    "            raise FileNotFoundError(f\"åœ¨DSWçš„æºè·¯å¾„ {INPUT_CHINA_DATA_ORIGINAL} æœªæ‰¾åˆ°æ–‡ä»¶ï¼\")\n",
    "\n",
    "        source_size = os.path.getsize(INPUT_CHINA_DATA_ORIGINAL)\n",
    "        temp_exists = os.path.exists(TEMP_INPUT_CHINA_DATA)\n",
    "\n",
    "        # æ™ºèƒ½å¤åˆ¶ï¼šä»…åœ¨ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨æˆ–å¤§å°ä¸Žæºæ–‡ä»¶ä¸ä¸€è‡´æ—¶æ‰æ‰§è¡Œå¤åˆ¶æ“ä½œï¼Œé¿å…é‡å¤å·¥ä½œ\n",
    "        if not temp_exists or os.path.getsize(TEMP_INPUT_CHINA_DATA) != source_size:\n",
    "            print(f\"æ­£åœ¨ä»Ž {INPUT_CHINA_DATA_ORIGINAL} å¤åˆ¶åˆ° {TEMP_INPUT_CHINA_DATA} (é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...\")\n",
    "            shutil.copy(INPUT_CHINA_DATA_ORIGINAL, TEMP_INPUT_CHINA_DATA)\n",
    "            print(\"å¤åˆ¶å®Œæˆã€‚\")\n",
    "        else:\n",
    "            print(f\"ä¸´æ—¶è¾“å…¥æ–‡ä»¶ {TEMP_INPUT_CHINA_DATA} å·²å­˜åœ¨ä¸”å¤§å°ä¸€è‡´ï¼Œè·³è¿‡å¤åˆ¶ã€‚\")\n",
    "\n",
    "        # [å…³é”®] é‡å®šå‘è·¯å¾„å˜é‡ï¼Œè®©åŽç»­ä»£ç å—é€æ˜Žåœ°ä½¿ç”¨/tmpä¸‹çš„æ–‡ä»¶\n",
    "        INPUT_CHINA_DATA_PATH = TEMP_INPUT_CHINA_DATA # é¢„åˆ†è¯å°†è¯»å–è¿™ä¸ªä¸´æ—¶æ–‡ä»¶\n",
    "        TOKEN_LISTS_PATH = TEMP_TOKEN_LISTS           # é¢„åˆ†è¯çš„ç»“æžœå°†å†™å…¥è¿™é‡Œ\n",
    "        CANDIDATES_GENSIM_PATH = TEMP_CANDIDATES_GENSIM # Gensimçš„ç»“æžœå°†å†™å…¥è¿™é‡Œ\n",
    "        CANDIDATES_NER_PATH = TEMP_CANDIDATES_NER       # NERçš„ç»“æžœå°†å†™å…¥è¿™é‡Œ\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç†DSWä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "        # å¦‚æžœå‡ºé”™ï¼Œåœæ­¢æ‰§è¡Œï¼Œå› ä¸ºåœ¨DSWä¸Šä½¿ç”¨åŽŸå§‹ç½‘ç»œè·¯å¾„é£Žé™©å¾ˆé«˜\n",
    "        raise e\n",
    "\n",
    "# =============================================================================\n",
    "# --- å…¨å±€å¤„ç†ä¸Žå¯åŠ¨ä¿¡æ¯ ---\n",
    "# =============================================================================\n",
    "\n",
    "# --- å¹¶è¡Œå¤„ç†é…ç½® ---\n",
    "# ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ç‰©ç†CPUæ ¸å¿ƒï¼Œå¹¶ä¿ç•™ä¸€ä¸ªæ ¸å¿ƒç»™æ“ä½œç³»ç»Ÿï¼Œä»¥é¿å…ç³»ç»Ÿå¡é¡¿\n",
    "# logical=False åœ¨ç‰©ç†æœºæˆ–é«˜æ€§èƒ½è™šæ‹Ÿæœºä¸Šæ›´ç¨³å¥ï¼Œé¿å…è¶…çº¿ç¨‹å¹²æ‰°\n",
    "# æœ€å¤šä½¿ç”¨8ä¸ªè¿›ç¨‹ï¼Œé˜²æ­¢åœ¨æ ¸å¿ƒæ•°æžå¤šçš„æœåŠ¡å™¨ä¸Šè¿‡åº¦æ¶ˆè€—èµ„æº\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "# --- æ‰“å°æœ€ç»ˆé…ç½®ä¿¡æ¯ï¼Œæ–¹ä¾¿è¿½æº¯å’Œè°ƒè¯• ---\n",
    "print(\"\\n--- çŽ¯å¢ƒå‡†å¤‡ ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"ðŸš€ðŸš€ðŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†å‰ {TEST_SAMPLE_SIZE} è¡Œæ•°æ®ï¼ðŸš€ðŸš€ðŸš€\")\n",
    "else:\n",
    "    print(\"ðŸš¢ðŸš¢ðŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®ã€‚ðŸš¢ðŸš¢ðŸš¢\")\n",
    "print(f\"æœ€ç»ˆè¾“å…¥æ–‡ä»¶è·¯å¾„ (è¯»): {INPUT_CHINA_DATA_PATH}\")\n",
    "print(f\"æœ€ç»ˆTokenåˆ—è¡¨è·¯å¾„ (å†™/è¯»): {TOKEN_LISTS_PATH}\")\n",
    "print(f\"Gensimå€™é€‰è·¯å¾„ (å†™): {CANDIDATES_GENSIM_PATH}\")\n",
    "print(f\"NERå€™é€‰è·¯å¾„ (å†™): {CANDIDATES_NER_PATH}\")\n",
    "print(f\"å°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\")\n",
    "\n",
    "# --- åŠ è½½spaCyæ¨¡åž‹ ---\n",
    "print(\"\\næ­£åœ¨åŠ è½½spaCyæ¨¡åž‹ 'en_core_web_lg' (æŽ¨èä½¿ç”¨ large æ¨¡åž‹ä»¥èŽ·å¾—æ›´å¥½çš„NERæ•ˆæžœ)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # 'en_core_web_lg' åŒ…å«è¯å‘é‡ï¼Œå¯¹NERç­‰ä»»åŠ¡æœ‰å¸®åŠ©\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(f\"spaCyæ¨¡åž‹ '{nlp.meta['name']}' åŠ è½½æˆåŠŸï¼è€—æ—¶: {time.time() - start_time:.2f} ç§’\")\n",
    "except OSError:\n",
    "    print(\"é”™è¯¯: spaCyæ¨¡åž‹ 'en_core_web_lg' æœªå®‰è£…ã€‚\")\n",
    "    print(\"è¯·åœ¨ä½ çš„ç»ˆç«¯æˆ–å‘½ä»¤è¡Œä¸­è¿è¡Œ: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None"
   ],
   "id": "3d437f5f4516c145",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ°è¿è¡ŒçŽ¯å¢ƒä¸º: ã€LOCALã€‘\n",
      "ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\n",
      "\n",
      "--- çŽ¯å¢ƒå‡†å¤‡ ---\n",
      "ðŸš€ðŸš€ðŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†å‰ 1000 è¡Œæ•°æ®ï¼ðŸš€ðŸš€ðŸš€\n",
      "æœ€ç»ˆè¾“å…¥æ–‡ä»¶è·¯å¾„ (è¯»): ../data_processed\\final_china_news.csv\n",
      "æœ€ç»ˆTokenåˆ—è¡¨è·¯å¾„ (å†™/è¯»): ../data_processed\\intermediate_raw_token_lists.pkl\n",
      "Gensimå€™é€‰è·¯å¾„ (å†™): ../data_processed\\candidates_gensim.pkl\n",
      "NERå€™é€‰è·¯å¾„ (å†™): ../data_processed\\candidates_ner.pkl\n",
      "å°†ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\n",
      "\n",
      "æ­£åœ¨åŠ è½½spaCyæ¨¡åž‹ 'en_core_web_lg' (æŽ¨èä½¿ç”¨ large æ¨¡åž‹ä»¥èŽ·å¾—æ›´å¥½çš„NERæ•ˆæžœ)...\n",
      "spaCyæ¨¡åž‹ 'core_web_lg' åŠ è½½æˆåŠŸï¼è€—æ—¶: 1.17 ç§’\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ­¥éª¤ 1.1 - æ–‡æœ¬é¢„åˆ†è¯\n",
    "\n",
    "éœ€æ±‚: ä¸º Gensim å‡†å¤‡ raw_token_listsã€‚\n",
    "å®žçŽ°: ä½¿ç”¨å¹¶è¡ŒåŒ–çš„ nlp.pipeï¼Œä½†åªå¯ç”¨åˆ†è¯åŠŸèƒ½ä»¥èŽ·å¾—æœ€é«˜é€Ÿåº¦ã€‚"
   ],
   "id": "ae1fe150798f3581"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:14:46.524539Z",
     "start_time": "2025-08-01T13:14:16.651722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.1: å¹¶è¡Œé¢„åˆ†è¯ ---\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "print(\"--- æ­¥éª¤ 1.1a: æ•°æ®åŠ è½½ ---\")\n",
    "start_time_load = time.time()\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    # ä½¿ç”¨åœ¨å—1ä¸­é…ç½®å¥½çš„è·¯å¾„å˜é‡\n",
    "    df = pd.read_csv(INPUT_CHINA_DATA_PATH, dtype=str, nrows=read_nrows)\n",
    "    # ä½ çš„åŽŸé€»è¾‘æ˜¯'CONTENT'ï¼Œä½†ä½ çš„æ–‡ä»¶åˆ—åå¯èƒ½æ˜¯'body'ç­‰ï¼Œè¿™é‡Œå‡è®¾'CONTENT'\n",
    "    # å¦‚æžœæŠ¥é”™ KeyError: 'CONTENT'ï¼Œè¯·æ£€æŸ¥ä½ çš„ final_china_news.csv çš„åˆ—å\n",
    "    df.dropna(subset=['CONTENT'], inplace=True)\n",
    "    print(f\"æ•°æ®åŠ è½½å®Œæˆï¼è€—æ—¶: {time.time() - start_time_load:.2f} ç§’ã€‚ æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_CHINA_DATA_PATH}\")\n",
    "except KeyError:\n",
    "    print(f\"âŒ é”™è¯¯: åœ¨ {INPUT_CHINA_DATA_PATH} ä¸­æ‰¾ä¸åˆ°åä¸º 'CONTENT' çš„åˆ—ã€‚è¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹ã€‚\")\n",
    "\n",
    "\n",
    "if not df.empty and nlp:\n",
    "    print(\"\\n--- æ­¥éª¤ 1.1b: å¼€å§‹å¹¶è¡Œé¢„åˆ†è¯ ---\")\n",
    "    start_time_process = time.time()\n",
    "    raw_token_lists = []\n",
    "\n",
    "    # ç¦ç”¨æ‰€æœ‰ä¸éœ€è¦çš„ç®¡é“ç»„ä»¶ä»¥èŽ·å¾—æœ€å¿«é€Ÿåº¦\n",
    "    disabled_pipes = [pipe for pipe in nlp.pipe_names if pipe not in ['tok2vec']]\n",
    "\n",
    "    with nlp.select_pipes(disable=disabled_pipes):\n",
    "        docs = nlp.pipe(df['CONTENT'], batch_size=1000, n_process=N_PROCESSES)\n",
    "        for doc in tqdm(docs, total=len(df), desc=\"é¢„åˆ†è¯ä¸­\"):\n",
    "            tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "            raw_token_lists.append(tokens)\n",
    "\n",
    "    print(f\"\\né¢„åˆ†è¯å¤„ç†å®Œæˆï¼è€—æ—¶: {(time.time() - start_time_process)/60:.2f} åˆ†é’Ÿã€‚\")\n",
    "\n",
    "    # ä½¿ç”¨åœ¨å—1ä¸­é…ç½®å¥½çš„è·¯å¾„å˜é‡\n",
    "    print(f\"æ­£åœ¨ä¿å­˜é¢„åˆ†è¯ç»“æžœ ({len(raw_token_lists)}ç¯‡æ–‡ç« ) åˆ° {TOKEN_LISTS_PATH}...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'wb') as f:\n",
    "        pickle.dump(raw_token_lists, f)\n",
    "    print(\"ä¿å­˜æˆåŠŸï¼\")\n",
    "else:\n",
    "    print(\"DataFrameä¸ºç©ºæˆ–spaCyæ¨¡åž‹æœªåŠ è½½ï¼Œè·³è¿‡å¤„ç†ã€‚\")"
   ],
   "id": "9a69b280a1c61df2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­¥éª¤ 1.1a: æ•°æ®åŠ è½½ ---\n",
      "æ•°æ®åŠ è½½å®Œæˆï¼è€—æ—¶: 0.04 ç§’ã€‚ æ•°æ®å½¢çŠ¶: (1000, 3)\n",
      "\n",
      "--- æ­¥éª¤ 1.1b: å¼€å§‹å¹¶è¡Œé¢„åˆ†è¯ ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "é¢„åˆ†è¯ä¸­:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff6425a593cd4d1c8593024ad8713570"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é¢„åˆ†è¯å¤„ç†å®Œæˆï¼è€—æ—¶: 0.50 åˆ†é’Ÿã€‚\n",
      "æ­£åœ¨ä¿å­˜é¢„åˆ†è¯ç»“æžœ (1000ç¯‡æ–‡ç« ) åˆ° ../data_processed\\intermediate_raw_token_lists.pkl...\n",
      "ä¿å­˜æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ­¥éª¤ 1.2 - Gensim Phrases å‘çŽ°\n",
    "\n",
    "éœ€æ±‚: ä½¿ç”¨ä¸Šä¸€æ­¥çš„ raw_token_lists è®­ç»ƒ Phrases æ¨¡åž‹ã€‚\n",
    "å®žçŽ°: ä»Ž .pkl æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œç„¶åŽè¿›è¡Œè®­ç»ƒå’Œä¿å­˜ã€‚"
   ],
   "id": "b636daf25653b7cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:22:20.066463Z",
     "start_time": "2025-08-01T13:22:18.920878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.2: Gensim Phrases å‘çŽ° (å·²æ ¹æ®æ­£ç¡®è¯Šæ–­è¿›è¡Œæœ€ç»ˆä¿®å¤) ---\n",
    "if os.path.exists(TOKEN_LISTS_PATH):\n",
    "    print(f\"--- æ­¥éª¤ 1.2: å¼€å§‹ Gensim Phrases å‘çŽ° ---\")\n",
    "    print(f\"æ­£åœ¨ä»Ž {TOKEN_LISTS_PATH} åŠ è½½é¢„åˆ†è¯ç»“æžœ...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'rb') as f:\n",
    "        raw_token_lists_gensim = pickle.load(f)\n",
    "    print(\"åŠ è½½å®Œæˆã€‚\")\n",
    "\n",
    "    print(\"æ­£åœ¨ä¸ºGensim Phraseså‡†å¤‡å°å†™åŒ–çš„tokenåˆ—è¡¨...\")\n",
    "    documents_for_gensim = [[token.lower() for token in doc] for doc in tqdm(raw_token_lists_gensim, desc=\"è½¬å°å†™\")]\n",
    "    del raw_token_lists_gensim\n",
    "\n",
    "    print(\"\\nå¼€å§‹è®­ç»ƒPhrasesæ¨¡åž‹...\")\n",
    "    start_time = time.time()\n",
    "    min_count = 5 if TEST_MODE else 20\n",
    "    print(f\"ä½¿ç”¨ min_count = {min_count}\")\n",
    "    # æ³¨æ„ï¼šåœ¨è¿”å›žå­—å…¸çš„æ—§ç‰ˆGensimä¸­ï¼Œåˆ†éš”ç¬¦é€šå¸¸æ˜¯æ™®é€šå­—ç¬¦ä¸²\n",
    "    phrases_model = Phrases(documents_for_gensim, min_count=min_count, threshold=10.0, delimiter='_')\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Phrasesæ¨¡åž‹è®­ç»ƒå®Œæˆï¼è€—æ—¶: {train_time:.2f} ç§’ã€‚\")\n",
    "\n",
    "    print(\"\\næ­£åœ¨å¯¼å‡ºGensimå‘çŽ°çš„å€™é€‰çŸ­è¯­...\")\n",
    "    gensim_candidates_raw = phrases_model.export_phrases()\n",
    "\n",
    "    gensim_candidates = {\n",
    "        phrase.replace('_', ' '): score\n",
    "        for phrase, score in gensim_candidates_raw.items()\n",
    "    }\n",
    "\n",
    "    print(f\"Gensim Phraseså‘çŽ°äº† {len(gensim_candidates)} ä¸ªå€™é€‰çŸ­è¯­ã€‚\")\n",
    "\n",
    "    print(f\"\\næ­£åœ¨å°†Gensimå€™é€‰ç»“æžœä¿å­˜åˆ° {CANDIDATES_GENSIM_PATH}...\")\n",
    "    with open(CANDIDATES_GENSIM_PATH, 'wb') as f:\n",
    "        pickle.dump(gensim_candidates, f)\n",
    "    print(\"ä¿å­˜æˆåŠŸï¼\")\n",
    "else:\n",
    "    print(f\"âŒ é”™è¯¯: Gensimçš„è¾“å…¥æ–‡ä»¶ {TOKEN_LISTS_PATH} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œæ­¥éª¤ 1.1ã€‚\")"
   ],
   "id": "59e3749840b4aa71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­¥éª¤ 1.2: å¼€å§‹ Gensim Phrases å‘çŽ° ---\n",
      "æ­£åœ¨ä»Ž ../data_processed\\intermediate_raw_token_lists.pkl åŠ è½½é¢„åˆ†è¯ç»“æžœ...\n",
      "åŠ è½½å®Œæˆã€‚\n",
      "æ­£åœ¨ä¸ºGensim Phraseså‡†å¤‡å°å†™åŒ–çš„tokenåˆ—è¡¨...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "è½¬å°å†™:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2bac8c0f21e439d8997f0378ed2e222"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹è®­ç»ƒPhrasesæ¨¡åž‹...\n",
      "ä½¿ç”¨ min_count = 5\n",
      "Phrasesæ¨¡åž‹è®­ç»ƒå®Œæˆï¼è€—æ—¶: 0.60 ç§’ã€‚\n",
      "\n",
      "æ­£åœ¨å¯¼å‡ºGensimå‘çŽ°çš„å€™é€‰çŸ­è¯­...\n",
      "Gensim Phraseså‘çŽ°äº† 3172 ä¸ªå€™é€‰çŸ­è¯­ã€‚\n",
      "\n",
      "æ­£åœ¨å°†Gensimå€™é€‰ç»“æžœä¿å­˜åˆ° ../data_processed\\candidates_gensim.pkl...\n",
      "ä¿å­˜æˆåŠŸï¼\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ­¥éª¤ 1.3 - å‘½åå®žä½“è¯†åˆ« (NER)\n",
    "\n",
    "éœ€æ±‚: ä»ŽåŽŸæ–‡ä¸­å¹¶è¡Œæå–å‘½åå®žä½“ã€‚\n",
    "å®žçŽ°: ä½¿ç”¨ nlp.pipe å¹¶è¡Œå¤„ç†ï¼Œåªå¯ç”¨ NER ç›¸å…³ç»„ä»¶ã€‚"
   ],
   "id": "e11bff2e4f7bc265"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T13:23:06.826358Z",
     "start_time": "2025-08-01T13:22:34.081880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- æ­¥éª¤ 1.3: å¹¶è¡Œå‘½åå®žä½“è¯†åˆ« (NER) ---\n",
    "\n",
    "if 'df' in locals() and not df.empty and nlp:\n",
    "    print(\"\\n--- æ­¥éª¤ 1.3: å¼€å§‹å¹¶è¡ŒNERæå– ---\")\n",
    "    start_time_process = time.time()\n",
    "    ner_candidates = Counter()\n",
    "\n",
    "    print(f\"å°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹ã€‚\")\n",
    "    TARGET_ENTITY_LABELS = {'ORG', 'PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'PRODUCT', 'EVENT'}\n",
    "\n",
    "    # ä¸“æ³¨äºŽ NER, å¯ç”¨å…¶ä¾èµ–çš„ç»„ä»¶\n",
    "    with nlp.select_pipes(enable=[\"tok2vec\", \"tagger\", \"ner\"]):\n",
    "        docs = nlp.pipe(df['CONTENT'], batch_size=500, n_process=N_PROCESSES)\n",
    "        for doc in tqdm(docs, total=len(df), desc=\"NERæå–ä¸­\"):\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in TARGET_ENTITY_LABELS and ' ' in ent.text:\n",
    "                    ner_candidates[ent.text.strip().lower()] += 1\n",
    "\n",
    "    print(f\"\\nNERæå–å®Œæˆï¼è€—æ—¶: {(time.time() - start_time_process)/60:.2f} åˆ†é’Ÿã€‚\")\n",
    "\n",
    "    print(f\"æ­£åœ¨ä¿å­˜NERå€™é€‰ç»“æžœ ({len(ner_candidates)}ä¸ª) åˆ° {CANDIDATES_NER_PATH}...\")\n",
    "    with open(CANDIDATES_NER_PATH, 'wb') as f:\n",
    "        pickle.dump(ner_candidates, f)\n",
    "    print(\"ä¿å­˜æˆåŠŸï¼\")\n",
    "\n",
    "    # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ¸…ç†DataFrame\n",
    "    del ner_candidates\n",
    "    del df\n",
    "else:\n",
    "    print(\"DataFrame (df) æœªåŠ è½½æˆ–ä¸ºç©ºï¼Œè·³è¿‡NERæå–ã€‚\")\n",
    "\n",
    "print(\"\\n\\nâœ…âœ…âœ… é˜¶æ®µä¸€ï¼ˆç²¾ç®€ç‰ˆï¼‰å…¨éƒ¨å®Œæˆï¼âœ…âœ…âœ…\")\n",
    "print(\"Gensim å’Œ NER ä¸¤ç§æ¥æºçš„å€™é€‰çŸ­è¯­éƒ½å·²ç”Ÿæˆå¹¶ä¿å­˜ã€‚\")\n",
    "print(\"ä¸‹ä¸€æ­¥æ˜¯è¿è¡ŒåŽç»­çš„Notebookæ¥æ•´åˆå’Œå®¡æ ¸è¿™äº›ç»“æžœã€‚\")"
   ],
   "id": "ce704ecfd12a3fed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ­¥éª¤ 1.3: å¼€å§‹å¹¶è¡ŒNERæå– ---\n",
      "å°†ä½¿ç”¨ 5 ä¸ªè¿›ç¨‹ã€‚\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NERæå–ä¸­:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78f7b5db9e0f4e9eafaa3daf72d60625"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NERæå–å®Œæˆï¼è€—æ—¶: 0.55 åˆ†é’Ÿã€‚\n",
      "æ­£åœ¨ä¿å­˜NERå€™é€‰ç»“æžœ (7920ä¸ª) åˆ° ../data_processed\\candidates_ner.pkl...\n",
      "ä¿å­˜æˆåŠŸï¼\n",
      "\n",
      "\n",
      "âœ…âœ…âœ… é˜¶æ®µä¸€ï¼ˆç²¾ç®€ç‰ˆï¼‰å…¨éƒ¨å®Œæˆï¼âœ…âœ…âœ…\n",
      "Gensim å’Œ NER ä¸¤ç§æ¥æºçš„å€™é€‰çŸ­è¯­éƒ½å·²ç”Ÿæˆå¹¶ä¿å­˜ã€‚\n",
      "ä¸‹ä¸€æ­¥æ˜¯è¿è¡ŒåŽç»­çš„Notebookæ¥æ•´åˆå’Œå®¡æ ¸è¿™äº›ç»“æžœã€‚\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82fbf80ad289533e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
