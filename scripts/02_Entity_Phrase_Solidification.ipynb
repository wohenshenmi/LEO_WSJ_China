{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 阶段一: 候选短语的自动化发现 (The Discovery Engine)\n",
    "\n",
    "**总目标:** 从约17万条中国新闻中，通过三种互补的算法（Gensim Phrases, spaCy NER, spaCy Noun Chunks），大规模挖掘潜在的、有意义的多词短语，为后续的人工审核环节提供一个高质量的“候选池”。\n",
    "\n",
    "---\n",
    "### 步骤 1.0 - 环境设置与库导入"
   ],
   "metadata": {},
   "id": "473538b1c279a8ae"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 步骤 1.0: 环境设置与库导入 ---\n",
    "\n",
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from gensim.models.phrases import Phrases\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "\n",
    "# --- 快速测试模式开关 ---\n",
    "TEST_MODE = True\n",
    "TEST_SAMPLE_SIZE = 1000\n",
    "\n",
    "# --- 配置区 ---\n",
    "ALIYUN_OSS_PATH = ''\n",
    "# 输入文件路径\n",
    "INPUT_CHINA_DATA_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/final_china_news.csv\")\n",
    "\n",
    "# 中间产出文件路径\n",
    "TOKEN_LISTS_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/intermediate_raw_token_lists.pkl\")\n",
    "CANDIDATES_GENSIM_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/candidates_gensim.pkl\")\n",
    "CANDIDATES_NER_PATH = os.path.join(ALIYUN_OSS_PATH, \"../data_processed/candidates_ner.pkl\")\n",
    "# CANDIDATES_NOUN_CHUNKS_PATH 已被移除\n",
    "\n",
    "# --- 并行处理配置 ---\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "print(\"--- 环境准备 ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"🚀🚀🚀 运行在【快速测试模式】下，将处理前 {TEST_SAMPLE_SIZE} 行数据！🚀🚀🚀\")\n",
    "else:\n",
    "    print(\"🚢🚢🚢 运行在【完整数据模式】下，将处理所有数据。🚢🚢🚢\")\n",
    "print(f\"输入文件路径: {INPUT_CHINA_DATA_PATH}\")\n",
    "print(f\"将使用 {N_PROCESSES} 个进程进行并行处理。\")\n",
    "\n",
    "# --- 加载spaCy模型 ---\n",
    "print(\"\\n正在加载spaCy模型 'en_core_web_lg' (完整模式)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(f\"spaCy模型加载成功！耗时: {time.time() - start_time:.2f} 秒\")\n",
    "except OSError:\n",
    "    print(\"错误: spaCy模型 'en_core_web_lg' 未安装。\")\n",
    "    print(\"请在你的终端或命令行中运行: python -m spacy download en_core_web_lg\")\n",
    "    nlp = None"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:10:42.754605Z",
     "start_time": "2025-07-30T09:10:39.924655Z"
    }
   },
   "id": "b5deacf9889d8c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 环境准备 ---\n",
      "🚀🚀🚀 运行在【快速测试模式】下，将处理前 1000 行数据！🚀🚀🚀\n",
      "输入文件路径: ../data_processed/final_china_news.csv\n",
      "将使用 5 个进程进行并行处理。\n",
      "\n",
      "正在加载spaCy模型 'en_core_web_lg' (完整模式)...\n",
      "spaCy模型加载成功！耗时: 1.16 秒\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.1 - 文本预分词\n",
    "\n",
    "需求: 为 Gensim 准备 raw_token_lists。\n",
    "实现: 使用并行化的 nlp.pipe，但只启用分词功能以获得最高速度。"
   ],
   "metadata": {},
   "id": "fa0acf6370673b99"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 步骤 1.1: 并行预分词 ---\n",
    "\n",
    "# 1. 加载数据\n",
    "print(\"--- 步骤 1.1a: 数据加载 ---\")\n",
    "start_time_load = time.time()\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    df = pd.read_csv(INPUT_CHINA_DATA_PATH, dtype=str, nrows=read_nrows)\n",
    "    df.dropna(subset=['CONTENT'], inplace=True)\n",
    "    print(f\"数据加载完成！耗时: {time.time() - start_time_load:.2f} 秒。 数据形状: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 输入文件不存在: {INPUT_CHINA_DATA_PATH}\")\n",
    "\n",
    "if not df.empty and nlp:\n",
    "    print(\"\\n--- 步骤 1.1b: 开始并行预分词 ---\")\n",
    "    start_time_process = time.time()\n",
    "    raw_token_lists = []\n",
    "\n",
    "    # 禁用所有不需要的管道组件以获得最快速度\n",
    "    disabled_pipes = [pipe for pipe in nlp.pipe_names if pipe not in ['tok2vec']]\n",
    "\n",
    "    with nlp.select_pipes(disable=disabled_pipes):\n",
    "        docs = nlp.pipe(df['CONTENT'], batch_size=1000, n_process=N_PROCESSES)\n",
    "        for doc in tqdm(docs, total=len(df), desc=\"预分词中\"):\n",
    "            tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "            raw_token_lists.append(tokens)\n",
    "\n",
    "    print(f\"\\n预分词处理完成！耗时: {(time.time() - start_time_process)/60:.2f} 分钟。\")\n",
    "\n",
    "    print(f\"正在保存预分词结果 ({len(raw_token_lists)}篇文章) 到 {TOKEN_LISTS_PATH}...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'wb') as f:\n",
    "        pickle.dump(raw_token_lists, f)\n",
    "    print(\"保存成功！\")\n",
    "else:\n",
    "    print(\"DataFrame为空或spaCy模型未加载，跳过处理。\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:11:14.566073Z",
     "start_time": "2025-07-30T09:10:42.909682Z"
    }
   },
   "id": "b041192e11a76d3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 1.1a: 数据加载 ---\n",
      "数据加载完成！耗时: 0.04 秒。 数据形状: (1000, 2)\n",
      "\n",
      "--- 步骤 1.1b: 开始并行预分词 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "预分词中:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba53445db8bf4219b42d68ede9789d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预分词处理完成！耗时: 0.53 分钟。\n",
      "正在保存预分词结果 (1000篇文章) 到 ../data_processed/intermediate_raw_token_lists.pkl...\n",
      "保存成功！\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.2 - Gensim Phrases 发现\n",
    "\n",
    "需求: 使用上一步的 raw_token_lists 训练 Phrases 模型。\n",
    "实现: 从 .pkl 文件加载数据，然后进行训练和保存。"
   ],
   "id": "d1ff7362e63509a5"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 步骤 1.2: Gensim Phrases 发现 ---\n",
    "if os.path.exists(TOKEN_LISTS_PATH):\n",
    "    print(f\"--- 步骤 1.2: 开始 Gensim Phrases 发现 ---\")\n",
    "    print(f\"正在从 {TOKEN_LISTS_PATH} 加载预分词结果...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'rb') as f:\n",
    "        raw_token_lists_gensim = pickle.load(f)\n",
    "    print(\"加载完成。\")\n",
    "\n",
    "    print(\"正在为Gensim Phrases准备小写化的token列表...\")\n",
    "    documents_for_gensim = [[token.lower() for token in doc] for doc in tqdm(raw_token_lists_gensim, desc=\"转小写\")]\n",
    "    del raw_token_lists_gensim\n",
    "\n",
    "    print(\"\\n开始训练Phrases模型...\")\n",
    "    start_time = time.time()\n",
    "    min_count = 5 if TEST_MODE else 20\n",
    "    print(f\"使用 min_count = {min_count}\")\n",
    "    phrases_model = Phrases(documents_for_gensim, min_count=min_count, threshold=10.0, delimiter='_')\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Phrases模型训练完成！耗时: {train_time:.2f} 秒。\")\n",
    "\n",
    "    print(\"\\n正在导出Gensim发现的候选短语...\")\n",
    "    gensim_candidates_raw = phrases_model.export_phrases()\n",
    "\n",
    "    # 使用最简单、最兼容的二元组解包\n",
    "    gensim_candidates = {\n",
    "        phrase.replace('_', ' '): score\n",
    "        for phrase, score in gensim_candidates_raw\n",
    "    }\n",
    "\n",
    "    print(f\"Gensim Phrases发现了 {len(gensim_candidates)} 个候选短语。\")\n",
    "\n",
    "    print(f\"\\n正在将Gensim候选结果保存到 {CANDIDATES_GENSIM_PATH}...\")\n",
    "    with open(CANDIDATES_GENSIM_PATH, 'wb') as f:\n",
    "        pickle.dump(gensim_candidates, f)\n",
    "    print(\"保存成功！\")\n",
    "else:\n",
    "    print(f\"❌ 错误: Gensim的输入文件 {TOKEN_LISTS_PATH} 不存在。请先运行步骤 1.1。\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:11:15.759147Z",
     "start_time": "2025-07-30T09:11:14.577439Z"
    }
   },
   "id": "39e6bc84e9f5a316",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 1.2: 开始 Gensim Phrases 发现 ---\n",
      "正在从 ../data_processed/intermediate_raw_token_lists.pkl 加载预分词结果...\n",
      "加载完成。\n",
      "正在为Gensim Phrases准备小写化的token列表...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "转小写:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "024bf10bba7e4e3780d6196d3ca7b1ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练Phrases模型...\n",
      "使用 min_count = 5\n",
      "Phrases模型训练完成！耗时: 0.67 秒。\n",
      "\n",
      "正在导出Gensim发现的候选短语...\n",
      "检测到Gensim返回字典格式，使用 .items() 进行解包。\n",
      "Gensim Phrases发现了 3262 个候选短语。\n",
      "\n",
      "正在将Gensim候选结果保存到 ../data_processed/candidates_gensim.pkl...\n",
      "保存成功！\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.3 - 命名实体识别 (NER)\n",
    "\n",
    "需求: 从原文中并行提取命名实体。\n",
    "实现: 使用 nlp.pipe 并行处理，只启用 NER 相关组件。"
   ],
   "id": "c5cb1602462a0186"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:11:49.274956Z",
     "start_time": "2025-07-30T09:11:15.771376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 步骤 1.3: 并行命名实体识别 (NER) ---\n",
    "\n",
    "if 'df' in locals() and not df.empty and nlp:\n",
    "    print(\"\\n--- 步骤 1.3: 开始并行NER提取 ---\")\n",
    "    start_time_process = time.time()\n",
    "    ner_candidates = Counter()\n",
    "\n",
    "    print(f\"将使用 {N_PROCESSES} 个进程。\")\n",
    "    TARGET_ENTITY_LABELS = {'ORG', 'PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'PRODUCT', 'EVENT'}\n",
    "\n",
    "    # 专注于 NER, 启用其依赖的组件\n",
    "    with nlp.select_pipes(enable=[\"tok2vec\", \"tagger\", \"ner\"]):\n",
    "        docs = nlp.pipe(df['CONTENT'], batch_size=500, n_process=N_PROCESSES)\n",
    "        for doc in tqdm(docs, total=len(df), desc=\"NER提取中\"):\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in TARGET_ENTITY_LABELS and ' ' in ent.text:\n",
    "                    ner_candidates[ent.text.strip().lower()] += 1\n",
    "\n",
    "    print(f\"\\nNER提取完成！耗时: {(time.time() - start_time_process)/60:.2f} 分钟。\")\n",
    "\n",
    "    print(f\"正在保存NER候选结果 ({len(ner_candidates)}个) 到 {CANDIDATES_NER_PATH}...\")\n",
    "    with open(CANDIDATES_NER_PATH, 'wb') as f:\n",
    "        pickle.dump(ner_candidates, f)\n",
    "    print(\"保存成功！\")\n",
    "\n",
    "    # 所有任务完成，清理DataFrame\n",
    "    del ner_candidates\n",
    "    del df\n",
    "else:\n",
    "    print(\"DataFrame (df) 未加载或为空，跳过NER提取。\")\n",
    "\n",
    "print(\"\\n\\n✅✅✅ 阶段一（精简版）全部完成！✅✅✅\")\n",
    "print(\"Gensim 和 NER 两种来源的候选短语都已生成并保存。\")\n",
    "print(\"下一步是运行后续的Notebook来整合和审核这些结果。\")"
   ],
   "id": "84483f7d0a465376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 1.3: 开始并行NER提取 ---\n",
      "将使用 5 个进程。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NER提取中:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5f6f56f2daa49589bfcd51bab7e9fdd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER提取完成！耗时: 0.56 分钟。\n",
      "正在保存NER候选结果 (8493个) 到 ../data_processed/candidates_ner.pkl...\n",
      "保存成功！\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "552cfac281b6b864"
  }
 ],
 "execution_count": null
}
