{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 阶段一: 候选短语的自动化发现 (The Discovery Engine)\n",
    "\n",
    "**总目标:** 从约17万条中国新闻中，通过三种互补的算法（Gensim Phrases, spaCy NER, spaCy Noun Chunks），大规模挖掘潜在的、有意义的多词短语，为后续的人工审核环节提供一个高质量的“候选池”。\n",
    "\n",
    "---\n",
    "### 步骤 1.0: 环境设置与库导入"
   ],
   "id": "473538b1c279a8ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "from gensim.models.phrases import Phrases\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import pickle  # 用于保存中间结果\n",
    "import multiprocessing as mp\n",
    "\n",
    "# --- 配置区 ---\n",
    "\n",
    "ALIYUN_OSS_PATH = ''  #ALIYUN_OSS_PATH = '/mnt/data/scripts/'\n",
    "\n",
    "# 输入文件: 筛选并去重后的中国相关文章数据 (CSV格式)\n",
    "INPUT_CHINA_DATA_PATH = ALIYUN_OSS_PATH + \"../data_processed/final_china_news.csv\"\n",
    "# 中间产出文件: 保存分词结果，避免重复运行耗时步骤\n",
    "TOKEN_LISTS_PATH = ALIYUN_OSS_PATH + \"../data_processed/intermediate_raw_token_lists.pkl\"\n",
    "# 最终产出文件: 保存本阶段发现的所有候选短语，以便下一阶段整合\n",
    "CANDIDATES_GENSIM_PATH = ALIYUN_OSS_PATH + \"../data_processed/candidates_gensim.pkl\"\n",
    "CANDIDATES_NER_PATH = ALIYUN_OSS_PATH + \"../data_processed/candidates_ner.pkl\"\n",
    "CANDIDATES_NOUN_CHUNKS_PATH = ALIYUN_OSS_PATH + \"../data_processed/candidates_noun_chunks.pkl\"\n",
    "\n",
    "print(\"--- 环境准备 ---\")\n",
    "print(f\"输入文件路径: {INPUT_CHINA_DATA_PATH}\")\n",
    "\n",
    "# --- 加载spaCy模型 ---\n",
    "# 这里我们需要完整的模型来进行NER和语法分析\n",
    "print(\"\\n正在加载spaCy模型 'en_core_web_lg' (完整模式)...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\", disable=[\"lemmatizer\"])\n",
    "    print(f\"spaCy模型加载成功！耗时: {time.time() - start_time:.2f} 秒\")\n",
    "except OSError:\n",
    "    print(\"错误: spaCy模型 'en_core_web_lg' 未安装。\")\n",
    "    print(\"请在你的终端或命令行中运行: python -m spacy download en_core_web_lg\")\n"
   ],
   "id": "b5deacf9889d8c5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.1: 前置任务 - 文本预分词\n",
    "\n",
    "**逻辑与目的:**\n",
    "为了让后续的`Gensim Phrases`模型能够高效运行，我们需要先将长文本切分成单词列表。此处的处理非常轻量，目的是保留尽可能多的原始短语结构，包括大小写和特殊字符组合。\n",
    "\n",
    "**动作:**\n",
    "1. 加载数据。\n",
    "2. 创建分词器。\n",
    "3. 批量分词，只进行最基础的过滤（非标点、非空格），保留原始文本。\n",
    "4. 生成并保存内存对象 `raw_token_lists`。"
   ],
   "id": "fa0acf6370673b99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 步骤 1.1, 1.3, 1.4: 一体化并行处理（内存优化版）---\n",
    "\n",
    "# 1. 加载数据 (修正您指出的缺失问题)\n",
    "print(\"--- 步骤 1: 数据加载与准备 ---\")\n",
    "print(f\"正在从 {INPUT_CHINA_DATA_PATH} 读取数据...\")\n",
    "start_time_load = time.time()\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CHINA_DATA_PATH, dtype={'DATE': str, 'CONTENT': str})\n",
    "    df.dropna(subset=['CONTENT'], inplace=True)\n",
    "    print(f\"数据加载完成！耗时: {time.time() - start_time_load:.2f} 秒。 数据形状: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误: 输入文件不存在: {INPUT_CHINA_DATA_PATH}\")\n",
    "    df = pd.DataFrame() # 创建空DataFrame以避免后续代码出错\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n--- 步骤 2: 开始一体化并行处理：预分词、NER、名词短语提取 ---\")\n",
    "    start_time_process = time.time()\n",
    "\n",
    "    # 初始化所有结果容器\n",
    "    # 注意：这里我们仍然一次性将结果保存在内存中。\n",
    "    # 如果内存非常紧张，后续可以修改为流式写入文件。\n",
    "    # 但对于17万篇文章，现代PC通常可以承受这些结果对象的内存占用。\n",
    "    raw_token_lists = []\n",
    "    ner_candidates = Counter()\n",
    "    noun_chunk_candidates = Counter()\n",
    "\n",
    "    # 定义常量和智能配置进程数 (采纳您的建议)\n",
    "    import psutil\n",
    "    cpu_cores = psutil.cpu_count(logical=False) # 使用物理核心数可能更稳定\n",
    "    # 根据可用内存和CPU核心数动态调整，这里设置一个经验值：每进程至少需要2GB空闲内存\n",
    "    max_proc_by_mem = psutil.virtual_memory().available // (2 * 1024**3)\n",
    "    n_processes = min(cpu_cores - 1 if cpu_cores > 1 else 1, int(max_proc_by_mem), 8) # 上限设为8，避免过度消耗\n",
    "    if n_processes < 1: n_processes = 1\n",
    "\n",
    "    print(f\"系统CPU物理核心: {cpu_cores}, 可用内存: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n",
    "    print(f\"将使用 {n_processes} 个进程进行一体化并行处理。\")\n",
    "\n",
    "    TARGET_ENTITY_LABELS = {'ORG', 'PERSON', 'GPE', 'NORP', 'FAC', 'LOC', 'PRODUCT', 'EVENT'}\n",
    "    determiners = {'the', 'a', 'an', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "    # --- 只进行一次 nlp.pipe 循环，并启用并行处理 ---\n",
    "    # batch_size 可以适当调大，例如 500-1000，以减少进程通信开销\n",
    "    with nlp.select_pipes(enable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\"]): # 明确启用需要的组件\n",
    "        for doc in tqdm(nlp.pipe(df['CONTENT'], batch_size=500, n_process=n_processes), total=len(df), desc=\"一体化处理中\"):\n",
    "\n",
    "            # === 任务1: 预分词 (来自步骤 1.1) ===\n",
    "            tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "            raw_token_lists.append(tokens)\n",
    "\n",
    "            # === 任务2: NER提取 (来自步骤 1.3) ===\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in TARGET_ENTITY_LABELS:\n",
    "                    entity_text = ent.text.strip()\n",
    "                    if ' ' in entity_text:\n",
    "                        ner_candidates[entity_text.lower()] += 1\n",
    "\n",
    "            # === 任务3: 名词短语提取 (来自步骤 1.4) ===\n",
    "            for chunk in doc.noun_chunks:\n",
    "                phrase_text = chunk.text.lower()\n",
    "                words = phrase_text.split()\n",
    "                if words:\n",
    "                    if words[0] in determiners: words = words[1:]\n",
    "                    if words and words[-1] in determiners: words = words[:-1]\n",
    "                cleaned_phrase = \" \".join(words).strip()\n",
    "                if cleaned_phrase and ' ' in cleaned_phrase:\n",
    "                    noun_chunk_candidates[cleaned_phrase] += 1\n",
    "\n",
    "    total_time = time.time() - start_time_process\n",
    "    print(f\"\\n一体化处理完成！总耗时: {total_time/60:.2f} 分钟。\")\n",
    "\n",
    "    # --- 步骤 3: 保存所有中间结果 ---\n",
    "    print(\"\\n--- 步骤 3: 保存所有处理结果 ---\")\n",
    "\n",
    "    print(f\"正在保存预分词结果 ({len(raw_token_lists)}篇文章) 到 {TOKEN_LISTS_PATH}...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'wb') as f:\n",
    "        pickle.dump(raw_token_lists, f)\n",
    "    print(\"保存成功！\")\n",
    "    # 及时清理内存 (采纳您的建议)\n",
    "    del raw_token_lists\n",
    "\n",
    "    print(f\"\\n正在保存NER候选结果 ({len(ner_candidates)}个) 到 {CANDIDATES_NER_PATH}...\")\n",
    "    with open(CANDIDATES_NER_PATH, 'wb') as f:\n",
    "        pickle.dump(ner_candidates, f)\n",
    "    print(\"保存成功！\")\n",
    "    del ner_candidates\n",
    "\n",
    "    print(f\"\\n正在保存名词短语候选结果 ({len(noun_chunk_candidates)}个) 到 {CANDIDATES_NOUN_CHUNKS_PATH}...\")\n",
    "    with open(CANDIDATES_NOUN_CHUNKS_PATH, 'wb') as f:\n",
    "        pickle.dump(noun_chunk_candidates, f)\n",
    "    print(\"保存成功！\")\n",
    "    del noun_chunk_candidates\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame为空，跳过处理步骤。\")"
   ],
   "id": "b041192e11a76d3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 步骤 1.2: )？？？？？候选发现策略一 - 统计共现 (Gensim `Phrases`\n",
    "\n",
    "**逻辑与目的:**\n",
    "利用统计学原理，找出那些“粘合度”很高的、频繁在一起出现的单词组合。此方法对发现**专业术语**和高频实体特别有效。\n",
    "\n",
    "**动作:**\n",
    "1.  标准化输入：创建`raw_token_lists`的小写副本。\n",
    "2.  训练`Phrases`模型。\n",
    "3.  导出候选短语及其得分。"
   ],
   "id": "562b1bdb5b1b55b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "traceback": [
    "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
    "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
    "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     17\u001B[39m raw_token_lists = []\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# 使用nlp.pipe进行高效批量处理\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# 我们只对CONTENT列进行操作\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m tqdm(nlp.pipe(df[\u001B[33m'\u001B[39m\u001B[33mCONTENT\u001B[39m\u001B[33m'\u001B[39m], batch_size=\u001B[32m500\u001B[39m), total=\u001B[38;5;28mlen\u001B[39m(df), desc=\u001B[33m\"\u001B[39m\u001B[33m预分词中\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# 过滤条件：只要token既不是纯标点符号，也不是纯空格，就保留其原始文本\u001B[39;00m\n\u001B[32m     22\u001B[39m     tokens = [token.text \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token.is_punct \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m token.is_space]\n\u001B[32m     23\u001B[39m     raw_token_lists.append(tokens)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m   1182\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[32m   1183\u001B[39m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[32m   1184\u001B[39m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1622\u001B[39m, in \u001B[36mLanguage.pipe\u001B[39m\u001B[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001B[39m\n\u001B[32m   1620\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1621\u001B[39m         docs = pipe(docs)\n\u001B[32m-> \u001B[39m\u001B[32m1622\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs:\n\u001B[32m   1623\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m doc\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:245\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:245\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1714\u001B[39m, in \u001B[36m_pipe\u001B[39m\u001B[34m(docs, proc, name, default_error_handler, kwargs)\u001B[39m\n\u001B[32m   1704\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_pipe\u001B[39m(\n\u001B[32m   1705\u001B[39m     docs: Iterable[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   1706\u001B[39m     proc: \u001B[33m\"\u001B[39m\u001B[33mPipeCallable\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1711\u001B[39m     kwargs: Mapping[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m   1712\u001B[39m ) -> Iterator[\u001B[33m\"\u001B[39m\u001B[33mDoc\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m   1713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(proc, \u001B[33m\"\u001B[39m\u001B[33mpipe\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1714\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m proc.pipe(docs, **kwargs)\n\u001B[32m   1715\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1716\u001B[39m         \u001B[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001B[39;00m\n\u001B[32m   1717\u001B[39m         kwargs = \u001B[38;5;28mdict\u001B[39m(kwargs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001B[39m, in \u001B[36mpipe\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\util.py:1661\u001B[39m, in \u001B[36mminibatch\u001B[39m\u001B[34m(items, size)\u001B[39m\n\u001B[32m   1659\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m   1660\u001B[39m     batch_size = \u001B[38;5;28mnext\u001B[39m(size_)\n\u001B[32m-> \u001B[39m\u001B[32m1661\u001B[39m     batch = \u001B[38;5;28mlist\u001B[39m(itertools.islice(items, \u001B[38;5;28mint\u001B[39m(batch_size)))\n\u001B[32m   1662\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(batch) == \u001B[32m0\u001B[39m:\n\u001B[32m   1663\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1619\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1616\u001B[39m     docs = \u001B[38;5;28mself\u001B[39m._multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001B[32m   1617\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1618\u001B[39m     \u001B[38;5;66;03m# if n_process == 1, no processes are forked.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1619\u001B[39m     docs = (\u001B[38;5;28mself\u001B[39m._ensure_doc(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts)\n\u001B[32m   1620\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pipe \u001B[38;5;129;01min\u001B[39;00m pipes:\n\u001B[32m   1621\u001B[39m         docs = pipe(docs)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1132\u001B[39m, in \u001B[36mLanguage._ensure_doc\u001B[39m\u001B[34m(self, doc_like)\u001B[39m\n\u001B[32m   1130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m doc_like\n\u001B[32m   1131\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.make_doc(doc_like)\n\u001B[32m   1133\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc_like, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[32m   1134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Doc(\u001B[38;5;28mself\u001B[39m.vocab).from_bytes(doc_like)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\language.py:1124\u001B[39m, in \u001B[36mLanguage.make_doc\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m   1120\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) > \u001B[38;5;28mself\u001B[39m.max_length:\n\u001B[32m   1121\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1122\u001B[39m         Errors.E088.format(length=\u001B[38;5;28mlen\u001B[39m(text), max_length=\u001B[38;5;28mself\u001B[39m.max_length)\n\u001B[32m   1123\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tokenizer(text)\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:160\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:196\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:400\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._tokenize\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\tokenizer.pyx:480\u001B[39m, in \u001B[36mspacy.tokenizer.Tokenizer._attach_tokens\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\vocab.pyx:205\u001B[39m, in \u001B[36mspacy.vocab.Vocab.get\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\vocab.pyx:234\u001B[39m, in \u001B[36mspacy.vocab.Vocab._new_lexeme\u001B[39m\u001B[34m()\u001B[39m\n",
    "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\LEO_WSJ_China\\Lib\\site-packages\\spacy\\lang\\lex_attrs.py:145\u001B[39m, in \u001B[36mlower\u001B[39m\u001B[34m(string)\u001B[39m\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mlower\u001B[39m(string: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m string.lower()\n",
    "\u001B[31mKeyboardInterrupt\u001B[39m: "
   ],
   "id": "6f322e7effca4844",
   "source": [
    "# --- 步骤 1.2: Gensim Phrases 发现 ---\n",
    "import pickle\n",
    "from gensim.models.phrases import Phrases\n",
    "import os\n",
    "\n",
    "# 确保输入文件存在\n",
    "if not os.path.exists(TOKEN_LISTS_PATH):\n",
    "    print(f\"❌ 错误: Gensim的输入文件 {TOKEN_LISTS_PATH} 不存在。请先运行上一步骤。\")\n",
    "else:\n",
    "    print(f\"正在从 {TOKEN_LISTS_PATH} 加载预分词结果...\")\n",
    "    with open(TOKEN_LISTS_PATH, 'rb') as f:\n",
    "        # 对于Gensim，我们需要一次性加载所有token列表来训练模型\n",
    "        # 如果内存极度紧张，这里可以考虑分块训练，但会损失一些精度\n",
    "        raw_token_lists = pickle.load(f)\n",
    "    print(\"加载完成。\")\n",
    "\n",
    "    # 1. 标准化输入\n",
    "    print(\"正在为Gensim Phrases准备小写化的token列表...\")\n",
    "    documents_for_gensim = [[token.lower() for token in doc] for doc in tqdm(raw_token_lists, desc=\"转小写\")]\n",
    "    del raw_token_lists # 及时清理\n",
    "\n",
    "    # 2. 训练模型\n",
    "    print(\"\\n开始训练Phrases模型...\")\n",
    "    start_time = time.time()\n",
    "    phrases_model = Phrases(documents_for_gensim, min_count=20, threshold=10.0, delimiter=b'_')\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Phrases模型训练完成！耗时: {train_time:.2f} 秒。\")\n",
    "\n",
    "    # 3. 导出候选\n",
    "    print(\"\\n正在导出Gensim发现的候选短语...\")\n",
    "    gensim_candidates_raw = phrases_model.export_phrases(documents_for_gensim)\n",
    "    gensim_candidates = {phrase.decode('utf-8').replace('_', ' '): score for phrase, score in gensim_candidates_raw}\n",
    "    print(f\"Gensim Phrases发现了 {len(gensim_candidates)} 个候选短语。\")\n",
    "\n",
    "    # 4. 保存结果\n",
    "    print(f\"\\n正在将Gensim候选结果保存到 {CANDIDATES_GENSIM_PATH}...\")\n",
    "    with open(CANDIDATES_GENSIM_PATH, 'wb') as f:\n",
    "        pickle.dump(gensim_candidates, f)\n",
    "    print(\"保存成功！\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "execution_count": null
}
