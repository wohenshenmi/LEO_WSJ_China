### **需求文档：中国相关新闻高效筛选与过滤流程 NEW**

**项目目标：** 从一个包含海量全球新闻的巨大CSV文件 (`final_merged_all_news.csv`) 中，通过一个高度优化的、分为“快速初筛”和“上下文精筛”两阶段的自动化流程，精准地筛选出与中国经济、政治、科技等领域高度相关的英文新闻，并生成一个干净、高相关性的最终数据集以供后续分析。

**输入与输出文件：**
*   **主要输入文件：**
    *   `../data_raw/final_merged_all_news.csv`: 包含了所有新闻的原始、未经过滤的庞大语料库。
    *   `../data_raw/china_keywords_collection.json`: 一个结构化的JSON文件，定义了所有与中国相关的关键词、它们的别名、类别以及相关性等级 (tier)。
*   **过程与输出文件：**
    *   `../data_processed/china_news_candidates.csv`: 经过第一阶段快速初筛后产生的“候选文章”集合。
    *   `../data_processed/final_china_news.csv`: 经过第二阶段精筛后得到的最终高相关性新闻集合（**核心产出**）。
    *   `../data_processed/china_news_rejected_articles.csv`: 在精筛阶段因未达到相关性分数阈值而被拒绝的文章，附带其最终得分和扣分详情，用于分析和优化规则。

---

### **阶段一：准备工作与环境设置**

**此阶段总目标：** 导入所有必需的Python库，定义文件路径，并根据运行环境的硬件资源（CPU核心数）智能配置并行处理参数，为后续高效处理奠定基础。

*   **逻辑与目的：**
    这是整个流程的初始化步骤。通过集中管理所有配置，可以方便地调整路径和参数。特别是，代码通过`psutil`库自动检测CPU核心数来设定并行进程数量，旨在最大化利用计算资源，缩短处理时间。
*   **动作：**
    1.  **导入核心库：** 加载 `pandas` (数据处理), `spacy` (NLP), `os` (路径操作), `re` (正则表达式), `json` (读取关键词)等。
    2.  **导入优化库：** 加载 `flashtext` (用于高速关键词匹配), `tqdm` (用于显示进度条), 和 `psutil` (用于检测系统资源)。
    3.  **定义全局路径：** 设置所有输入和输出文件的完整路径。
    4.  **智能配置参数：**
        *   自动获取物理CPU核心数。
        *   计算并设置一个合理的并行进程数 `N_PROCESSES`（通常是核心数-1，且上限为8），以避免系统过载。
        *   设置分块处理的大小 `CHUNKSIZE` 和 `spaCy` 的批处理大小 `BATCH_SIZE`。
*   **阶段产出：**
    *   所有必需的库被载入内存。
    *   `N_PROCESSES`, `CHUNKSIZE` 等全局参数被设定。

---

### **阶段二：构建高效关键词初筛引擎**

**此阶段总目标：** 读取关键词JSON文件，并构建一个基于`Flashtext`库的高速关键词查找处理器。

*   **逻辑与目的：**
    初筛阶段需要处理数百万篇文章，传统的正则表达式或简单的字符串查找会非常缓慢。`Flashtext`是一个专门为此类多关键词匹配场景设计的库，其算法复杂度与关键词数量无关，速度极快。此步骤就是为了构建这个核心的初筛工具。
*   **输入：** `KEYWORD_JSON_PATH` 指向的 `china_keywords_collection.json` 文件。
*   **动作：**
    1.  **加载关键词文件：** 读取 `china_keywords_collection.json` 文件。
    2.  **提取所有别名：** 遍历JSON数据，将每个条目的主关键词 (`keyword`) 和其所有别名 (`aliases`) 提取出来，放入一个集合中以确保唯一性。
    3.  **构建处理器：**
        *   初始化一个不区分大小写的 `Flashtext` `KeywordProcessor`。
        *   将提取出的所有不重复的关键词/别名添加到处理器中。
*   **阶段产出：**
    *   一个名为 `keyword_processor` 的 `Flashtext` 对象，已“学习”了所有待查找的词条，可用于后续的快速匹配。
    *   一个名为 `keywords_data` 的列表，包含了从JSON加载的原始结构化数据，供后续精筛阶段使用。

---

### **阶段三：执行第一阶段 - 大规模流式初筛**

**此阶段总目标：** 以**单进程流式处理**（streaming）的方式，对巨大的原始新闻文件进行扫描，使用上一阶段构建的 `Flashtext` 处理器进行快速匹配，并将包含任何关键词的文章筛选出来，存为候选集。

*   **逻辑与目的：**
    这是整个流程中计算密集度最高的步骤之一。为了处理G或TB级别的大文件而不耗尽内存，必须采用**流式处理**（即将文件分成小块`chunk`逐一处理）。为保证稳定性和避免复杂的进程间通信问题，此阶段采用单进程顺序处理。
*   **输入：**
    *   `SOURCE_NEWS_FILE` 指向的 `final_merged_all_news.csv`。
    *   上一阶段在内存中创建的 `keyword_processor` 对象。
*   **动作：**
    1.  **创建流式迭代器：** 使用 `pandas.read_csv` 的 `chunksize` 参数，创建一个文件块的迭代器。
    2.  **顺序处理：**
        *   在主进程中，遍历迭代器中的每一个 `chunk` (一个DataFrame)。
        *   对每个`chunk`调用 `process_chunk` 函数。该函数首先对新闻内容进行轻量级清洗，然后使用 `keyword_processor` 检查文章是否包含关键词。
        *   该函数返回一个只包含匹配到的文章的DataFrame。
    3.  **收集并写入结果：** 将由 `process_chunk` 返回的、已筛选过的DataFrame，以追加（append）模式实时写入到 `CANDIDATES_FILE` 中。第一个块写入时包含表头，后续块不再写入表头。
*   **阶段产出：**
    *   一个名为 `china_news_candidates.csv` 的CSV文件，包含了所有至少命中一个关键词的文章。这是进入下一阶段精筛的“原料”。

---

### **阶段四：准备第二阶段 - 构建多维度相关性评估引擎**

**此阶段总目标：** 加载 `spaCy` 模型，并构建一套基于**“积分与扣分系统”**的、更加智能和鲁棒的评估规则函数，放弃原有的“一票否决”制。

*   **逻辑与目的：**
    简单的“否决规则”会因个别句子而“误伤”高价值文章。新的评估引擎通过综合评估文章的多个维度（关键词权重、位置、上下文风险），计算出一个**相关性分数**，从而更准确地判断文章的整体价值，实现“宁愿放错，不错杀”的原则。
*   **输入：** 从阶段二加载到内存的 `keywords_data`。
*   **动作：**
    1.  **加载优化后的 spaCy 模型：**
        *   加载 `en_core_web_lg` 模型，并禁用所有非必需的管道组件，只保留 `parser`。
    2.  **构建关键词信息查找表：**
        *   创建一个`keyword_lookup`字典，将每个关键词（小写）映射到其详细信息，特别是 `relevance_tier`。这是后续计分的基础。
    3.  **构建 PhraseMatcher：**
        *   初始化一个 `spacy.matcher.PhraseMatcher`，并添加所有关键词。
    4.  **定义积分/扣分规则函数：**
        *   **正向加分规则：**
            *   `score_title_lead_presence`: 检查文章**标题**和**首段**是否出现核心关键词(Tier 3)。如果出现，给予**高额加分**（如标题+10，首段+5）。
            *   `score_keyword_frequency`: 根据文中出现的所有关键词的**频率**和**等级(tier)**进行加权计分。高等级的词出现次数越多，得分越高。
        *   **负向扣分规则：**
            *   `penalize_hypothetical`: 每当发现一个包含关键词的**假设句**（如以`if`, `unless`开头），进行**适度扣分**（如-2分），而不是直接拒绝。
            *   `penalize_negation`: 每当发现一个被**否定的关键词**，进行**适度扣分**（如-3分），并尝试区分强否定与弱否定。
*   **阶段产出：**
    *   一个轻量化的 `nlp` 对象和一个 `matcher` 对象。
    *   一个 `keyword_lookup` 字典。
    *   一套可调用的Python函数，分别对应各项**加分**和**扣分**规则。
    *   一个预设的**接受门槛**分数 `ACCEPTANCE_THRESHOLD`（例如 5.0 分）。

---

### **阶段五：执行第二阶段 - 上下文精筛与产出**

**此阶段总目标：** 读取候选集，对每篇文章应用“积分与扣分”评估体系，计算其最终相关性分数，并根据分数是否达到预设门槛来决定接受或拒绝。

*   **逻辑与目的：**
    这是质量控制的最后一步。通过并行化的深度语法分析和多维度的量化评估，确保最终产出的数据集具有最高的信噪比，同时最大限度地保留了那些内容相关但可能包含个别复杂句式的文章。
*   **输入：**
    *   `CANDIDATES_FILE` 指向的 `china_news_candidates.csv`。
    *   上一阶段准备好的 `nlp` 对象、`matcher` 对象、`keyword_lookup`、评估规则函数及 `ACCEPTANCE_THRESHOLD`。
*   **动作：**
    1.  **加载候选集：** 将 `china_news_candidates.csv` 读入内存。
    2.  **并行NLP处理：** 使用 `nlp.pipe` 对所有候选文章进行高效并行的 `spaCy` 处理。
    3.  **应用积分规则进行精筛：**
        *   遍历每一个 `Doc` 对象，并为每篇文章初始化 `relevance_score = 0`。
        *   调用**加分函数** (`score_title_lead_presence`, `score_keyword_frequency`)，将得分累加到 `relevance_score`。
        *   调用**扣分函数** (`penalize_hypothetical`, `penalize_negation`)，从 `relevance_score` 中减去扣分。
    4.  **决策与分离：**
        *   将每篇文章的最终 `relevance_score` 与 `ACCEPTANCE_THRESHOLD` 进行比较。
        *   如果 `score >= threshold`，则将该文章划入“接受”集合。
        *   如果 `score < threshold`，则划入“拒绝”集合，并记录其最终分数和主要的扣分项作为拒绝原因。
    5.  **保存结果：**
        *   将“接受”的DataFrame保存到 `FINAL_RESULT_FILE`。
        *   将被拒绝的文章及其详细的得分/扣分信息保存到 `REJECTED_FILE`。
*   **阶段产出：**
    *   **`final_china_news.csv`**: 最终的高质量、高相关性新闻数据集。
    *   **`china_news_rejected_articles.csv`**: 被过滤掉的文章，附带其**最终得分**和**评估明细**，为后续规则微调提供强大的数据支持。